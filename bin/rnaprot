#!/usr/bin/env python3

from rnaprot.RNNNets import RNNModel, RNNDataset
from torch.utils.data import DataLoader
from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit
from torch.nn import BCEWithLogitsLoss
#from torch.nn.functional import nll_loss
from rnaprot import rnn_util
from rnaprot import rplib
import numpy as np
import statistics
import argparse
import random
import shutil
import torch
import torch.nn as nn
import uuid
import copy
import sys
import os
import re

__version__ = "0.1"

"""

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~ OPEN FOR BUSINESS ~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


AuthoR: uhlm [at] informatik [dot] uni-freiburg [dot] de


~~~~~~~~~~~~~~~~~~~~~~~~~~
Check out available modes
~~~~~~~~~~~~~~~~~~~~~~~~~~

rnaprot -h


"""


# Loss criterion.
criterion = BCEWithLogitsLoss()


################################################################################

def setup_argument_parser():
    """Setup argparse parser."""
    # Tool description text.
    help_description = """

    Modelling RBP binding preferences to predict RPB binding sites.

    """

    # Define argument parser.
    p = argparse.ArgumentParser(#add_help=False,
                                prog="rnaprot",
                                description=help_description)

    # Tool version.
    p.add_argument("-v", "--version", action="version",
                   version="rnaprot v" + __version__)

    # Add subparsers.
    subparsers = p.add_subparsers(help='Program modes')

    """
    Model training mode.
    """
    p_tr = subparsers.add_parser('train',
                                  help='Train a binding site prediction model')
    p_tr.set_defaults(which='train')

    # Add required arguments group.
    p_trm = p_tr.add_argument_group("required arguments")

    # Required arguments for train.
    p_trm.add_argument("--in",
                   dest="in_folder",
                   type=str,
                   required = True,
                   help = "Input training data folder (output of rnaprot gt)")
    p_trm.add_argument("--out",
                   dest="out_folder",
                   type=str,
                   required = True,
                   help = "Model training results output folder")

    # Add feature usage arguments group.
    p_trf = p_tr.add_argument_group("feature definition arguments")
    p_trf.add_argument("--only-seq",
                   dest = "only_seq",
                   default = False,
                   action = "store_true",
                   help = "Use only sequence feature. By default all features present in --in are used (default: False)")
    p_trf.add_argument("--use-phastcons",
                   dest = "use_pc_con",
                   default = False,
                   action = "store_true",
                   help = "Add phastCons conservation scores. Set --use-xxx to define which features to add on top of sequence feature (by default all --in features are used)")
    p_trf.add_argument("--use-phylop",
                   dest = "use_pp_con",
                   default = False,
                   action = "store_true",
                   help = "Add phyloP conservation scores. Set --use-xxx to define which features to add on top of sequence feature (by default all --in features are used)")
    p_trf.add_argument("--use-eia",
                   dest = "use_eia",
                   default = False,
                   action = "store_true",
                   help = "Add exon-intron annotations. Set --use-xxx to define which features to add on top of sequence feature (by default all --in features are used)")
    p_trf.add_argument("--use-tra",
                   dest = "use_tra",
                   default = False,
                   action = "store_true",
                   help = "Add transcript region annotations. Set --use-xxx to define which features to add on top of sequence feature (by default all --in features are used)")
    p_trf.add_argument("--use-rra",
                   dest = "use_rra",
                   default = False,
                   action = "store_true",
                   help = "Add repeat region annotations. Set --use-xxx to define which features to add on top of sequence feature (by default all --in features are used)")
    p_trf.add_argument("--use-str",
                   dest = "use_str",
                   default = False,
                   action = "store_true",
                   help = "Add secondary structure features (type defined by --str-mode). Set --use-xxx to define which features to add on top of sequence feature (by default all --in features are used)")
    p_trf.add_argument("--str-mode",
                   dest="str_mode",
                   type = int,
                   default = 1,
                   choices = [1,2,3,4],
                   help = "Define secondary structure feature representation: 1) use probabilities of five structural elements (E,I,H,M,S) 2) same as 1) but encoded as one-hot (element with highest probability gets 1, others 0) 3) use unpaired probabilities 4) same as 3) but encoded as one-hot (default: 1)")
    p_trf.add_argument("--use-add-feat",
                   dest = "use_add_feat",
                   default = False,
                   action = "store_true",
                   help = "Add additional feature annotations. Set --use-xxx to define which features to add on top of sequence feature (by default all --in features are used)")

    # Add advanced model definition arguments group.
    p_tra = p_tr.add_argument_group("model definition arguments")
    # Optional arguments for train.
    p_tra.add_argument("--gen-cv",
                   dest = "gen_cv",
                   default = False,
                   action = "store_true",
                   help = "Run cross validation in combination with set hyperparameters to evaluate model generalization performance (default: False)")
    p_tra.add_argument("--gen-cv-k",
                   dest="gen_cv_k",
                   type = int,
                   default = 10,
                   choices = [5,10],
                   help = "Cross validation k for evaluating generalization performance (use together with --gen-cv) (default: 10)")
    p_tra.add_argument("--gm-cv",
                   dest = "gm_cv",
                   default = False,
                   action = "store_true",
                   help = "Treat data as generic model data (positive IDs with specific format required). This turns on generic model data cross validation, with every fold leaving one RBP set out for testing (ignoring --gen-cv and --gen-cv-k) (default: False)")
    p_tra.add_argument("--val-size",
                   dest = "val_size",
                   type = float,
                   metavar='float',
                   default = 0.2,
                   help = "Validation set size for training final model as percentage of all training sites. NOTE that if --add-test is set, the test set will have the same size (so if --val-size 0.2, train on 60 percent, validate on 20 percent, and test on 20 percent) (default: 0.2)")
    p_tra.add_argument("--add-test",
                   dest="add_test",
                   default = False,
                   action = "store_true",
                   help = "Use a part of the training set as a test set to evaluate final model. Test set size is controlled by --val-size (default: False)")
    p_trm.add_argument("--test-ids",
                   dest="test_ids",
                   type=str,
                   metavar='str',
                   help = "Provide file with test IDs to be used as a test set for testing final model. Test IDs need to be part of --in training set. Not compatible with --add-test, --gen-cv, or --gm-cv")
    p_tra.add_argument("--keep-order",
                   dest="keep_order",
                   default = False,
                   action = "store_true",
                   help = "Use same train-validation(-test) split for each call to train final model. Test split only if --add-test or --test-ids (default: False)")
    p_tra.add_argument("--plot-lc",
                   dest = "plot_lc",
                   default = False,
                   action = "store_true",
                   help = "Plot learning curves (training vs validation loss) for each tested hyperparameter combination (default: False)")
    p_tra.add_argument("--verbose-train",
                   dest = "verbose_train",
                   default = False,
                   action = "store_true",
                   help = "Enable verbose output during model training to show performance over epochs (default: False)")
    p_tra.add_argument("--epochs",
                   dest="epochs",
                   type = int,
                   metavar='int',
                   default = 200,
                   help = "Number of training epochs (default: 200)")
    p_tra.add_argument("--patience",
                   dest="patience",
                   type = int,
                   metavar='int',
                   default = 30,
                   help = "Number of epochs to wait for further improvement on validation set before stopping (default: 30)")
    p_tra.add_argument("--batch-size",
                   dest="batch_size",
                   type = int,
                   metavar='int',
                   default=50,
                   help = "Gradient descent batch size (default: 50)")
    p_tra.add_argument("--lr",
                   dest="learn_rate",
                   type=float,
                   metavar='float',
                   default=0.001,
                   help="Learning rate of optimizer (default: 0.001)")
    p_tra.add_argument("--weight-decay",
                   dest="weight_decay",
                   type=float,
                   metavar='float',
                   default=0.0005,
                   help="Weight decay of optimizer (default: 0.0005)")
    p_tra.add_argument("--n-rnn-layers",
                   dest="n_rnn_layers",
                   type=int,
                   metavar='int',
                   default=1,
                   help="Number of RNN layers (default: 1)")
    p_tra.add_argument("--n-hidden-dim",
                   dest="n_rnn_dim",
                   type = int,
                   metavar='int',
                   default=32,
                   help = "Number of RNN layer dimensions (default: 32)")
    p_tra.add_argument("--dr",
                   dest="dropout_rate",
                   type=float,
                   metavar='float',
                   default=0.5,
                   help="Rate of dropout applied after RNN layers (default: 0.5)")
    p_tra.add_argument("--n-fc-layers",
                   dest="n_fc_layers",
                   type = int,
                   default = 1,
                   choices = [1,2],
                   help = "Number of fully connected layers following RNN layers (default: 1)")
    p_tra.add_argument("--model-type",
                   dest="model_type",
                   type = int,
                   default = 1,
                   choices = [1,2,3,4],
                   help = "RNN model type to use. 1: GRU, 2: LSTM, 3: biGRU, 4: biLSTM (default: 1)")
    p_tra.add_argument("--embed",
                   dest = "embed",
                   default = False,
                   action = "store_true",
                   help = "Use embedding layer for sequence feature, instead of one-hot encoding (default: False)")
    p_tra.add_argument("--embed-dim",
                   dest="embed_dim",
                   type = int,
                   metavar='int',
                   default = 10,
                   help = "Dimension of embedding layer (default: 10)")
    p_tra.add_argument("--run-bohb",
                   dest = "run_bohb",
                   default = False,
                   action = "store_true",
                   help = "Use BOHB to run a hyperparameter optimization. NOTE that this will overwrite set hyperparameters, and trains the final model with the found best hyperparameter setting. ALSO NOTE that this will take some time (!) (default: False)")
    p_tra.add_argument("--bohb-n",
                   dest="bohb_n_iter",
                   type = int,
                   metavar='int',
                   default=50,
                   help = "Number of BOHB iterations (default: 50)")
    p_tra.add_argument("--bohb-min-budget",
                   dest="bohb_min_budget",
                   type = int,
                   metavar='int',
                   default=5,
                   help = "BOHB minimum budget (default: 5)")
    p_tra.add_argument("--bohb-max-budget",
                   dest="bohb_max_budget",
                   type = int,
                   metavar='int',
                   default=20,
                   help = "BOHB maximum budget (default: 20)")
    p_tra.add_argument("--verbose-bohb",
                   dest = "verbose_bohb",
                   default = False,
                   action = "store_true",
                   help = "Enable verbose output for BOHB hyperparameter optimization. By default only warnings are print out (default: False)")

    """
    Model evaluation mode.
    """
    p_ev = subparsers.add_parser('eval',
                                  help='Evaluate properties learned from positive sites')
    p_ev.set_defaults(which='eval')

    # Add required arguments group.
    p_evm = p_ev.add_argument_group("required arguments")
    # Required arguments for eval.
    p_evm.add_argument("--train-in",
                   dest="in_train_folder",
                   type=str,
                   required = True,
                   help = "Input model training folder (output of rnaprot train)")
    p_evm.add_argument("--gt-in",
                   dest="in_gt_folder",
                   type=str,
                   required = True,
                   help = "Input training data folder (output of rnaprot gt)")
    p_evm.add_argument("--out",
                   dest="out_folder",
                   type=str,
                   required = True,
                   help = "Evaluation results output folder")

    # Optional arguments for eval.
    p_ev.add_argument("--nr-top-sites",
                   dest="list_nr_top_sites",
                   type = int,
                   nargs='+',
                   default = [200],
                   help = "Specify number(s) of top predicted sites used for motif extraction. Provide multiple numbers (e.g. --nr-top-sites 100 200 500) to extract one motif plot from each site set (default: 200)")
    p_ev.add_argument("--nr-top-profiles",
                   dest="nr_top_profiles",
                   type = int,
                   default = 25,
                   metavar='int',
                   help = "Specify number of top predicted sites to plot profiles for (default: 25)")
    p_ev.add_argument("--motif-size",
                   dest="list_motif_sizes",
                   type = int,
                   nargs='+',
                   default = [7],
                   help = "Motif size(s) (widths) for extracting and plotting motifs. Provide multiple sizes (e.g. --motif-size 5 7 9 ) to extract a motif for each size (default: 7)")
    p_ev.add_argument("--win-size",
                   dest="win_size",
                   type = int,
                   default = 7,
                   metavar='int',
                   help = "Windows size for calculating position-wise scoring profiles (default: 7)")
    p_ev.add_argument("--lookup-profile",
                   dest="list_lookup_ids",
                   type=str,
                   nargs='+',
                   default=False,
                   help = "Provide site ID(s) for which to plot the feature profile in addition to --nr-top-profiles (e.g. --lookup-profile site_id1 site_id2 ). Site ID needs to be in positive set from --gt-in")
    p_ev.add_argument("--bottoms-up",
                   dest = "bottoms_up",
                   default = False,
                   action = "store_true",
                   help = "Plot bottom profiles and motifs as well (default: False)")
    p_ev.add_argument("--report",
                   dest="report",
                   default = False,
                   action = "store_true",
                   help = "Generate additional statistics and plots regarding k-mer content and scores and also output an .html report containing various statistics and plots (default: False)")
    p_ev.add_argument("--kmer-size",
                   dest="kmer_size",
                   type = int,
                   default = 6,
                   metavar='int',
                   help = "Size (length) of k-mers to extract for --report (default: 5)")
    p_ev.add_argument("--lookup-kmer",
                   dest="lookup_kmer",
                   type=str,
                   metavar='str',
                   help = "Provide RNA k-mer of size --kmer-size to look up and return stats and motif (if found in sequences, no plot if --only-seq)")
    p_ev.add_argument("--add-train-in",
                   dest="add_train_folder",
                   type=str,
                   metavar='str',
                   default = False,
                   help = "Second model training folder (output of rnaprot train) for comparing prediction scores of both models on --gt-in positive dataset. Note that if dataset features of the two models are not identical, comparison might be less informative")
    p_ev.add_argument("--theme",
                   dest="theme",
                   type = int,
                   default = 1,
                   choices = [1,2],
                   help = "Set theme for .html report (1: palm beach, 2: midnight sunset) (default: 1)")
    p_ev.add_argument("--plot-format",
                   dest="plot_format",
                   type = int,
                   default = 1,
                   choices = [1,2],
                   help = "Plotting format of motifs and profiles (does not affect plots generated for --report). 1: png, 2: pdf (default: 1)")

    """
    Binding site prediction mode.
    """
    p_pr = subparsers.add_parser('predict',
                                  help='Predict binding sites (whole sites or profiles)')
    p_pr.set_defaults(which='predict')

    # Add required arguments group.
    p_prm = p_pr.add_argument_group("required arguments")
    # Required arguments for predict.
    p_prm.add_argument("--in",
                   dest="in_folder",
                   type=str,
                   required = True,
                   help = "Input prediction data folder (output of rnaprot gp)")
    p_prm.add_argument("--train-in",
                   dest="in_train_folder",
                   type=str,
                   required = True,
                   help = "Input model training folder containing model file and parameters (output of rnaprot train)")
    p_prm.add_argument("--out",
                   dest="out_folder",
                   type=str,
                   metavar='str',
                   required = True,
                   help = "Prediction results output folder")

    # Optional arguments for predict.
    p_pr.add_argument("--mode",
                   dest="mode",
                   type = int,
                   default = 1,
                   choices = [1,2],
                   help = "Define prediction mode. (1) predict whole sites, (2) predict position-wise scoring profiles and extract top-scoring sites from profiles (default: 1)")
    p_pr.add_argument("--win-ext",
                   dest="win_ext",
                   type = int,
                   metavar='int',
                   default = 20,
                   help = "Window middle position extension on both sides for calculating position-wise scoring profiles. The center position of the window gets the window score assigned. If --seq-ext + --mode 1 was used in rnaprot train, best set --win-ext to --seq-ext value (default: 20)")
    p_pr.add_argument("--thr",
                   dest="sc_thr",
                   type = float,
                   metavar='float',
                   default = 0.5,
                   help = "Minimum profile position score for extracting peak regions and top-scoring sites. Further increase e.g. in case of too many or too broad peaks (default: 0.5)")
    p_pr.add_argument("--max-merge-dist",
                   dest = "max_merge_dist",
                   type = int,
                   metavar='int',
                   default = 0,
                   help = "Maximum distance between two peaks for merging. Two peaks get merged to one if they are <= --max-merge-dist away from each other (default: 0)")

    """
    Training set generation mode.
    """
    p_gt = subparsers.add_parser('gt',
                                  help='Generate training data set')
    p_gt.set_defaults(which='gt')

    # Add required arguments group.
    p_gtm = p_gt.add_argument_group("required arguments")
    # Required arguments for gt.
    p_gtm.add_argument("--in",
                   dest="in_sites",
                   type=str,
                   metavar='str',
                   required = True,
                   help = "Genomic or transcript RBP binding sites file in BED (6-column format) or FASTA format. If --in FASTA, only --str is supported as additional feature. If --in BED, --gtf and --gen become mandatory")
    p_gtm.add_argument("--out",
                   dest="out_folder",
                   type=str,
                   metavar='str',
                   required = True,
                   help = "Output training data folder (== input folder to rnaprot train)")

    # Optional arguments for gt.
    p_gt.add_argument("--gtf",
                   dest="in_gtf",
                   type=str,
                   metavar='str',
                   help = "Genomic annotations GTF file (.gtf or .gtf.gz)")
    p_gt.add_argument("--gen",
                   dest="in_2bit",
                   type=str,
                   metavar='str',
                   help = "Genomic sequences .2bit file")
    p_gt.add_argument("--mode",
                   dest="mode",
                   type = int,
                   default = 1,
                   choices = [1,2,3],
                   help = "Define mode for --in BED site extraction. (1) Take the center of each site, (2) Take the complete site, (3) Take the upstream end for each site. Note that --min-len applies only for --mode 2 (default: 1)")
    p_gt.add_argument("--mask-bed",
                   dest="mask_bed",
                   type=str,
                   metavar='str',
                   help = "Additional BED regions file (6-column format) for masking negatives (e.g. all positive RBP CLIP sites)")
    p_gt.add_argument("--seq-ext",
                   dest = "seq_ext",
                   type = int,
                   metavar='int',
                   default = 30,
                   help = "Up- and downstream sequence extension length of sites (site definition by --mode) (default: 30)")
    p_gt.add_argument("--thr",
                   dest="sc_thr",
                   type = float,
                   metavar='float',
                   default = None,
                   help = "Minimum site score (--in BED column 5) for filtering (assuming higher score == better site) (default: None)")
    p_gt.add_argument("--rev-filter",
                   dest="rev_filter",
                   default = False,
                   action = "store_true",
                   help = "Reverse --thr filtering (i.e. the lower the better, e.g. for p-values) (default: False)")
    p_gt.add_argument("--max-len",
                   dest = "max_len",
                   type = int,
                   metavar='int',
                   default = 300,
                   help = "Maximum length of --in sites (default: 300)")
    p_gt.add_argument("--min-len",
                   dest = "min_len",
                   type = int,
                   metavar='int',
                   default = 21,
                   help = "Minimum length of --in sites (only effective for --mode 2). If length < --min-len, take center and extend to --min-len. Use uneven numbers for equal up- and downstream extension (default: 21)")
    p_gt.add_argument("--keep-ids",
                   dest="keep_ids",
                   default = False,
                   action = "store_true",
                   help = "Keep --in BED column 4 site IDs. Note that site IDs have to be unique (default: False)")
    p_gt.add_argument("--allow-overlaps",
                   dest="allow_overlaps",
                   default = False,
                   action = "store_true",
                   help = "Do not select for highest-scoring sites in case of overlapping sites (default: False)")
    p_gt.add_argument("--no-gene-filter",
                   dest="no_gene_filter",
                   default = False,
                   action = "store_true",
                   help = "Do not filter positives based on gene coverage (gene annotations from --gtf) (default: False)")
    p_gt.add_argument("--neg-comp-thr",
                   dest="neg_comp_thr",
                   type = float,
                   metavar='float',
                   default = 0.5,
                   help = "Sequence complexity (Shannon entropy) threshold for filtering random negative regions (default: 0.5)")
    p_gt.add_argument("--neg-factor",
                   dest="neg_factor",
                   type = int,
                   default = 2,
                   choices = [2,3,4,5],
                   help = "Determines number of initial random negatives to be extracted (== --neg-factor n times # positives) (default: 2)")
    p_gt.add_argument("--keep-add-neg",
                   dest="keep_add_neg",
                   default = False,
                   action = "store_true",
                   help = "Keep additional negatives (# controlled by --neg-factor) instead of outputting same numbers of positive and negative sites (default: False)")
    p_gt.add_argument("--neg-in",
                   dest="in_neg_sites",
                   type=str,
                   metavar='str',
                   help = "Negative genomic or transcript sites in BED (6-column format) or FASTA format (unique IDs required). Use with --in BED/FASTA. If not set, negatives are generated by shuffling --in sequences (if --in FASTA) or random selection of genomic or transcript sites (if --in BED)")
    p_gt.add_argument("--shuffle-k",
                   dest="shuffle_k",
                   type = int,
                   default = 2,
                   choices = [1,2,3],
                   help = "Supply k for k-nucleotide shuffling of --in sequences to generate negative sequences (if no --neg-fa supplied) (default: 2)")
    p_gt.add_argument("--report",
                   dest="report",
                   default = False,
                   action = "store_true",
                   help = "Output an .html report providing various training set statistics and plots (default: False)")
    p_gt.add_argument("--theme",
                   dest="theme",
                   type = int,
                   default = 1,
                   choices = [1,2],
                   help = "Set theme for .html report (1: palm beach, 2: midnight sunset) (default: 1)")

    # Add additional feature arguments group.
    p_gta = p_gt.add_argument_group("additional annotation arguments")
    p_gta.add_argument("--eia",
                   dest="exon_intron_annot",
                   default = False,
                   action = "store_true",
                   help = "Add exon-intron annotations to genomic regions (default: False)")
    p_gta.add_argument("--eia-ib",
                   dest="intron_border_annot",
                   default = False,
                   action = "store_true",
                   help = "Add intron border annotations to genomic regions (in combination with --eia) (default: False)")
    p_gta.add_argument("--eia-n",
                   dest="exon_intron_n",
                   default = False,
                   action = "store_true",
                   help = "Label regions not covered by intron or exon regions as N instead of labelling them as introns (I) (in combination with --eia) (default: False)")
    p_gta.add_argument("--eia-all-ex",
                   dest="eia_all_ex",
                   default = False,
                   action = "store_true",
                   help = "Use all annotated exons in --gtf file, instead of exons of most prominent transcripts or exon defined by --tr-list. Set this and --tr-list will be effective only for --tra (default: False)")
    p_gta.add_argument("--tr-list",
                   dest="tr_list",
                   type=str,
                   metavar='str',
                   help = "Supply file with transcript IDs (one ID per row) for exon-intron labeling (using the corresponding exon regions from --gtf). By default, exon regions of the most prominent transcripts (automatically selected from --gtf) are used (default: False)")
    p_gta.add_argument("--phastcons",
                   dest="pc_bw",
                   type=str,
                   metavar='str',
                   help = "Genomic .bigWig file with phastCons conservation scores to add as annotations")
    p_gta.add_argument("--phylop",
                   dest="pp_bw",
                   type=str,
                   metavar='str',
                   help = "Genomic .bigWig file with phyloP conservation scores to add as annotations")
    p_gta.add_argument("--tra",
                   dest="tr_reg_annot",
                   default = False,
                   action = "store_true",
                   help = "Add transcript region annotations (5'UTR, CDS, 3'UTR, None) to genomic and transcript regions (default: False)")
    p_gta.add_argument("--tra-codons",
                   dest="tr_reg_codon_annot",
                   default = False,
                   action = "store_true",
                   help = "Add start and stop codon annotations to genomic or transcript regions (in combination with --tra) (default: False)")
    p_gta.add_argument("--tra-borders",
                   dest="tr_reg_border_annot",
                   default = False,
                   action = "store_true",
                   help = "Add transcript and exon border annotations to transcript regions (in combination with --tra) (default: False)")
    p_gta.add_argument("--rra",
                   dest="rep_reg_annot",
                   default = False,
                   action = "store_true",
                   help = "Add repeat region annotations for genomic or transcript regions retrieved from --gen .2bit (default: False)")
    p_gta.add_argument("--str",
                   dest="add_str",
                   default = False,
                   action = "store_true",
                   help = "Add secondary structure probabilities features (calculate with RNAplfold) (default: False)")
    p_gta.add_argument("--plfold-u",
                   dest="plfold_u",
                   type = int,
                   metavar='int',
                   default = 3,
                   help = "RNAplfold -u parameter value (default: 3)")
    p_gta.add_argument("--plfold-l",
                   dest="plfold_l",
                   type = int,
                   metavar='int',
                   default = 100,
                   help = "RNAplfold -L parameter value (default: 100)")
    p_gta.add_argument("--plfold-w",
                   dest="plfold_w",
                   type = int,
                   metavar='int',
                   default = 150,
                   help = "RNAplfold -W parameter value (default: 150)")
    p_gta.add_argument("--feat-in",
                   dest="feat_in",
                   type=str,
                   metavar='str',
                   help = "Provide tabular file with additional position-wise genomic region features (infos and paths to BED files) to add")
    p_gta.add_argument("--feat-in-1h",
                   dest="feat_in_1h",
                   default = False,
                   action = "store_true",
                   help = "Use one-hot encoding for all additional position-wise features from --feat-in table, ignoring type definitions in --feat-in table (default: False)")
    p_gta.add_argument("--feat-in-norm",
                   dest="feat_in_norm",
                   default = False,
                   action = "store_true",
                   help = "Normalize feature values (min-max normalization (0..1) of --feat-in features. --feat-in table column 3 information further controls feature normalization (default: False)")

    """
    Test / prediction set generation mode.
    """
    p_gp = subparsers.add_parser('gp',
                                  help='Generate prediction data set')
    p_gp.set_defaults(which='gp')

    # Add required arguments group.
    p_gpm = p_gp.add_argument_group("required arguments")
    # Required arguments for gp.
    p_gpm.add_argument("--in",
                   dest="in_sites",
                   type=str,
                   metavar='str',
                   required = True,
                   help = "Genomic or transcript RBP binding sites file in BED (6-column format) or FASTA format. If --in FASTA, only --str is supported as additional feature. If --in BED, --gtf and --gen become mandatory")
    p_gpm.add_argument("--train-in",
                   dest="in_train_folder",
                   type=str,
                   metavar='str',
                   required = True,
                   help = "Training input folder (output folder of rnaprot train) to extract the same features for --in sites which were used to train the model (info stored in --train-in folder)")
    p_gpm.add_argument("--out",
                   dest="out_folder",
                   type=str,
                   metavar='str',
                   required = True,
                   help = "Output prediction dataset folder (== input folder to rnaprot predict)")

    # Optional arguments for gp.
    p_gp.add_argument("--gtf",
                   dest="in_gtf",
                   type=str,
                   metavar='str',
                   help = "Genomic annotations GTF file (.gtf or .gtf.gz)")
    p_gp.add_argument("--gen",
                   dest="in_2bit",
                   type=str,
                   metavar='str',
                   help = "Genomic sequences .2bit file")
    p_gp.add_argument("--mode",
                   dest="mode",
                   type = int,
                   default = 2,
                   choices = [1,2,3],
                   help = "Define mode for --in BED site extraction. (1) Take the center of each site, (2) Take the complete site, (3) Take the upstream end for each site. Use --seq-ext to extend center sites again (default: 2)")
    p_gp.add_argument("--seq-ext",
                   dest = "seq_ext",
                   type = int,
                   metavar='int',
                   default = False,
                   help = "Up- and downstream sequence extension length of --in sites (if --in BED, site definition by --mode) (default: False)")
    p_gp.add_argument("--gene-filter",
                   dest="gene_filter",
                   default = False,
                   action = "store_true",
                   help = "Filter --in sites based on gene coverage (gene annotations from --gtf) (default: False)")
    p_gp.add_argument("--report",
                   dest="report",
                   default = False,
                   action = "store_true",
                   help = "Output an .html report providing various training set statistics and plots (default: False)")
    p_gp.add_argument("--theme",
                   dest="theme",
                   type = int,
                   default = 1,
                   choices = [1,2],
                   help = "Set theme for .html report (1: palm beach, 2: midnight sunset) (default: 1)")
    # Add additional feature arguments group.
    p_gpa = p_gp.add_argument_group("additional annotation arguments")
    p_gpa.add_argument("--tr-list",
                   dest="tr_list",
                   type=str,
                   metavar='str',
                   help = "Supply file with transcript IDs (one ID per row) for exon-intron labeling (using the corresponding exon regions from --gtf). By default, exon regions of the most prominent transcripts (automatically selected from --gtf) are used (default: False)")
    p_gpa.add_argument("--eia-all-ex",
                   dest="eia_all_ex",
                   default = False,
                   action = "store_true",
                   help = "Use all annotated exons in --gtf file, instead of exons of most prominent transcripts or exon defined by --tr-list. Set this and --tr-list will be effective only for --tra. NOTE that by default --eia-all-ex is disabled, even if --train-in model was trained with --eia-all-ex (default: False)")
    p_gpa.add_argument("--phastcons",
                   dest="pc_bw",
                   type=str,
                   metavar='str',
                   help = "Genomic .bigWig file with phastCons conservation scores to add as annotations")
    p_gpa.add_argument("--phylop",
                   dest="pp_bw",
                   type=str,
                   metavar='str',
                   help = "Genomic .bigWig file with phyloP conservation scores to add as annotations")
    p_gpa.add_argument("--feat-in",
                   dest="feat_in",
                   type=str,
                   metavar='str',
                   help = "Provide tabular file with additional position-wise genomic region features (infos and paths to BED files) to add. BE SURE to use the same file as used for generating the training dataset (rnaprot gt --feat-in) for training the model from --train-in!")
    return p


################################################################################

def main_train(args):
    """
    Train a rnaprot model.

    """

    print("Running for you in TRAIN mode ... ")

    # Checks.
    assert os.path.exists(args.in_folder), "--in folder does not exist"
    indiv_feat_check = args.use_pc_con + args.use_pp_con + args.use_eia + args.use_tra + args.use_rra + args.use_str + args.use_add_feat
    if indiv_feat_check and args.only_seq:
        assert False, "--only-seq useless in combination with individually selected features (--use-xxx)"
    if args.add_test:
        assert args.val_size >= 0.1 and args.val_size <= 0.3, "use reasonable values for --val-size in combination with --add-test (>= 0.1 and <= 0.3)"
        assert not args.test_ids, "use either --add-test to automatically select a test set for final model testing, or supply specified test set IDs via --test-ids to test final model"
    else:
        assert args.val_size >= 0.1 and args.val_size <= 0.5, "use reasonable values for --val-size (>= 0.1 and <= 0.5)"
    if args.gm_cv and args.gen_cv:
        assert False, "use either --gen-cv to estimate single model generalization performance, or --gm-cv to estimate generic model generalization performance"
    if args.gen_cv:
        assert not args.run_bohb, "incompatible options --run-bohb and --gen-cv. Use either --run-bohb to optimize hyperparameters, or --gen-cv to use the set of optimized parameters (provided as command line arguments) to estimate generalization performance via cross validation"
        assert not args.keep_order, "incompatible options --keep-order and --gen-cv. --keep-order is only effective for final model training (--gen-cv disabled)"
        assert not args.add_test, "incompatible options --add-test and --gen-cv. --add-test only for training final model, not for --gen-cv cross validation"
        assert not args.test_ids, "incompatible options --test-ids and --gen-cv. --test-ids only for training final model, not for --gen-cv cross validation"
    if args.gm_cv:
        assert not args.run_bohb, "incompatible options --run-bohb and --gm-cv. Use either --run-bohb to optimize hyperparameters, or --gm-cv to apply the set of optimized parameters (provided as command line arguments) to estimate generic model performance via cross validation"
        assert not args.keep_order, "incompatible options --keep-order and --gm-cv. --keep-order is only effective for final model training (--gm-cv disabled)"
        assert not args.add_test, "incompatible options --add-test and --gm-cv. --add-test only for training final model, not for --gm-cv cross validation"
        assert not args.test_ids, "incompatible options --test-ids and --gm-cv. --test-ids only for training final model, not for --gm-cv cross validation"

    assert args.epochs >= 1 and args.epochs <= 500, "use reasonable number for --epochs (>= 1 and <= 500)"
    assert args.patience > 0 and args.patience <= 100, "use reasonable number for --patience (> 0 and <= 100)"

    # Get additional infos from rnaprot gt settings.
    gt_settings_file = args.in_folder + "/settings.rnaprot_gt.out"
    assert os.path.exists(gt_settings_file), "rnaprot gt settings file %s not found" %(gt_settings_file)
    gt_settings_dic = {}
    with open(gt_settings_file) as f:
        for line in f:
            cols = line.strip().split("\t")
            gt_settings_dic[cols[0]] = cols[1]
    f.closed
    # Settings to port from gt to train settings file.
    assert "in_gtf" in gt_settings_dic, "in_gtf info missing in --in settings file %s" %(gt_settings_file)
    assert "in_2bit" in gt_settings_dic, "in_2bit info missing in --in settings file %s" %(gt_settings_file)
    assert "tr_list" in gt_settings_dic, "tr_list info missing in --in settings file %s" %(gt_settings_file)
    assert "pc_bw" in gt_settings_dic, "pc_bw info missing in --in settings file %s" %(gt_settings_file)
    assert "pp_bw" in gt_settings_dic, "pp_bw info missing in --in settings file %s" %(gt_settings_file)
    assert "feat_in" in gt_settings_dic, "feat_in info missing in --in settings file %s" %(gt_settings_file)
    assert "feat_in_1h" in gt_settings_dic, "feat_in_1h info missing in --in settings file %s" %(gt_settings_file)
    assert "feat_in_norm" in gt_settings_dic, "feat_in_norm info missing in --in settings file %s" %(gt_settings_file)
    assert "plfold_u" in gt_settings_dic, "plfold_u info missing in --in settings file %s" %(gt_settings_file)
    assert "plfold_l" in gt_settings_dic, "plfold_l info missing in --in settings file %s" %(gt_settings_file)
    assert "plfold_w" in gt_settings_dic, "plfold_w info missing in --in settings file %s" %(gt_settings_file)
    assert "exon_intron_annot" in gt_settings_dic, "exon_intron_annot info missing in --in settings file %s" %(gt_settings_file)
    assert "intron_border_annot" in gt_settings_dic, "intron_border_annot info missing in --in settings file %s" %(gt_settings_file)
    assert "exon_intron_n" in gt_settings_dic, "exon_intron_n info missing in --in settings file %s" %(gt_settings_file)
    assert "eia_all_ex" in gt_settings_dic, "eia_all_ex info missing in --in settings file %s" %(gt_settings_file)
    assert "tr_reg_annot" in gt_settings_dic, "tr_reg_annot info missing in --in settings file %s" %(gt_settings_file)
    assert "tr_reg_codon_annot" in gt_settings_dic, "tr_reg_codon_annot info missing in --in settings file %s" %(gt_settings_file)
    assert "tr_reg_border_annot" in gt_settings_dic, "tr_reg_border_annot info missing in --in settings file %s" %(gt_settings_file)
    args.in_gtf = gt_settings_dic["in_gtf"]
    args.in_2bit = gt_settings_dic["in_2bit"]
    args.tr_list = gt_settings_dic["tr_list"]
    args.pc_bw = gt_settings_dic["pc_bw"]
    args.pp_bw = gt_settings_dic["pp_bw"]
    args.feat_in = gt_settings_dic["feat_in"]
    args.feat_in_1h = gt_settings_dic["feat_in_1h"]
    args.feat_in_norm = gt_settings_dic["feat_in_norm"]
    args.plfold_u = gt_settings_dic["plfold_u"]
    args.plfold_l = gt_settings_dic["plfold_l"]
    args.plfold_w = gt_settings_dic["plfold_w"]
    args.exon_intron_annot = gt_settings_dic["exon_intron_annot"]
    args.intron_border_annot = gt_settings_dic["intron_border_annot"]
    args.exon_intron_n = gt_settings_dic["exon_intron_n"]
    args.eia_all_ex = gt_settings_dic["eia_all_ex"]
    args.tr_reg_annot = gt_settings_dic["tr_reg_annot"]
    args.tr_reg_codon_annot = gt_settings_dic["tr_reg_codon_annot"]
    args.tr_reg_border_annot = gt_settings_dic["tr_reg_border_annot"]

    # Embedding (by default True).
    args.embed_k = 1
    args.embed_vocab_size = 4**args.embed_k + 1
    kmer2idx_dic = False
    if args.embed:
        kmer2idx_dic = rplib.get_kmer_dic(args.embed_k, rna=True, fill_idx=True)

    # Number of classes.
    args.n_class = 1

    # RNN type + bidirectional switch.
    args.bidirect = False
    args.rnn_type = 1
    if args.model_type == 2 or args.model_type == 4:
        args.rnn_type = 2
    if args.model_type == 3 or args.model_type == 4:
        args.bidirect = True

    # Additional FC layer.
    args.add_fc_layer = False
    if args.n_fc_layers == 2:
        args.add_fc_layer = True

    # Generate results output folder.
    out_folder = args.out_folder
    if not os.path.exists(out_folder):
        os.makedirs(out_folder)
    in_folder = args.in_folder

    print("Training set input folder:     %s" %(in_folder))
    print("Model training output folder:  %s" %(out_folder))

    # Output mode settings.
    settings_file = out_folder + "/settings.rnaprot_train.out"
    SETOUT = open(settings_file, "w")
    for arg in vars(args):
        SETOUT.write("%s\t%s\n" %(arg, str(getattr(args, arg))))
    SETOUT.close()

    # Class label to RBP label dictionary.
    li2label_dic = {}

    # Feature info dictionary.
    feat_info_dic = {}

    # Load training data.
    seqs_dic, idx2id_dic, labels, all_features = rplib.load_training_data(args,
                                                           kmer2idx_dic=kmer2idx_dic,
                                                           store_tensors=True,
                                                           feat_info_dic=feat_info_dic,
                                                           li2label_dic=li2label_dic)

    # Number of features (feature vector length).
    n_feat = len(all_features[0][0])
    assert n_feat, "invalid number of features (%s)" %(n_feat)
    args.n_feat = n_feat
    # Additional features switch.
    args.add_feat = False
    if args.embed:
        if args.n_feat > 1:
            args.add_feat = True
    else:
        if args.n_feat > 4:
            args.add_feat = True
    if args.add_feat:
        print("Training model using additional features ... ")

    # Reverse mapping.
    id2idx_dic = {}
    for idx in idx2id_dic:
        seq_id = idx2id_dic[idx]
        id2idx_dic[seq_id] = idx

    # Check for CUDA support.
    if torch.cuda.is_available():
        print("CUDA: I'm available. Using GPU ... ")
    else:
        print("CUDA: I'm NOT available. Using CPU ... ")

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    print("# dataset features:  %i" %(n_feat))
    print("Dataset size:        %i" %(len(all_features)))

    """
    Run cross validation (--gen-cv, --gen-cv-k) to estimate generalization
    performance.

    """

    if args.gen_cv:

        print("Running CV to estimate generalization performance (--gen-cv) ... ")

        # Path to store CV models.
        model_folder = out_folder + "/cv_models/"
        if not os.path.exists(model_folder):
            os.makedirs(model_folder)

        # CV results (accuracies) file.
        cv_results_file = out_folder + "/cv_results.out"
        if os.path.exists(cv_results_file):
            os.remove(cv_results_file)

        # Shuffle.
        random.seed(1)
        labels, all_features = rplib.shuffle_idx_feat_labels(labels, all_features,
                                                             idx2id_dic=idx2id_dic)

        # Split datasets into folds.
        skf = StratifiedKFold(n_splits=args.gen_cv_k, shuffle=False)
        acc_list = []
        auc_list = []
        # Fold count.
        i_cv = 0

        # Switch for always using validation set (even with just one HPC).
        run_hpo_gen_cv = run_hpo
        gen_cv_always_val = True
        if gen_cv_always_val:
            run_hpo_gen_cv = True

        # Run --gen-cv-k fold cross validation with hyperparameter optimization.
        for list_tr_idx, list_te_idx in skf.split(np.zeros(len(labels)), labels):

            i_cv += 1
            print("Starting --gen-cv fold %i ... " %(i_cv))

            test_dataset = RNNDataset([all_features[idx] for idx in list_te_idx], [labels[idx] for idx in list_te_idx])
            if run_hpo_gen_cv:
                # Further split up train portion into train + validation.
                # Training set == all list entries except last length(test_set).
                train_dataset = RNNDataset([all_features[idx] for idx in list_tr_idx[:-len(list_te_idx)]], [labels[idx] for idx in list_tr_idx[:-len(list_te_idx)]])
                # Validation set == last length(test_set) entries.
                val_dataset = RNNDataset([all_features[idx] for idx in list_tr_idx[-len(list_te_idx):]], [labels[idx] for idx in list_tr_idx[-len(list_te_idx):]])
            else:
                train_dataset = RNNDataset([all_features[idx] for idx in list_tr_idx], [labels[idx] for idx in list_tr_idx])

            # Batch loaders.
            train_loader = DataLoader(dataset=train_dataset, batch_size=args.batch_size, collate_fn=rnn_util.pad_collate, pin_memory=True)
            val_loader = DataLoader(dataset=val_dataset, batch_size=args.batch_size, collate_fn=rnn_util.pad_collate, pin_memory=True)
            test_loader = DataLoader(dataset=test_dataset, batch_size=args.batch_size, collate_fn=rnn_util.pad_collate, pin_memory=True)

            # Report set sizes.
            c_train = len(list_tr_idx) - len(list_te_idx)
            c_test = len(list_te_idx)
            c_val = len(list_te_idx)
            print("# training instances:    %i" %(c_train))
            print("# validation instances:  %i" %(c_val))
            print("# test instances:        %i" %(c_test))

            # Plot learning curves.
            plot_lc_folder = False
            if args.plot_lc:
                plot_lc_folder = model_folder + "/lc_plots_%i" %(i_cv)

            # Model path.
            model_path = model_folder + "/gen_cv.model"

            # Train model on fold dataset.
            run_id = "cv_fold_%i" %(i_cv)
            val_auc, val_acc, c_epochs = rnn_util.train_model(args, train_loader,
                                                              val_loader, model_path,
                                                              device, criterion,
                                                              run_id=run_id,
                                                              verbose=args.verbose_train,
                                                              plot_lc_folder=plot_lc_folder)

            # Get model test set performance.
            test_auc, test_acc = rnn_util.test_model(args, test_loader, model_path,
                                                     device, criterion)

            # Store test ACC and AUC.
            acc_list.append(acc)
            auc_list.append(auc)

            # Report fold ACC+AUC.
            print("Model test accuracy:  %f" %(acc))
            print("Model test AUC:       %f" %(auc))

        # Output CV results by round.
        CVROUT = open(cv_results_file, "w")
        CVROUT.write("fold\tACC\tAUC\n")
        for idx in range(0, i_cv):
            cv_round = idx + 1
            round_acc = acc_list[cv_round]
            round_auc = auc_list[cv_round]
            CVROUT.write("%i\t%f\t%f\n" %(cv_round, round_acc, round_auc))
        CVROUT.close()

        # Report average ACC / AUC with stdev.
        avg_acc = statistics.mean(acc_list)
        stdev_acc = statistics.stdev(acc_list)
        avg_auc = statistics.mean(auc_list)
        stdev_auc = statistics.stdev(auc_list)

        print("Average accuracy + stdev (--gen-cv):  %f (+- %f)" %(avg_acc, stdev_acc))
        print("Average AUC + stdev (--gen-cv):       %f (+- %f)" %(avg_auc, stdev_auc))

        avg_acc_out_file = args.out_folder + "/avg_acc_auc.gen_cv.out"
        #AAOUT = open(avg_acc_out_file, "a+")
        AAOUT = open(avg_acc_out_file, "w")
        AAOUT.write("avg_acc\tstdev_acc\tavg_auc\tstdev_auc\n")
        AAOUT.write("%f\t%f\t%f\t%f\n" %(avg_acc, stdev_acc, avg_auc, stdev_auc))
        AAOUT.close()

        sys.exit()

    """
    Run cross validation to estimate generalzation performance of generic
    model.

    """
    if args.gm_cv:

        print("Run CV to estimate generic model generalization performance (--gm-cv) ... ")

        print("# of distinct RBP sets (== # of splits):  %i" %(len(li2label_dic)))

        # Path to store generic CV models.
        model_folder = out_folder + "/generic_cv_models/"
        if not os.path.exists(model_folder):
            os.makedirs(model_folder)
        # CV results (accuracies) file.
        cv_results_file = out_folder + "/generic_cv_results.out"
        if os.path.exists(cv_results_file):
            os.remove(cv_results_file)

        acc_list = []
        auc_list = []
        rbp_label_list = []
        i_cv = 0

        all_pos_feat = []
        all_neg_feat = []
        all_pos_feat_labels = []

        for idx,feat in enumerate(all_features):
            feat_label = labels[idx]
            if feat_label == 0:
                all_neg_feat.append(feat)
            else:
                all_pos_feat.append(feat)
                all_pos_feat_labels.append(feat_label)

        random.seed(1)
        random.shuffle(all_neg_feat)

        list_pos_feat_labels = [l.item() for l in all_pos_feat_labels]
        list_uniq_pos_feat_labels = list(set(list_pos_feat_labels))

        for pos_feat_label in list_uniq_pos_feat_labels:
            #all_pos_feat_temp = copy.deepcopy(all_pos_feat)
            train_feat_pos = []
            test_feat_pos = []
            for idx,feat in enumerate(all_pos_feat):
                feat_label = labels[idx]
                if feat_label == pos_feat_label:
                    test_feat_pos.append(feat)
                else:
                    train_feat_pos.append(feat)

            # Get RBP label.
            assert li2label_dic[pos_feat_label], "No RBP label for positive class label %i" %(pos_feat_label)
            rbp_label = li2label_dic[pos_feat_label]

            i_cv += 1
            print("Starting --gm-cv fold %i (test set = %s) ... " %(i_cv, rbp_label))

            n_test_feat_pos = len(test_feat_pos)
            train_feat_neg = all_neg_feat[:-n_test_feat_pos]
            test_feat_neg = all_neg_feat[-n_test_feat_pos:]

            all_train_feat = train_feat_pos + train_feat_neg
            test_feat = test_feat_pos + test_feat_neg
            all_train_feat_labels = [1]*len(train_feat_pos) + [0]*len(train_feat_neg)
            test_feat_labels = [1]*len(test_feat_pos) + [0]*len(test_feat_neg)

            # Shuffle features.
            all_train_feat_labels, all_train_feat = rplib.shuffle_idx_feat_labels(all_train_feat_labels, all_train_feat)
            test_feat_labels, test_feat = rplib.shuffle_idx_feat_labels(test_feat_labels, test_feat)

            n_test_feat = len(test_feat)
            train_feat = all_train_feat[:-n_test_feat]
            val_feat = all_train_feat[-n_test_feat:]
            train_feat_labels = all_train_feat_labels[:-n_test_feat]
            val_feat_labels = all_train_feat_labels[-n_test_feat:]

            train_dataset = RNNDataset(train_feat, train_feat_labels)
            test_dataset = RNNDataset(test_feat, test_feat_labels)
            val_dataset = RNNDataset(val_feat, val_feat_labels)

            # B*tch loaders.
            train_loader = DataLoader(dataset=train_dataset, batch_size=args.batch_size, collate_fn=rnn_util.pad_collate, pin_memory=True)
            val_loader = DataLoader(dataset=val_dataset, batch_size=args.batch_size, collate_fn=rnn_util.pad_collate, pin_memory=True)
            test_loader = DataLoader(dataset=test_dataset, batch_size=args.batch_size, collate_fn=rnn_util.pad_collate, pin_memory=True)

            # Plot learning curves: yes [] no [] maybe [].
            plot_lc_folder = False
            if args.plot_lc:
                plot_lc_folder = model_folder + "/lc_plots_%i" %(i_cv)

            # Model path to luck.
            model_path = model_folder + "/gen_cv.model"

            # Train model on fold dataset.
            run_id = "cv_fold_%i" %(i_cv)
            val_auc, val_acc, c_epochs = rnn_util.train_model(args, train_loader,
                                                              val_loader, model_path,
                                                              device, criterion,
                                                              run_id=run_id,
                                                              verbose=args.verbose_train,
                                                              plot_lc_folder=plot_lc_folder)

            # Get model test set performance.
            test_auc, test_acc = rnn_util.test_model(args, test_loader, model_path,
                                                     device, criterion)

            # Store test ACC and AUC.
            acc_list.append(acc)
            auc_list.append(auc)
            rbp_label_list.append(rbp_label)

            # Report fold ACC+AUC.
            print("Generic model test accuracy:  %f" %(acc))
            print("Generic model test AUC:       %f" %(auc))

        # Output CV results by round.
        CVROUT = open(cv_results_file, "w")
        CVROUT.write("fold\tRBP\tACC\tAUC\n")
        for idx in range(0, i_cv):
            cv_round = idx + 1
            rbp_label = rbp_label_list[cv_round]
            round_acc = acc_list[cv_round]
            round_auc = auc_list[cv_round]
            CVROUT.write("%i\t%s\t%f\t%f\n" %(cv_round, rbp_label, round_acc, round_auc))
        CVROUT.close()

        # Report average accuracy with stdev.
        avg_acc = statistics.mean(acc_list)
        stdev_acc = statistics.stdev(acc_list)
        avg_auc = statistics.mean(auc_list)
        stdev_auc = statistics.stdev(auc_list)
        print("Average generic model accuarcy (+- stdev):  %f (+- %f)" %(avg_acc, stdev_acc))
        print("Average generic model AUC (+- stdev):       %f (+- %f)" %(avg_auc, stdev_auc))

        avg_acc_auc_out_file = args.out_folder + "/avg_acc_auc.gm_cv.out"
        #AAOUT = open(avg_acc_out_file, "a+")
        AAOUT = open(avg_acc_auc_out_file, "w")
        AAOUT.write("avg_acc\tstdev_acc\tavg_auc\tstdev_auc\n")
        AAOUT.write("%f\t%f\t%f\t%f\n" %(avg_acc, stdev_acc, avg_auc, stdev_auc))
        AAOUT.close()

        sys.exit()

    """
    Train final model.

    Do this by taking the whole training set, then split into train and
    validation set (validation set size can be set by --val-size).
    Use either the specified hyperparameters (via command line), or
    run BOHB hyperparameter optimization with --run-bohb. If --run-bohb
    is set, the final model will be trained using the optimized
    hyperparameters returned by Mr BOHB himself.
    NOTE that if --add-test, --val-size determines size of extra test set.

    """

    # Path to store final models.
    model_folder = out_folder + "/final_model/"
    if not os.path.exists(model_folder):
        os.makedirs(model_folder)

    final_model_path = out_folder + "/" + "final.model"
    final_params_path = out_folder + "/" + "final.params"

    """
    In case we have test set IDs provided via --test-ids, seperate the
    training set (labels, all_features) into test set and new train set
    for training and evaluating final model.

    """

    # Read in test IDs for testing final model.
    test_ids_dic = {}
    list_test_features = []
    list_test_labels = []
    test_idx2id_dic = {}
    # Temp store train features.
    train_features = []
    train_labels = []
    train_idx2id_dic = {}

    if args.test_ids:
        print("Test IDs provided via --test-ids. Split train into test-train ... ")
        # Read in test IDs.
        test_ids_dic = rplib.read_ids_into_dic(args.test_ids)
        # Check if IDs in training set.
        for seq_id in test_ids_dic:
            assert seq_id in seqs_dic, "--test-ids ID %s not in training set!" %(seq_id)
        # Make new test / train set.
        test_idx = 0
        train_idx = 0
        for seq_id in id2idx_dic:
            old_idx = id2idx_dic[seq_id]
            label = labels[old_idx]
            if seq_id in test_ids_dic:
                list_test_features.append(all_features[old_idx])
                list_test_labels.append(label)
                test_idx2id_dic[test_idx] = seq_id
                test_idx += 1
            else:
                train_features.append(all_features[old_idx])
                train_labels.append(label)
                train_idx2id_dic[train_idx] = seq_id
                train_idx += 1
        # Overwrite old train set.
        idx2id_dic = train_idx2id_dic
        all_features = train_features
        labels = train_labels
        # Reverse mapping again.
        id2idx_dic = {}
        for idx in idx2id_dic:
            seq_id = idx2id_dic[idx]
            id2idx_dic[seq_id] = idx
        c_train = len(labels)
        c_test = len(list_test_labels)
        print("Test set (--test-ids) size:  %i" %(c_test))
        print("New training set size:       %i" %(c_train))

    # Keep order of splits?
    random_state = None
    if args.keep_order:
        random_state = 1

    val_set_perc = args.val_size * 100
    if args.add_test:
        print("Using %.1f" %(val_set_perc) + '% ' + " for validation and testing ... ")

    else:
        print("Using %.1f" %(val_set_perc) + '% ' + " for validation ... ")

    # Make one train-validation split.
    sss = StratifiedShuffleSplit(n_splits=1, test_size=args.val_size, random_state=random_state)

    list_train_features = []
    list_train_labels = []
    list_val_features = []
    list_val_labels = []

    for list_tr_idx, list_val_idx in sss.split(np.zeros(len(labels)), labels):
        list_test_idx = [] # only populated if --add-test.
        n_val_idx = len(list_val_idx)
        list_new_tr_idx = list_tr_idx
        if args.add_test:
            list_test_idx = list_tr_idx[-n_val_idx:]
            list_new_tr_idx = list_tr_idx[:-n_val_idx]
        for idx,label in enumerate(labels):
            if idx in list_test_idx:
                list_test_features.append(all_features[idx])
                list_test_labels.append(labels[idx])
            elif idx in list_val_idx:
                list_val_features.append(all_features[idx])
                list_val_labels.append(labels[idx])
            else:
                list_train_features.append(all_features[idx])
                list_train_labels.append(labels[idx])
        if args.verbose_train:
            print("Train set indices:      ", list_new_tr_idx)
            print("Validation set indices: ", list_val_idx)
            if args.add_test:
                print("Test set indices:       ", list_test_idx)

    # Joint shuffle label + feature lists.
    #random.seed(1)
    random_seed = 1
    list_train_labels, list_train_features = rplib.shuffle_idx_feat_labels(list_train_labels, list_train_features,
                                                            random_seed=random_seed)
    list_val_labels, list_val_features = rplib.shuffle_idx_feat_labels(list_val_labels, list_val_features,
                                                            random_seed=random_seed)
    train_dataset = RNNDataset(list_train_features, list_train_labels)
    val_dataset = RNNDataset(list_val_features, list_val_labels)
    c_train = len(list_train_labels)
    c_val = len(list_val_labels)
    print("# training instances:    %i" %(c_train))
    print("# validation instances:  %i" %(c_val))
    if args.add_test or args.test_ids:
        list_test_labels, list_test_features = rplib.shuffle_idx_feat_labels(list_test_labels, list_test_features,
                                                            random_seed=random_seed)
        test_dataset = RNNDataset(list_test_features, list_test_labels)
        c_test = len(list_test_labels)
        print("# test instances:        %i" %(c_test))

    """
    If --run-bohb is set, run BOHB (part of HpBandSter package) to get
    optimized parameters.

    bohb_n_iter:
        Number of iterations performed by BOHB, controlling how many
        HP configurations are evaluated.
    bohb_min_budget:
        Minimum number of epochs for training one HP configuration.
    bohb_max_budget:
        Maximum number of epochs for training one HP configuration.

    For debugging:
    Use tiny budgets, one iteration, and set logging level to DEBUG,
    i.e., verbose_bohb=True

    """

    # To store HPs for final model.
    opt_dic = {}
    opt_dic["batch_size"] = args.batch_size
    opt_dic["n_rnn_dim"] = args.n_rnn_dim
    opt_dic["n_rnn_layers"] = args.n_rnn_layers
    opt_dic["weight_decay"] = args.weight_decay
    opt_dic["learn_rate"] = args.learn_rate
    opt_dic["dropout_rate"] = args.dropout_rate
    opt_dic["bidirect"] = args.bidirect
    opt_dic["add_fc_layer"] = args.add_fc_layer
    opt_dic["rnn_type"] = args.rnn_type
    opt_dic["embed"] = args.embed
    opt_dic["embed_dim"] = args.embed_dim
    opt_dic["embed_vocab_size"] = args.embed_vocab_size
    opt_dic["n_feat"] = args.n_feat
    opt_dic["n_class"] = args.n_class
    opt_dic["add_feat"] = args.add_feat

    if args.run_bohb:

        # BOHB output folder.
        bohb_out_folder = out_folder + "/bohb_out"
        if not os.path.exists(bohb_out_folder):
            os.makedirs(bohb_out_folder)

        # Run BOHB.
        print("Starting BOHB hyperparameter optimization (!) ... ")
        best_hp_dic, best_acc, best_auc  = rnn_util.run_BOHB(args, train_dataset,
                                                    val_dataset, bohb_out_folder,
                                                    verbose_bohb=args.verbose_bohb,
                                                    n_bohb_iter=args.bohb_n_iter,
                                                    min_budget=args.bohb_min_budget,
                                                    max_budget=args.bohb_max_budget)
        print("BOHB's done!")
        # Get keys.
        opt_keys = []
        for key in best_hp_dic:
            opt_keys.append(key)
            assert key in opt_dic, "optimized HP key %s not in opt_dic HP dictionary" %(key)
            opt_dic[key] = best_hp_dic[key]
        #print("HPs optimized by BOHB:")
        #for key in opt_keys:
        #    print(key, " (", opt_dic[key], ")")
        print("Best configuration accuracy:    %f" %(best_acc))
        print("Best configuration AUC:         %f" %(best_auc))

    """
    Train final model, either with optimizd HPs from BOHB or default ones /
    set ones from command line.

    """

    # B*tch loaders.
    train_loader = DataLoader(dataset=train_dataset, batch_size=opt_dic["batch_size"], collate_fn=rnn_util.pad_collate, pin_memory=True)
    val_loader = DataLoader(dataset=val_dataset, batch_size=opt_dic["batch_size"], collate_fn=rnn_util.pad_collate, pin_memory=True)

    # Plot learning curves.
    plot_lc_folder = False
    if args.plot_lc:
        plot_lc_folder = model_folder + "/lc_plots"

    # Train model on fold dataset.
    print("Training model ... ")
    val_auc, val_acc, c_epochs = rnn_util.train_model(args, train_loader,
                                                      val_loader, final_model_path,
                                                      device, criterion,
                                                      opt_dic=opt_dic,
                                                      run_id="final_model",
                                                      verbose=args.verbose_train,
                                                      plot_lc_folder=plot_lc_folder)

    print("# epochs used to train model:   %i" %(c_epochs))
    print("Model validation set accuracy:  %f" %(val_acc))
    print("Model validation set AUC:       %f" %(val_auc))

    if args.add_test or args.test_ids:
        if args.add_test:
            print("Testing model ... ")
        else:
            print("Testing model on --test-ids set ... ")
        test_loader = DataLoader(dataset=test_dataset, batch_size=opt_dic["batch_size"], collate_fn=rnn_util.pad_collate, pin_memory=True)
        # Get model test set performance.
        test_auc, test_acc = rnn_util.test_model(args, test_loader, final_model_path,
                                                 device, criterion,
                                                 opt_dic=opt_dic)

        print("Model test set accuracy:        %f" %(val_acc))
        print("Model test set AUC:             %f" %(val_auc))

    final_model_path = out_folder + "/" + "final.model"
    final_params_path = out_folder + "/" + "final.params"

    # Write model params file.
    PAROUT = open(final_params_path, "w")
    PAROUT.write("n_feat\t%i\n" %(args.n_feat))
    PAROUT.write("n_class\t%i\n" %(args.n_class))

    PAROUT.write("epochs\t%s\n" %(str(args.epochs)))
    PAROUT.write("patience\t%s\n" %(str(args.patience)))
    PAROUT.write("final_model_epochs\t%s\n" %(str(c_epochs)))
    PAROUT.write("val_acc\t%s\n" %(str(val_acc)))
    PAROUT.write("val_auc\t%s\n" %(str(val_auc)))
    if args.add_test or args.test_ids:
        PAROUT.write("test_acc\t%s\n" %(str(test_acc)))
        PAROUT.write("test_auc\t%s\n" %(str(test_auc)))
    PAROUT.write("batch_size\t%s\n" %(str(opt_dic["batch_size"])))
    PAROUT.write("learn_rate\t%s\n" %(str(opt_dic["learn_rate"])))
    PAROUT.write("weight_decay\t%s\n" %(str(opt_dic["weight_decay"])))
    PAROUT.write("dropout_rate\t%s\n" %(str(opt_dic["dropout_rate"])))
    PAROUT.write("rnn_type\t%s\n" %(str(opt_dic["rnn_type"])))
    PAROUT.write("n_rnn_dim\t%s\n" %(str(opt_dic["n_rnn_dim"])))
    PAROUT.write("n_rnn_layers\t%s\n" %(str(opt_dic["n_rnn_layers"])))
    PAROUT.write("bidirect\t%s\n" %(str(opt_dic["bidirect"])))
    PAROUT.write("add_fc_layer\t%s\n" %(str(opt_dic["add_fc_layer"])))
    PAROUT.write("add_feat\t%s\n" %(str(args.add_feat)))
    PAROUT.write("embed\t%s\n" %(str(args.embed)))
    PAROUT.write("embed_k\t%s\n" %(str(args.embed_k)))
    PAROUT.write("embed_vocab_size\t%s\n" %(str(args.embed_vocab_size)))
    PAROUT.write("embed_dim\t%s\n" %(str(opt_dic["embed_dim"])))
    PAROUT.write("feat_info_dic\t%s\n" %(str(feat_info_dic)))
    PAROUT.close()

    # RNN type string.
    rnn_model_type = "GRU"
    if opt_dic["rnn_type"] == 2:
        rnn_model_type = "LSTM"
    n_fc_layers = 1
    if opt_dic["add_fc_layer"]:
        n_fc_layers = 2

    print("")
    print("=========================")
    print("  MODEL HYPERPARAMETERS")
    print("=========================")
    print("Batch size:               ", opt_dic["batch_size"])
    print("Learning rate:            ", opt_dic["learn_rate"])
    print("Weight decay:             ", opt_dic["weight_decay"])
    print("RNN model type:           ", rnn_model_type)
    print("Bidirectional RNN:        ", opt_dic["bidirect"])
    print("# RNN layers:             ", opt_dic["n_rnn_layers"])
    print("# RNN layer dimensions:   ", opt_dic["n_rnn_dim"])
    print("# fully connected layers: ", n_fc_layers)
    print("Dropout rate:             ", opt_dic["dropout_rate"])
    if args.embed:
        print("Embedding:                ", args.embed)
        print("Embedding k:              ", args.embed_k)
        print("Embedding dimensions:     ", opt_dic["embed_dim"])
    else:
        print("Embedding:                ", args.embed)
    print("")
    print("Model (hyper)parameters file:")
    print(final_params_path)
    print("Model file:")
    print(final_model_path)
    print("")


################################################################################

def main_eval(args):
    """
    Evaluate model properties using the training set.

    """

    print("Running for you in EVAL mode ... ")

    assert os.path.isdir(args.in_train_folder), "--train-in folder does not exist"

    # Model + model parameter file.
    params_file = args.in_train_folder + "/final.params"
    model_file = args.in_train_folder + "/final.model"
    assert os.path.isfile(model_file), "--train-in folder is missing a model file. Run rnaprot train without --gen-cv to generate and store a model" %(model_file)
    assert os.path.isfile(params_file), "--train-in folder is missing a model parameter file. Run rnaprot train without --gen-cv to generate and store a model" %(params_file)
    # Additional model checks.
    model_file2 = False
    if args.add_train_folder:
        assert os.path.isdir(args.add_train_folder), "--add-train-in folder does not exist"
        # Model + model parameter file.
        params_file2 = args.add_train_folder + "/final.params"
        model_file2 = args.add_train_folder + "/final.model"
        assert os.path.isfile(model_file2), "--add-train-in folder is missing a model file. Run rnaprot train without --gen-cv to generate and store a model" %(model_file2)
        assert os.path.isfile(params_file2), "--add-train-in folder is missing a model parameter file. Run rnaprot train without --gen-cv to generate and store a model" %(params_file2)

    # Read in model parameters.
    params_dic = rplib.read_settings_into_dic(params_file)
    # Check parameters.
    assert "rnn_type" in params_dic, "rnn_type info missing in model parameter file %s" %(params_file)
    assert "n_rnn_dim" in params_dic, "n_rnn_dim info missing in model parameter file %s" %(params_file)
    assert "n_rnn_layers" in params_dic, "n_rnn_layers info missing in model parameter file %s" %(params_file)
    assert "bidirect" in params_dic, "bidirect info missing in model parameter file %s" %(params_file)
    assert "add_fc_layer" in params_dic, "add_fc_layer info missing in model parameter file %s" %(params_file)
    assert "add_feat" in params_dic, "add_feat info missing in model parameter file %s" %(params_file)
    assert "embed" in params_dic, "embed info missing in model parameter file %s" %(params_file)
    assert "embed_k" in params_dic, "embed_k info missing in model parameter file %s" %(params_file)
    assert "embed_vocab_size" in params_dic, "embed_vocab_size info missing in model parameter file %s" %(params_file)
    assert "embed_dim" in params_dic, "embed_dim info missing in model parameter file %s" %(params_file)
    assert "n_class" in params_dic, "n_class info missing in model parameter file %s" %(params_file)
    assert "n_feat" in params_dic, "n_feat info missing in model parameter file %s" %(params_file)
    assert "batch_size" in params_dic, "batch_size info missing in model parameter file %s" %(params_file)
    assert "dropout_rate" in params_dic, "dropout_rate info missing in model parameter file %s" %(params_file)

    # Store model parameters in args.
    args.rnn_type = int(params_dic["rnn_type"])
    args.n_rnn_dim = int(params_dic["n_rnn_dim"])
    args.n_rnn_layers = int(params_dic["n_rnn_layers"])
    args.bidirect = False
    if params_dic["bidirect"] == "True":
        args.bidirect = True
    args.add_fc_layer = False
    if params_dic["add_fc_layer"] == "True":
        args.add_fc_layer = True
    args.add_feat = False
    if params_dic["add_feat"] == "True":
        args.add_feat = True
    args.embed = False
    if params_dic["embed"] == "True":
        args.embed = True
    args.embed_k = int(params_dic["embed_k"])
    args.embed_vocab_size = int(params_dic["embed_vocab_size"])
    args.embed_dim = int(params_dic["embed_dim"])
    kmer2idx_dic = False
    if args.embed:
        kmer2idx_dic = rplib.get_kmer_dic(args.embed_k, rna=True, fill_idx=True)
    args.n_class = int(params_dic["n_class"])
    args.n_feat = int(params_dic["n_feat"])
    args.batch_size = int(params_dic["batch_size"])
    args.dropout_rate = float(params_dic["dropout_rate"])

    # Get additional model training run parameters.
    rp_train_settings_file = args.in_train_folder + "/settings.rnaprot_train.out"
    assert os.path.exists(rp_train_settings_file), "rnaprot train settings file %s not found" %(rp_train_settings_file)
    train_settings_dic = rplib.read_settings_into_dic(rp_train_settings_file)
    assert "str_mode" in train_settings_dic, "dropout_rate info missing in train run parameter file %s" %(rp_train_settings_file)
    assert "only_seq" in train_settings_dic, "only_seq info missing in train run parameter file %s" %(rp_train_settings_file)
    args.only_seq = False
    if train_settings_dic["only_seq"] == "True":
        args.only_seq = True
    onlyseq = args.only_seq
    # Checks.
    if onlyseq:
        if args.embed:
            assert args.n_feat == 1, "onlyseq = True and embed = True, but # features != 1 (found %i)" %(args.n_feat)
        else:
            assert args.n_feat == 4, "onlyseq = True and embed = False, but # features != 4 (found %i)" %(args.n_feat)
    # Structure mode.
    args.str_mode = int(train_settings_dic["str_mode"])
    str_mode_check = [1, 2, 3, 4]
    assert args.str_mode in str_mode_check, "invalid str_mode setting found in train run parameter file %s" %(rp_train_settings_file)

    """
    Output files.

    """
    # Output folder.
    if not os.path.exists(args.out_folder):
        os.makedirs(args.out_folder)
    # Two model scores output table.
    two_model_scores_out = args.out_folder + "/two_model_scores.out"
    # Motif output folder.
    motif_plots_folder = args.out_folder

    # Plot format.
    plot_format = "png"
    if args.plot_format == 2:
        plot_format = "pdf"

    # More checks.
    # Window size.
    if (args.win_size-1) % 2:
        assert False, "only uneven window size values (--win-size) supported (5,7,9, ...)"
    if args.win_size < 5:
        assert False, "window size should be >= 5"
    win_info_str = "w%i" %(args.win_size)
    # k-mer size.
    if args.report:
        assert args.kmer_size >= 3 and args.kmer_size <= 12, "set a reasonable k-mer size (--kmer-size) (>= 3 and <= 12)"
        if args.lookup_kmer:
            assert len(args.lookup_kmer) == args.kmer_size, "--lookup-kmer length needs to be == --kmer-size"
    else:
        assert not args.lookup_kmer, "--lookup-kmer requires --report to be set"

    # Window extension from center left and right.
    win_extlr = int(args.win_size/2)

    # Motif size.
    max_motif_size = 0
    for motif_size in args.list_motif_sizes:
        if motif_size > max_motif_size:
            max_motif_size = motif_size
        if (motif_size-1) % 2:
            assert False, "only uneven motif size values supported"
        if motif_size < 5:
            assert False, "motif sizes should be >= 5"
        if motif_size > 21:
            assert False, "motif sizes should be <= 21"
    max_motif_extlr = int(max_motif_size/2)

    # Some facts.
    print("# dataset features:    %i" %(args.n_feat))
    print("Window size:          ", args.win_size)
    print("Top site numbers:     ", args.list_nr_top_sites)
    print("Motif sizes:          ", args.list_motif_sizes)

    # Check for CUDA support.
    if torch.cuda.is_available():
        print("CUDA: I'm available. Using GPU ... ")
    else:
        print("CUDA: I'm NOT available. Using CPU ... ")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # Output mode settings.
    settings_file = args.out_folder + "/" + "/settings.rnaprot_eval.out"
    SETOUT = open(settings_file, "w")
    for arg in vars(args):
        SETOUT.write("%s\t%s\n" %(arg, str(getattr(args, arg))))
    SETOUT.close()

    # Load data.
    print("Load positive training set from --gt-in ... ")
    seqs_dic, idx2id_dic, all_features, ch_info_dic = rplib.load_eval_data(args,
                                                            kmer2idx_dic=kmer2idx_dic,
                                                            store_tensors=False)
    l_dataset = len(all_features)
    print("Positive set size:     %i" %(l_dataset))

    # Check numbers of sites to extract.
    max_nr_top_sites = max(args.list_nr_top_sites)
    assert args.nr_top_profiles <= l_dataset, "set --nr-top-profiles > number of positive sites! Pleaser lower --nr-top-profiles"
    assert args.nr_top_profiles <= max_nr_top_sites, "set --nr-top-profiles > maximum --nr-top-sites. Please adjust --nr-top-profiles or --nr-top-sites"
    assert l_dataset >= max_nr_top_sites, "positive dataset size < maximum given --nr-top-sites. Please lower max --nr-top-sites"

    if args.list_lookup_ids:
        for seq_id in args.list_lookup_ids:
            assert seq_id in seqs_dic, "given --lookup-profile ID %s not found in --gt-in positive set" %(seq_id)
        lookup_out_folder = args.out_folder + "/lookup_profiles"
        if not os.path.exists(lookup_out_folder):
            os.makedirs(lookup_out_folder)

    # Convert to tensors for prediction, keep all_features for motif extraction later.
    all_feat_tens = []
    for feat in all_features:
        all_feat_tens.append(torch.tensor(feat, dtype=torch.float))

    # Check sequence size vs set window size.
    min_seq_len = 66666666
    all_seq_len = 0
    for seq_id in seqs_dic:
        seq_l = len(seqs_dic[seq_id])
        all_seq_len += seq_l
        if seq_l < min_seq_len:
            min_seq_len = seq_l
    assert win_extlr <= min_seq_len+1, "--win-size needs to be < 2*min_seq_len+1. Please lower --win-size"
    if args.report:
        assert args.kmer_size <= min_seq_len, "minimum sequence length < --kmer-size (%i < %i). Please lower --kmer-size" %(min_seq_len, args.kmer_size)

    # Check # features.
    check_num_feat = 0
    for fid in ch_info_dic:
        check_num_feat += len(ch_info_dic[fid][1])
    assert args.n_feat == check_num_feat, "num_features != check_num_feat (%i != %i)" %(args.n_feat, check_num_feat)

    # Load whole-site dataset.
    c_all_feat = len(all_feat_tens)
    all_labels = [1]*c_all_feat
    predict_dataset = RNNDataset(all_feat_tens, all_labels)
    predict_loader = DataLoader(dataset=predict_dataset, batch_size=50, collate_fn=rnn_util.pad_collate, pin_memory=True)

    # Define and load --train-in model.
    print("Load --train-in model ... ")
    model = rnn_util.define_model(args, device,
                                      opt_dic=False)
    model.load_state_dict(torch.load(model_file))

    # Predict.
    print("Predict whole sites ... ")
    ws_scores = rnn_util.test_scores(predict_loader, model, device)

    print("Maximum whole-site score: ", max(ws_scores))
    print("Minimum whole-site score: ", min(ws_scores))

    # Checks.
    c_ws_scores = len(ws_scores)
    assert c_ws_scores == c_all_feat, "# whole-site scores != size features list (%i != %i)" %(c_ws_scores, c_all_feat)

    # Connect whole-site indices to scores.
    si2sc_dic = {}
    for i, sc in enumerate(ws_scores):
        si2sc_dic[i] = sc
    assert len(si2sc_dic) == len(seqs_dic), "length si2sc_dic != length seqs_dic"

    """
    If --report, predict on negative set too
    Read in negative set as graphs
    Predict on whole-site graphs and store predictions in neg_ws_scores

    """
    neg_ws_scores = []
    if args.report:
        # Load data again.
        print("Load negative training set for --report ... ")
        neg_seqs_dic, idx2negid_dic, all_neg_feat, ch_info_neg_dic = rplib.load_eval_data(args,
                                                                     kmer2idx_dic=kmer2idx_dic,
                                                                     load_negatives=True)

        # Check # features.
        check_num_feat = 0
        for fid in ch_info_neg_dic:
            check_num_feat += len(ch_info_neg_dic[fid][1])
        assert args.n_feat == check_num_feat, "num_features != check_num_feat (%i != %i)" %(num_features, check_num_feat)

        # Load dataset.
        c_neg_feat = len(all_neg_feat)
        all_neg_labels = [1]*c_neg_feat
        predict_dataset = RNNDataset(all_neg_feat, all_neg_labels)
        predict_loader = DataLoader(dataset=predict_dataset, batch_size=args.batch_size, collate_fn=rnn_util.pad_collate, pin_memory=True)

        # Predict.
        print("Predict whole-site scores for negatives ... ")
        neg_ws_scores = rnn_util.test_scores(predict_loader, model, device)

        # Checks.
        assert neg_ws_scores, "neg_ws_scores empty"
        c_neg_ws_scores = len(neg_ws_scores)
        print("Negative set size:     %i" %(c_neg_feat))
        assert c_neg_ws_scores == c_neg_feat, "c_neg_ws_scores != c_neg_feat (%i != %i)" %(c_neg_ws_scores, c_neg_feat)

    """
    Additional model given via --add-train-in
    - Check model and parameters
    - Read in positive training data as graphs
    - Predict on whole sites and store predictions in add_ws_scores

    """

    add_ws_scores = []
    if args.add_train_folder:
        print("Take care of additional model from --add-train-in ... ")
        # Model + model parameter file.
        params_file2 = args.add_train_folder + "/final.params"
        model_file2 = args.add_train_folder + "/final.model"
        # Read in model parameters.
        params_dic2 = rplib.read_settings_into_dic(params_file2)

        # Check additional model parameters.
        assert "rnn_type" in params_dic2, "rnn_type info missing in model parameter file %s" %(params_file2)
        assert "n_rnn_dim" in params_dic2, "n_rnn_dim info missing in model parameter file %s" %(params_file2)
        assert "n_rnn_layers" in params_dic2, "n_rnn_layers info missing in model parameter file %s" %(params_file2)
        assert "bidirect" in params_dic2, "bidirect info missing in model parameter file %s" %(params_file2)
        assert "add_fc_layer" in params_dic2, "add_fc_layer info missing in model parameter file %s" %(params_file2)
        assert "add_feat" in params_dic2, "add_feat info missing in model parameter file %s" %(params_file2)
        assert "embed" in params_dic2, "embed info missing in model parameter file %s" %(params_file2)
        assert "embed_k" in params_dic2, "embed_k info missing in model parameter file %s" %(params_file2)
        assert "embed_vocab_size" in params_dic2, "embed_vocab_size info missing in model parameter file %s" %(params_file2)
        assert "embed_dim" in params_dic2, "embed_dim info missing in model parameter file %s" %(params_file2)
        assert "n_class" in params_dic2, "n_class info missing in model parameter file %s" %(params_file2)
        assert "n_feat" in params_dic2, "n_feat info missing in model parameter file %s" %(params_file2)
        assert "batch_size" in params_dic2, "batch_size info missing in model parameter file %s" %(params_file2)
        assert "dropout_rate" in params_dic2, "dropout_rate info missing in model parameter file %s" %(params_file2)

        # Store model 2 parameters in m2p_dic.
        m2p_dic = {}
        m2p_dic["batch_size"] = int(params_dic2["batch_size"])
        m2p_dic["rnn_type"] = int(params_dic2["rnn_type"])
        m2p_dic["n_rnn_dim"] = int(params_dic2["n_rnn_dim"])
        m2p_dic["n_rnn_layers"] = int(params_dic2["n_rnn_layers"])
        m2p_dic["dropout_rate"] = float(params_dic2["dropout_rate"])
        m2p_dic["bidirect"] = False
        if params_dic2["bidirect"] == "True":
            m2p_dic["bidirect"] = True
        m2p_dic["add_fc_layer"] = False
        if params_dic2["add_fc_layer"] == "True":
            m2p_dic["add_fc_layer"] = True
        m2p_dic["embed"] = False
        if params_dic2["embed"] == "True":
            m2p_dic["embed"] = True
        m2p_dic["add_feat"] = False
        if params_dic2["add_feat"] == "True":
            m2p_dic["add_feat"] = True
        m2p_dic["embed_k"] = int(params_dic2["embed_k"])
        m2p_dic["embed_dim"] = int(params_dic2["embed_dim"])
        m2p_dic["embed_vocab_size"] = int(params_dic2["embed_vocab_size"])
        m2p_dic["n_class"] = int(params_dic2["n_class"])
        m2p_dic["n_feat"] = int(params_dic2["n_feat"])

        # Embedding.
        kmer2idx_dic = False
        if m2p_dic["embed"]:
            kmer2idx_dic = rplib.get_kmer_dic(m2p_dic["embed_k"], rna=True, fill_idx=True)

        # Get rnaprot train parameters for additional checks.
        rp_train_settings_file = args.add_train_folder + "/settings.rnaprot_train.out"
        assert os.path.isfile(rp_train_settings_file), "missing --add-train-in settings file %s" %(rp_train_settings_file)
        rp_train_settings_dic = rplib.read_settings_into_dic(rp_train_settings_file)

        assert "str_mode" in rp_train_settings_dic, "str_mode info missing in --add-train-in settings file %s" %(rp_train_settings_file)
        str_mode2 = int(rp_train_settings_dic["str_mode"])
        str_mode_check = [1,2,3,4]
        assert str_mode2 in str_mode_check, "invalid str_mode2 given"
        m2p_dic["str_mode"] = str_mode2

        # Load data again.
        print("Load positive training set from --gt-in for --add-train-in model ... ")
        seqs_dic2, idx2id_dic2, all_feat2, ch_info_dic2 = rplib.load_eval_data(args,
                                                            train_folder=args.add_train_folder,
                                                            kmer2idx_dic=kmer2idx_dic,
                                                            embed=m2p_dic["embed"],
                                                            num_features=m2p_dic["n_feat"],
                                                            str_mode=m2p_dic["str_mode"])

        # Check # features.
        check_num_feat = 0
        for fid in ch_info_dic2:
            check_num_feat += len(ch_info_dic2[fid][1])
        assert m2p_dic["n_feat"] == check_num_feat, "# --add-train-in model features != check_num_feat (%i != %i)" %(m2p_dic["n_feat"], check_num_feat)

        # Demand training set to be same for both models.
        c_seqs1 = len(seqs_dic)
        c_seqs2 = len(seqs_dic2)
        assert c_seqs1 == c_seqs2, "# positive --train-in sequences != # positive --add-train-in sequences (%i != %i). Please train two models on the same training set in order to compare the models" %(c_seqs1, c_seqs2)
        for seq_id1 in seqs_dic:
            assert seq_id1 in seqs_dic2, "--train-in sequence ID %s not found in --add-train-in sequence set. Please train two models on the same training set in order to compare the models" %(seq_id1)
            assert seqs_dic[seq_id1] == seqs_dic2[seq_id1],  "--train-in sequence %s is not equal to --add-train-in sequence %s. Please train two models on the same training set in order to compare the models" %(seq_id1, seq_id1)

        # Load dataset.
        c_all_feat2 = len(all_feat2)
        all_labels2 = [1]*c_all_feat2
        predict_dataset = RNNDataset(all_feat2, all_labels2)
        predict_loader = DataLoader(dataset=predict_dataset, batch_size=m2p_dic["batch_size"], collate_fn=rnn_util.pad_collate, pin_memory=True)

        # Define and Load model.
        print("Load --add-train-in model ... ")
        add_model = rnn_util.define_model(args, device,
                                          opt_dic=m2p_dic)
        add_model.load_state_dict(torch.load(model_file2))

        # Predict.
        print("Predict whole sites with --add-train-in model ... ")
        add_ws_scores = rnn_util.test_scores(predict_loader, add_model, device)
        # Checks.
        c_add_ws_scores = len(add_ws_scores)
        assert c_all_feat == c_all_feat2, "c_all_feat != c_all_feat2 (%i != %i)" %(c_all_feat, c_all_feat2)
        assert c_add_ws_scores == c_all_feat2, "# whole-site scores != size of features list (%i != %i)" %(c_add_ws_scores, c_all_feat2)

        # Just 2 B sure bro.
        if add_ws_scores:
            assert len(add_ws_scores) == len(ws_scores), "len(add_ws_scores) != len(ws_scores) (%i != %i)" %(len(add_ws_scores), len(ws_scores))

        # Print out two model scores stats.
        print("Output whole-site scores for both models ... ")

        TMOUT = open(two_model_scores_out, "w")
        TMOUT.write("model1_score_rank\tsequence_id\tmodel1_score\tmodel2_score\tscore_diff\tsequence\n")

        ws_sc_rank = 0

        for si, ws_sc in sorted(si2sc_dic.items(), key=lambda item: item[1], reverse=True):

            ws_sc_rank += 1
            add_ws_sc = add_ws_scores[si]
            seq_id = idx2id_dic[si]
            seq = seqs_dic[seq_id]
            abs_diff = abs(ws_sc - add_ws_sc)
            TMOUT.write("%i\t%s\t%.6f\t%.6f\t%.6f\t%s\n" %(ws_sc_rank, seq_id, ws_sc, add_ws_sc, abs_diff, seq))

            # Plot lookup profile for --add-train-in model too.
            if args.list_lookup_ids and seq_id in args.list_lookup_ids:

                print("Generate lookup profile plot %s for additional model ... " %(seq_id))

                sal_list = rnn_util.get_saliency_from_feat_list(all_feat2[si],
                                            add_model, device,
                                            got_tensors=True,
                                            sal_type=1)
                l_sal = len(sal_list)
                assert l_sal == l_sc, "length saliency list (--lookup-profile, --add-train-in) != length profile scores list (%i != %i)" %(l_sal, l_sc)
                all_feat2_no_tens = all_feat2[si].tolist()

                single_pert_list = rnn_util.get_single_nt_perturb_scores(args,
                                            seq, all_feat2_no_tens,
                                            add_model, device,
                                            model_hp_dic=m2p_dic,
                                            load_model=False)

                plot_out_file = lookup_out_folder + "/" + seq_id + "_" + win_info_str + "_sc" + str(add_ws_sc) + ".add_model." + plot_format

                rplib.make_feature_attribution_plot(seq, all_feat2_no_tens,
                                            ch_info_dic2, plot_out_file,
                                            sal_list=sal_list,
                                            single_pert_list=single_pert_list)

        TMOUT.close()

    """
    Window predictions.

    """

    print("Create and predict windows ... ")
    profile_scores_ll = rnn_util.get_window_predictions(args, model, device,
                                                        all_feat_tens, win_extlr,
                                                        load_model=False,
                                                        got_tensors=True)

    assert len(profile_scores_ll) == c_ws_scores, "length profile scores list != # whole-site scores"

    # Get best + worst scoring window.
    best_sc_win, worst_sc_win = rnn_util.get_best_worst_scoring_window(profile_scores_ll, all_features, win_extlr)

    """
    Plot profiles and motifs
    ========================

    - Sort sites by score and evaluate top sites.
    - Get profile plot for each site.
    - Get motifs inside the set by selecting positions with max score,
      and take sequence around them (defined by motif size).
    - Also generate a bottom motif and the bottom scoring sites, if
      --bottoms-up is set.

    """
    top_profiles_folder = args.out_folder + "/" + "top_profiles_" + win_info_str + "_out"
    if os.path.exists(top_profiles_folder):
        shutil.rmtree(top_profiles_folder)
    os.makedirs(top_profiles_folder)
    bottom_profiles_folder = args.out_folder + "/" + "bottom_profiles_" + win_info_str + "_out"
    if os.path.exists(bottom_profiles_folder):
        shutil.rmtree(bottom_profiles_folder)
    if args.bottoms_up:
        os.makedirs(bottom_profiles_folder)

    # Sort sites by score to evaluate top sites.
    rank_scores_out = args.out_folder + "/whole_site_scores.out"
    RSOUT = open(rank_scores_out, "w")
    site_count = 0
    id_i = 10 ** len(str(args.nr_top_profiles))

    # Collect window perturbation scores lists.
    worst_win_pert_dic = {}
    worst_win_pos_dic = {}

    # Store min/max score positions for each sequence.
    # id2minpos_dic = {}
    # id2maxpos_dic = {}
    # Store min/max saliency positions for each sequence.
    # id2minsalpos_dic = {}
    # id2maxsalpos_dic = {}
    # c_maxsal_borders = 0
    # c_maxsal_center = 0

    # Go through positive sites by descending score.
    print("Extract saliency and mutation info + plot top %i profiles ... " %(args.nr_top_profiles))
    for si, ws_sc in sorted(si2sc_dic.items(), key=lambda item: item[1], reverse=True):

        profile_scores = profile_scores_ll[si]
        if not profile_scores:
            continue
        site_count += 1
        id_i += 1
        seq_id = idx2id_dic[si]
        seq = seqs_dic[seq_id]
        l_sc = len(profile_scores)

        # Store whole-site rank + ID + score in file.
        RSOUT.write("%i\t%s\t%f\t%s\n" %(site_count, seq_id, ws_sc, seq))

        # Decide what to extract for given site.
        sal_list = False
        single_pert_list = False
        get_sal = False
        get_single_pert = False
        if site_count <= args.nr_top_profiles:
            get_sal = True
            get_single_pert = True
        if args.list_lookup_ids and seq_id in args.list_lookup_ids:
            get_sal = True
            get_single_pert = True

        # Get worst scoring window perturbation scores.
        worst_win_pert_list = rnn_util.get_window_perturb_scores(args,
                                        all_features[si], worst_sc_win,
                                        model, device,
                                        avg_win_extlr=3,
                                        load_model=False)

        worst_win_pert_dic[seq_id] = worst_win_pert_list
        # Get worst score position in site (with padding).
        worst_sc_pos = rplib.get_top_sc_list_pos(worst_win_pert_list,
                                                 get_lowest=True,
                                                 padding=win_extlr)
        worst_win_pos_dic[seq_id] = worst_sc_pos

        if get_sal:
            # Get mean saliency profile for site.
            sal_list = rnn_util.get_saliency_from_feat_list(all_feat_tens[si],
                                        model, device,
                                        got_tensors=True,
                                        sal_type=1)
        if get_single_pert:
            # Get single position perturbation scores.
            single_pert_list = rnn_util.get_single_nt_perturb_scores(args,
                                        seq, all_features[si],
                                        model, device,
                                        load_model=False)

        # Plot profile if site ID == lookup site ID.
        if args.list_lookup_ids and seq_id in args.list_lookup_ids:
            plot_out_file = lookup_out_folder + "/" + seq_id + "_" + win_info_str + "_sc" + str(ws_sc) + "." + plot_format
            rplib.make_feature_attribution_plot(seq, all_features[si],
                                        ch_info_dic, plot_out_file,
                                        sal_list=sal_list,
                                        single_pert_list=single_pert_list,
                                        worst_win_pert_list=worst_win_pert_list)

        # Plot --nr-top-profiles profiles.
        if site_count <= args.nr_top_profiles:
            plot_file_name = str(id_i)[1:] + "_" + seq_id + "_" + win_info_str + "_sc" + str(ws_sc) + "." + plot_format
            plot_out_file = top_profiles_folder + "/" + plot_file_name
            rplib.make_feature_attribution_plot(seq, all_features[si],
                                        ch_info_dic, plot_out_file,
                                        sal_list=sal_list,
                                        single_pert_list=single_pert_list,
                                        worst_win_pert_list=worst_win_pert_list)

        # if not site_count % 100:
        #     print("%i sites processed" %(site_count))

    RSOUT.close()

    # Plot bottom profiles too.
    if args.bottoms_up:
        print("Plot bottom %i profiles ... " %(args.nr_top_profiles))
        site_count = 0
        for si, sc in sorted(si2sc_dic.items(), key=lambda item: item[1], reverse=False):
            # Profile scores.
            profile_scores = profile_scores_ll[si]
            if not profile_scores:
                continue
            site_count += 1
            id_i += 1
            seq_id = idx2id_dic[si]
            seq = seqs_dic[seq_id]
            l_sc = len(profile_scores)

            # Print only top --nr-top-profiles profiles.
            if site_count <= args.nr_top_profiles:

                # Get best scoring window perturbation scores.
                best_win_pert_list = rnn_util.get_window_perturb_scores(args,
                                                all_features[si], best_sc_win,
                                                model, device,
                                                avg_win_extlr=3,
                                                load_model=False)

                # Get mean saliency profile for site.
                sal_list = rnn_util.get_saliency_from_feat_list(all_feat_tens[si],
                                            model, device,
                                            got_tensors=True,
                                            sal_type=1)

                # Get single position perturbation scores.
                single_pert_list = rnn_util.get_single_nt_perturb_scores(args,
                                            seq, all_features[si],
                                            model, device,
                                            load_model=False)

                # Plot file.
                plot_file_name = str(id_i)[1:] + "_" + win_info_str + "_sc" + str(sc) + "." + plot_format
                plot_out_file = bottom_profiles_folder + "/" + plot_file_name

                rplib.make_feature_attribution_plot(seq, all_features[si],
                                            ch_info_dic, plot_out_file,
                                            sal_list=sal_list,
                                            single_pert_list=single_pert_list,
                                            best_win_pert_list=best_win_pert_list)

            else:
                break

    """
    Format of ch_info_dic:
    ch_info_dic: {'fa': ['C', [0], ['embed'], 'embed'],
    'CTFC': ['C', [1, 2], ['0', '1'], 'one_hot'],
    'pc.con': ['N', [3], ['phastcons_score'], 'prob'],
    'pp.con': ['N', [4], ['phylop_score'], 'minmax2'],
    'rra': ['C', [5, 6], ['N', 'R'], '-'],
    'str': ['N', [7, 8, 9, 10, 11], ['E', 'H', 'I', 'M', 'S'], 'prob'],
    'tra': ['C', [12, 13, 14, 15, 16, 17, 18, 19, 20], ['A', 'B', 'C', 'E', 'F', 'N', 'S', 'T', 'Z'], '-']}

    """
    # Convert ch_info_dic if args.embed and get new # features.
    n_feat_matrix = args.n_feat
    if args.embed:
        ch_info_dic = rplib.conv_ch_info_dic(ch_info_dic, conv_mode=1)
        # For plotting we need sequence feature as one-hot again.
        n_feat_matrix = args.n_feat + 3

    # Numerical features with standard deviations.
    stdev_fid2fidx_dic = {}
    stdev_fidx2fid_dic = {}
    for fid in ch_info_dic:
        feat_type = ch_info_dic[fid][0] # C or N.
        feat_idxs = ch_info_dic[fid][1] # channel numbers occupied by feature.
        l_idxs = len(feat_idxs)
        if feat_type == "N" and l_idxs == 1:
            stdev_fid2fidx_dic[fid] = feat_idxs[0]
            stdev_fidx2fid_dic[feat_idxs[0]] = fid
    calc_stdev = True

    """
    Plot top-scoring motifs.

    """

    print("Plot top motifs ... ")

    top_motif_file_dic = {}

    weighted_motif = True
    map_nt2idx_dix = {"A" : 0, "C" : 1, "G" : 2, "U" : 3}

    # For each motif size.
    for motif_size in args.list_motif_sizes:

        # Motif center position extension.
        motif_extlr = int(motif_size / 2)

        # For each number of top sites.
        for nr_top_sites in args.list_nr_top_sites:

            print("Create motif (size %i) from top %i sites ... " %(motif_size, nr_top_sites))
            site_count = 0
            c_min_len_skipped = 0

            # Init motif score matrix.
            motif_matrix = []

            for i in range(motif_size):
                motif_matrix.append([0.0]*n_feat_matrix)
            l_mm = len(motif_matrix) # Motif length.
            l_sv = len(motif_matrix[0]) # site vector length.

            # Standard deviations for numerical features.
            fid2sc_dic = {}
            if calc_stdev:
                for fid in stdev_fid2fidx_dic:
                    fid2sc_dic[fid] = []
                    for i in range(motif_size):
                        fid2sc_dic[fid].append([])

            # Brick by brick.
            for si, sc in sorted(si2sc_dic.items(), key=lambda item: item[1], reverse=True):

                # Feature list.
                if args.embed:
                    feat_list = rplib.conv_embed_feature_list(all_features[si])
                else:
                    feat_list = all_features[si]

                # Profile scores.
                profile_scores = profile_scores_ll[si]
                if not profile_scores:
                    continue

                seq_id = idx2id_dic[si]
                seq = seqs_dic[seq_id]
                seq_list = seq.tolist()
                l_seq = len(seq)

                worst_win_pert_list = worst_win_pert_dic[seq_id]
                max_pos = worst_win_pos_dic[seq_id] # 0-based.

                # Minimum sequence length for extracting motifs.
                site_l = len(worst_win_pert_list)
                assert site_l == l_seq, "site_l != l_seq (%i != %i)" %(site_l, l_seq)
                if site_l < (motif_extlr*2 + 1):
                    c_min_len_skipped += 1
                    continue # should be captured by if not max_pos check already.

                # Extract max_pos motif brick.
                s_brick = max_pos - motif_extlr
                e_brick = max_pos + motif_extlr + 1 # 1-based, thus + 1.
                worst_sc_brick = worst_win_pert_list[s_brick:e_brick]
                seq_brick = seq_list[s_brick:e_brick]

                assert s_brick >= 0, "s_brick <= 0 (s_brick = %i) which should not happen since max_pos selection takes care of this" %(s_brick)
                assert e_brick <= site_l, "e_brick > site length (%i > %i) which should not happen since max_pos selection takes care of this" %(e_brick, site_l)

                start_j = 0
                if weighted_motif:
                    # Update sequence features in weigthed fashion.
                    for i in range(l_mm):
                        worst_sc = worst_sc_brick[i]
                        worst_sc_nt = seq_brick[i]
                        j = map_nt2idx_dix[worst_sc_nt]
                        motif_matrix[i][j] = motif_matrix[i][j] - worst_sc
                    start_j = 4
                for i in range(l_mm):
                    # If weighted_motif=True, just update additional features.
                    for j in range(start_j, l_sv):
                        motif_matrix[i][j] += feat_list[s_brick:e_brick][i][j]
                        # For one-channel numerical features, store scores to calculate stdev.
                        if calc_stdev:
                            if j in stdev_fidx2fid_dic:
                                fid2sc_dic[stdev_fidx2fid_dic[j]][i].append(feat_list[s_brick:e_brick][i][j])

                # Increment site count.
                site_count += 1
                if site_count >= nr_top_sites:
                    break

            # Checks.
            assert site_count, "no top motif information extracted for motif_size %i and nr_top_sites %i (site_count = 0)" %(motif_size, nr_top_sites)

            print("# motif sites extracted:    %i" %(site_count))
            if c_min_len_skipped:
                print("# sites skipped (min_len):  %i" %(c_min_len_skipped))

            # Average motif matrix values.
            if weighted_motif:
                for i in range(l_mm):
                    # Hallo 123
                    # Sum sum4 = sum(l[0:4]) and divide each of 4 elements by sum to normalize.

            for i in range(l_mm):
                for j in range(l_sv):
                    motif_matrix[i][j] = motif_matrix[i][j] / site_count

            # Calculate tandard deviations for one-channel numerical features at each motif position.
            fid2stdev_dic = {}
            if calc_stdev:
                for fid in fid2sc_dic:
                    fid2stdev_dic[fid] = []
                    for i in range(l_mm):
                        fid2stdev_dic[fid].append(statistics.stdev(fid2sc_dic[fid][i]))

            # Plot motif.
            motif_out_file = motif_plots_folder + "/" + "top_motif_l" + str(motif_size) + "_" + win_info_str + "_top" + str(nr_top_sites) + "." + plot_format
            rplib.make_motif_plot(motif_matrix, ch_info_dic, motif_out_file,
                                  fid2stdev_dic=fid2stdev_dic)

            top_motif_file_dic[motif_out_file] = 1



    """


# motif sites extracted:           200
# sites skipped (--motif-sc-thr):  240
fl_dic: {21: 35559, 24: 35640}
INVESTIGATE ....

    phastcons + phylop plotted from 1 to .. instead from 0 to ... (?)

motif_matrix[0]: [0.265, 0.15, 0.3, 0.285, 1.0, 0.0, 0.812765, 0.37611250000000007, 0.98, 0.02, 0.08512925499999997, 0.09797658500000002, 0.05295604000000001, 0.019393650000000005, 0.7445445149999996, 0.0, 0.015, 0.88, 0.0, 0.02, 0.005, 0.0, 0.08, 0.0]
len(motif_matrix[0]): 24
motif_matrix[1]: [0.235, 0.235, 0.2, 0.33, 1.0, 0.0, 0.805775, 0.3881699999999999, 0.98, 0.02, 0.08425141500000001, 0.11461877000000004, 0.04979506999999996, 0.02082011500000001, 0.7305146799999995, 0.0, 0.015, 0.88, 0.0, 0.02, 0.005, 0.0, 0.08, 0.0]
motif_matrix[2]: [0.235, 0.08, 0.12, 0.565, 1.0, 0.0, 0.7901600000000001, 0.2990674999999998, 0.985, 0.015, 0.08601288000000001, 0.096460675, 0.06146663500000002, 0.019583385000000005, 0.7364764050000003, 0.0, 0.01, 0.885, 0.0, 0.02, 0.005, 0.0, 0.08, 0.0]
fid2stdev_dic: {'pc.con': [0.3615982386655529, 0.37039015742672343, 0.38175036490555925, 0.37393580240678753, 0.35882918361676686, 0.3689855875152441, 0.3762026736124636], 'pp.con': [0.34052431770505387, 0.33745294285754285, 0.3285518928957447, 0.3607840682280551, 0.3539530479179435, 0.3371724225252679, 0.3562587208150615]}
ch_info_dic: {'fa': ['C', [0, 1, 2, 3], ['A', 'C', 'G', 'U'], 'one_hot'], 'CTFC': ['C', [4, 5], ['0', '1'], 'one_hot'], 'pc.con': ['N', [6], ['phastcons_score'], 'prob'], 'pp.con': ['N', [7], ['phylop_score'], 'minmax2'], 'rra': ['C', [8, 9], ['N', 'R'], '-'], 'str': ['N', [10, 11, 12, 13, 14], ['E', 'H', 'I', 'M', 'S'], 'prob'], 'tra': ['C', [15, 16, 17, 18, 19, 20, 21, 22, 23], ['A', 'B', 'C', 'E', 'F', 'N', 'S', 'T', 'Z'], '-']}


    print("motif_matrix:", motif_matrix)
    print("ch_info_dic:", ch_info_dic)
    print("fid2stdev_dic:", fid2stdev_dic)

    motif_matrix: [[0.265, 0.15, 0.3, 0.285, 1.0, 0.0, 0.812765, 0.37611250000000007, 0.98, 0.02, 0.08512925499999997, 0.09797658500000002, 0.05295604000000001, 0.019393650000000005, 0.7445445149999996, 0.0, 0.015, 0.88, 0.0, 0.02, 0.005], [0.235, 0.235, 0.2, 0.33, 1.0, 0.0, 0.805775, 0.3881699999999999, 0.98, 0.02, 0.08425141500000001, 0.11461877000000004, 0.04979506999999996, 0.02082011500000001, 0.7305146799999995, 0.0, 0.015, 0.88, 0.0, 0.02, 0.005], [0.235, 0.08, 0.12, 0.565, 1.0, 0.0, 0.7901600000000001, 0.2990674999999998, 0.985, 0.015, 0.08601288000000001, 0.096460675, 0.06146663500000002, 0.019583385000000005, 0.7364764050000003, 0.0, 0.01, 0.885, 0.0, 0.02, 0.005], [0.085, 0.305, 0.445, 0.165, 1.0, 0.0, 0.8028399999999999, 0.3905865000000002, 0.985, 0.015, 0.08840357500000001, 0.09450228500000005, 0.06388078499999997, 0.02036382, 0.7328494550000002, 0.0, 0.01, 0.885, 0.0, 0.02, 0.005], [0.56, 0.055, 0.145, 0.24, 1.0, 0.0, 0.8198300000000001, 0.43087949999999964, 0.985, 0.015, 0.08624503499999997, 0.10677079999999999, 0.06697489, 0.018173995000000002, 0.721835215, 0.0, 0.005, 0.89, 0.0, 0.02, 0.005], [0.435, 0.06, 0.27, 0.235, 1.0, 0.0, 0.8109449999999999, 0.34075099999999997, 0.985, 0.015, 0.09352080500000003, 0.11932234999999995, 0.08037490499999998, 0.019856950000000012, 0.6869249249999997, 0.0, 0.01, 0.885, 0.0, 0.02, 0.005], [0.38, 0.085, 0.305, 0.23, 1.0, 0.0, 0.8044750000000002, 0.40006800000000003, 0.985, 0.015, 0.09550320999999996, 0.11592726999999999, 0.07343648499999998, 0.02077943, 0.6943536350000002, 0.0, 0.02, 0.875, 0.0, 0.02, 0.005]]

    ch_info_dic: {'fa': ['C', [0, 1, 2, 3], ['A', 'C', 'G', 'U'], 'one_hot'],
    'CTFC': ['C', [4, 5], ['0', '1'], 'one_hot'],
    'pc.con': ['N', [6], ['phastcons_score'], 'prob'],
    'pp.con': ['N', [7], ['phylop_score'], 'minmax2'],
    'rra': ['C', [8, 9], ['N', 'R'], '-'],
    'str': ['N', [10, 11, 12, 13, 14], ['E', 'H', 'I', 'M', 'S'], 'prob'],
    'tra': ['C', [15, 16, 17, 18, 19, 20, 21, 22, 23], ['A', 'B', 'C', 'E', 'F', 'N', 'S', 'T', 'Z'], '-']}

    fid2stdev_dic: {'pc.con': [0.3615982386655529, 0.37039015742672343, 0.38175036490555925, 0.37393580240678753, 0.35882918361676686, 0.3689855875152441, 0.3762026736124636], 'pp.con': [0.34052431770505387, 0.33745294285754285, 0.3285518928957447, 0.3607840682280551, 0.3539530479179435, 0.3371724225252679, 0.3562587208150615]}


    """


    """
    Optionally plot lowest-scoring (bottom) motifs.

    """

    bottom_motif_file_dic = {}

    if args.bottoms_up:

        print("Plot bottom motifs ... ")

        # For each motif size.
        for motif_size in args.list_motif_sizes:

            # Motif center position extension.
            motif_extlr = int(motif_size / 2)

            # For each number of top sites.
            for nr_top_sites in args.list_nr_top_sites:

                print("Create motif (size %i) from bottom %i sites ... " %(motif_size, nr_top_sites))
                site_count = 0
                c_no_min_skipped = 0

                # Init motif score matrix.
                motif_matrix = []

                for i in range(motif_size):
                    motif_matrix.append([0.0]*n_feat_matrix)
                l_mm = len(motif_matrix) # Motif length.
                l_sv = len(motif_matrix[0]) # site vector length.

                # Standard deviations for numerical features.
                fid2sc_dic = {}
                if calc_stdev:
                    for fid in stdev_fid2fidx_dic:
                        fid2sc_dic[fid] = []
                        for i in range(motif_size):
                            fid2sc_dic[fid].append([])

                # Brick by brick.
                for si, sc in sorted(si2sc_dic.items(), key=lambda item: item[1], reverse=False):

                    # Feature list.
                    if args.embed:
                        feat_list = rplib.conv_embed_feature_list(all_features[si])
                    else:
                        feat_list = all_features[si]

                    # Profile scores.
                    profile_scores = profile_scores_ll[si]
                    if not profile_scores:
                        continue

                    seq_id = idx2id_dic[si]
                    seq = seqs_dic[seq_id]
                    min_pos = id2minpos_dic[seq_id]  # 0-based.
                    min_sc = profile_scores[min_pos]
                    # If min_pos == 0, no top scoring site extracted, skip this sequence.
                    if not min_pos:
                        c_no_min_skipped += 1
                        continue

                    # Minimum sequence length for extracting motifs.
                    site_l = len(profile_scores)
                    if site_l < (motif_extlr*2 + 1):
                        continue # should be captured by if not min_pos check already.

                    # Extract min_pos motif brick.
                    s_brick = min_pos - motif_extlr
                    e_brick = min_pos + motif_extlr + 1
                    assert s_brick >= 0, "s_brick <= 0 (s_brick = %i) which should not happen since min_pos selection takes care of this" %(s_brick)
                    assert e_brick <= site_l, "e_brick > site length (%i > %i) which should not happen since min_pos selection takes care of this" %(e_brick, site_l)
                    # .. and add values to motif_matrix.
                    for i in range(l_mm):
                        for j in range(l_sv):
                            motif_matrix[i][j] += feat_list[s_brick:e_brick][i][j]
                            # For one-channel numerical features, store scores to calculate stdev.
                            if calc_stdev:
                                if j in stdev_fidx2fid_dic:
                                    fid2sc_dic[stdev_fidx2fid_dic[j]][i].append(feat_list[s_brick:e_brick][i][j])

                    # Increment site count.
                    site_count += 1
                    if site_count >= nr_top_sites:
                        break

                # Checks.
                assert site_count, "no bottom motif information extracted for motif_size %i and nr_top_sites %i (site_count = 0)" %(motif_size, nr_top_sites)

                print("# motif sites extracted:           %i" %(site_count))
                if c_no_min_skipped:
                    print("# sites skipped (no min_pos):  %i" %(c_no_min_skipped))

                # Average motif matrix values.
                for i in range(l_mm):
                    for j in range(l_sv):
                        motif_matrix[i][j] = motif_matrix[i][j] / site_count

                # Calculate standard deviations for one-channel numerical features at each motif position.
                fid2stdev_dic = {}
                if calc_stdev:
                    for fid in fid2sc_dic:
                        fid2stdev_dic[fid] = []
                        for i in range(l_mm):
                            fid2stdev_dic[fid].append(statistics.stdev(fid2sc_dic[fid][i]))

                # Plot motif.
                motif_out_file = motif_plots_folder + "/" + "bottom_motif_l" + str(motif_size) + "_" + win_info_str + "_top" + str(nr_top_sites) + "." + plot_format
                rplib.make_motif_plot(motif_matrix, ch_info_dic, motif_out_file,
                                      fid2stdev_dic=fid2stdev_dic)

                bottom_motif_file_dic[motif_out_file] = 1

    """
    Generate --report statistics and plots.

    - Get k-mer scores:
        - If onlyseq, just score present sequence k-mers
        - If additional features, get average brick scores (containing k-mer),
          and bricks for plotting. Also get top scoring bricks and scores
          for each k-mer.
        - HTML report

    """
    lookup_kmer = False
    lookup_plot_file = False
    c_top_kmers = 20

    if args.report:

        kmer_feat = []
        kmer2sc_dic = {}
        # Dictionaries for additional feature analysis.
        kmer2mm_dic = {} # Dictionary of motif matrices.
        kmer_stdev_dic = {} # Standard deviation infos.
        # k-mer to best scoring matrix dictionary.
        kmer2bestmm_dic = {}
        # Store top scores for each k-mer.
        kmer2bestsc_dic = {}
        # k-mer score stdev.
        kmer2scstdev_dic = {}

        print("--report set. Gather more results ... ")
        print("Score k-mers ... ")
        print("--kmer-size:  %i" %(args.kmer_size))
        print("Extract %i-mers from sequences ... " %(args.kmer_size))
        kmer2c_dic = rplib.seqs_dic_count_kmer_freqs(seqs_dic, args.kmer_size,
                                                     rna=True,
                                                     convert_to_uc=True,
                                                     report_key_error=True)
        assert kmer2c_dic, "kmer2c_dic k-mer counts dictionary empty"

        # Check lookup k-mer.
        if args.lookup_kmer:
            # Check given k-mer.
            rna_alphabet = ["A", "C", "G", "U"]
            lookup_kmer = args.lookup_kmer.upper()
            for c in args.lookup_kmer:
                if c not in rna_alphabet:
                    assert False, "non-RNA character %s encountered in --lookup-kmer" %(c)
            if lookup_kmer in kmer2c_dic:
                print("--lookup-kmer %s present in sequences (count: %i) ... " %(lookup_kmer, kmer2c_dic[lookup_kmer]))
            else:
                print("--lookup-kmer %s not found in training sequences!" %(lookup_kmer))

        kmer_list = []
        for kmer, kmerc in sorted(kmer2c_dic.items()):
            if kmerc != 0:
                kmer_list.append(kmer)

        # If only sequence features.
        if onlyseq:
            print("--train-in model trained on sequences only. Generate k-mer features ... ")
            # Construct sequence kmer graphs.
            for kmer in kmer_list:
                if args.embed:
                    feat_list = rplib.convert_seq_to_kmer_embedding(kmer, args.embed_k, kmer2idx_dic,
                                                                     l2d=True)
                else:
                    feat_list = rplib.string_vectorizer(kmer, custom_alphabet=['A','C','G','U'])
                kmer_feat.append(torch.tensor(feat_list, dtype=torch.float))
            # Score kmers.
            c_kmer_feat = len(kmer_feat)
            kmer_feat_labels = [1]*c_kmer_feat
            predict_dataset = RNNDataset(kmer_feat, kmer_feat_labels)
            predict_loader = DataLoader(dataset=predict_dataset, batch_size=args.batch_size, collate_fn=rnn_util.pad_collate, pin_memory=True)

            print("Score k-mers present in sequences ... ")
            kmer_scores = rnn_util.test_scores(predict_loader, model, device)

            assert c_kmer_feat == len(kmer_scores), "size k-mer features list != # k-mer scores"

            for i,kmer in enumerate(kmer_list):
                kmer_score = kmer_scores[i]
                kmer2sc_dic[kmer] = kmer_score

        else:

            # More features comin at ya.
            print("--train-in model trained on sequences + additionrplib_pathal features. Generate k-mer features ... ")

            for feat_list in all_feat_tens:
                # For each position in feat_list.
                l_feat = len(feat_list)
                for wi in range(l_feat):
                    reg_s = wi
                    reg_e = wi + args.kmer_size
                    if reg_e > l_feat:
                        break
                    kmer_feat.append(feat_list[reg_s:reg_e])

            # Score kmers.
            c_kmer_feat = len(kmer_feat)
            kmer_feat_labels = [1]*c_kmer_feat
            predict_dataset = RNNDataset(kmer_feat, kmer_feat_labels)
            predict_loader = DataLoader(dataset=predict_dataset, batch_size=args.batch_size, collate_fn=rnn_util.pad_collate, pin_memory=True)

            print("Score k-mers ... ")
            kmer_scores = rnn_util.test_scores(predict_loader, model, device)

            assert len(kmer_feat) == len(kmer_scores), "len(kmer_feat) != len(kmer_scores)"

            """
            kmer2matrix_dic = {}
            Go through all_features, store score for each
            # Dictionary of motif matrices (2d lists) for kmers.
            motif_matrix_dic = {}
            # Standard deviation infos.
            mm_stdev_dic = {}

            """
            # "k-mer,fid" to 2d scores list (each k-mer position one scores list).
            kmerfid2scl_dic = {}
            # Check how many times k-mer seen.
            kmer2seenc_dic = {}

            # Motif matrix sizes.
            l_mm = args.kmer_size
            l_sv = n_feat_matrix

            # Init dictionaries.
            for kmer in kmer_list:
                # Init motif matrix for kmer.rplib_path
                motif_matrix = []
                for i in range(args.kmer_size):
                    motif_matrix.append([0.0]*n_feat_matrix)
                kmer2mm_dic[kmer] = motif_matrix
                # Init best scoring brick for k-mer matrix.
                best_motif_matrix = []
                for i in range(args.kmer_size):
                    best_motif_matrix.append([0.0]*n_feat_matrix)
                kmer2bestmm_dic[kmer] = best_motif_matrix
                # Init dictionary to store scores for stdev calculations.
                if calc_stdev:
                    for fid in stdev_fid2fidx_dic: # all numerical 1-channel features.
                        kmer_fid = "%s,%s" %(kmer, fid)
                        kmerfid2scl_dic[kmer_fid] = []
                        for i in range(args.kmer_size):
                            kmerfid2scl_dic[kmer_fid].append([])
                # Init best scores dic.
                kmer2bestsc_dic[kmer] = -1000

            # Go over all sequence windows.
            sc_i = 0
            kmer2sclist_dic = {}
            for si,feat_list in enumerate(all_features):

                # If embed, convert feature list to sequence one-hot containing list.
                if args.embed:
                    feat_list = rplib.conv_embed_feature_list(feat_list)

                seq_id = idx2id_dic[si]
                seq = seqs_dic[seq_id]
                # For each position in feat_list.
                l_feat = len(feat_list)
                l_seq = len(seq)
                assert l_feat == l_seq, "l_feat != l_seq"
                for wi in range(l_feat):
                    brick_s = wi
                    brick_e = wi + args.kmer_size
                    if brick_e > l_feat:
                        break
                    feat_list_win = feat_list[brick_s:brick_e]
                    win_score = float(kmer_scores[sc_i])
                    win_kmer = seq[brick_s:brick_e]
                    win_lookup_kmer = False
                    if lookup_kmer:
                        if win_kmer == lookup_kmer:
                            win_lookup_kmer = True

                    # Record score for average score calculation later.
                    if win_kmer not in kmer2sclist_dic:
                        kmer2sclist_dic[win_kmer] = [win_score]
                    else:
                        kmer2sclist_dic[win_kmer].append(win_score)
                    if win_kmer in kmer2seenc_dic:
                        kmer2seenc_dic[win_kmer] += 1
                    else:
                        kmer2seenc_dic[win_kmer] = 1
                    sc_i += 1
                    # Add hit brick values to kmer2mm_dic.
                    for i in range(l_mm):
                        for j in range(l_sv):
                            kmer2mm_dic[win_kmer][i][j] += feat_list_win[i][j]
                            # For 1-channel numerical features, store scores to calculate stdev.
                            if calc_stdev:
                                for fid in stdev_fid2fidx_dic: # all numerical 1-channel features.
                                    kmer_fid = "%s,%s" %(win_kmer, fid)
                                    kmerfid2scl_dic[kmer_fid][i].append(feat_list_win[i][j])
                    # Store best scoring brick for k-mer.
                    if win_score > kmer2bestsc_dic[win_kmer]:
                        kmer2bestsc_dic[win_kmer] = win_score
                        for i in range(l_mm):
                            for j in range(l_sv):
                                kmer2bestmm_dic[win_kmer][i][j] = feat_list_win[i][j]

            # Averaging of feature+k-mer scores and stdev calculation.
            assert sc_i == len(kmer_scores), "sc_i != len(kmer_scores) (%i != %i)" %(sc_i, len(kmer_scores))
            for kmer in kmer_list:
                kmer_count = kmer2c_dic[kmer]
                kmer_seen_count = kmer2seenc_dic[kmer]
                assert kmer_seen_count == kmer_count, "kmer_seen_count != kmer_count (%i != %i)" %(kmer_seen_count, kmer_count)
                # Average feature scores for plotting.
                for i in range(l_mm):
                    for j in range(l_sv):
                        kmer2mm_dic[kmer][i][j] = kmer2mm_dic[kmer][i][j] / kmer_count
                # Average k-mer score.
                kmer_sc_avg = kmer2sclist_dic[kmer][0]
                kmer_sc_stdev = 0.0
                if len(kmer2sclist_dic[kmer]) != 1:
                    #print(kmer2sclist_dic[kmer])
                    kmer_sc_avg = statistics.mean(kmer2sclist_dic[kmer])
                    kmer_sc_stdev = statistics.stdev(kmer2sclist_dic[kmer])
                kmer2sc_dic[kmer] = kmer_sc_avg
                kmer2scstdev_dic[kmer] = kmer_sc_stdev
                # Calculate standard deviations for one-channel numerical features at each motif position.
                fid2stdev_dic = {}
                if calc_stdev:
                    for fid in stdev_fid2fidx_dic:
                        fid2stdev_dic[fid] = []
                        kmer_fid = "%s,%s" %(kmer, fid)
                        for i in range(l_mm):
                            fid2stdev_dic[fid].append(statistics.stdev(kmerfid2scl_dic[kmer_fid][i]))
                kmer_stdev_dic[kmer] = fid2stdev_dic

        # Get k-mer count ranks and output sorted by score.
        kmer2rank_dic = {}
        kmer_rank = 0
        for kmer, kmerc in sorted(kmer2c_dic.items(), key=lambda item: item[1], reverse=True):
            kmer_rank += 1
            kmer2rank_dic[kmer] = kmer_rank

        """
        Output k-mer stats tables.

        """
        kmer2scrank_dic = {}
        kmer2avgscrank_dic = {}
        kmer_stats_out = motif_plots_folder + "/kmer_stats.out"
        KSOUT = open(kmer_stats_out, "w")
        if lookup_kmer:
            lookup_kmer_stats_out = motif_plots_folder + "/lookup_kmer_stats.out"
            LKSOUT = open(lookup_kmer_stats_out, "w")
            if onlyseq:
                LKSOUT.write("score_rank\tkmer\tscore\tkmer_count\tcount_rank\n")
            else:
                LKSOUT.write("best_score_rank\tavg_score_rank\tkmer\tbest_score\tavg_score\tavg_sc_stdev\tkmer_count\tcount_rank\n")

        if onlyseq:
            sc_rank = 0
            KSOUT.write("score_rank\tkmer\tscore\tkmer_count\tcount_rank\n")
            for kmer, sc in sorted(kmer2sc_dic.items(), key=lambda item: item[1], reverse=True):
                sc_rank += 1
                kmer_count_rank = kmer2rank_dic[kmer]
                kmer_count = kmer2c_dic[kmer]
                kmer2scrank_dic[kmer] = sc_rank
                if lookup_kmer and kmer == lookup_kmer:
                    LKSOUT.write("%i\t%s\t%.6f\t%i\t%i\n" %(sc_rank, kmer, sc, kmer_count, kmer_count_rank))
                else:
                    KSOUT.write("%i\t%s\t%.6f\t%i\t%i\n" %(sc_rank, kmer, sc, kmer_count, kmer_count_rank))
        else:
            # Get k-mer average score ranks.
            avg_sc_rank = 0
            for kmer, sc in sorted(kmer2sc_dic.items(), key=lambda item: item[1], reverse=True):
                avg_sc_rank += 1
                kmer2avgscrank_dic[kmer] = avg_sc_rank
            sc_rank = 0
            KSOUT.write("best_score_rank\tavg_score_rank\tkmer\tbest_score\tavg_score\tavg_sc_stdev\tkmer_count\tcount_rank\n")
            for kmer, sc in sorted(kmer2bestsc_dic.items(), key=lambda item: item[1], reverse=True):
                sc_rank += 1
                kmer_count_rank = kmer2rank_dic[kmer]
                kmer_count = kmer2c_dic[kmer]
                kmer2scrank_dic[kmer] = sc_rank
                avg_sc = kmer2sc_dic[kmer]
                avg_sc_stdev = kmer2scstdev_dic[kmer]
                avg_sc_rank = kmer2avgscrank_dic[kmer]
                if lookup_kmer and kmer == lookup_kmer:
                    LKSOUT.write("%i\t%i\t%s\t%.6f\t%.6f\t%.6f\t%i\t%i\n" %(sc_rank, avg_sc_rank, kmer, sc, avg_sc, avg_sc_stdev, kmer_count, kmer_count_rank))
                else:
                    KSOUT.write("%i\t%i\t%s\t%.6f\t%.6f\t%.6f\t%i\t%i\n" %(sc_rank, avg_sc_rank, kmer, sc, avg_sc, avg_sc_stdev, kmer_count, kmer_count_rank))
        KSOUT.close()
        if lookup_kmer:
            LKSOUT.close()

        # Plot / report lookup k-mer stats.
        lookup_plot_file = False

        if lookup_kmer and not onlyseq:
            lookup_plot_file = motif_plots_folder + "/" "lookup_" + lookup_kmer + "." + plot_format
            print("Plotting %s ... " %(lookup_plot_file))
            rplib.make_motif_plot(kmer2mm_dic[kmer], ch_info_dic, lookup_plot_file,
                                   fid2stdev_dic=kmer_stdev_dic[kmer])

        # HTML report output file.
        html_report_out = args.out_folder + "/" + "report.rnaprot_eval.html"
        # Plots subfolder.
        plots_subfolder = "html_plots"
        # Get library and logo path.
        rplib_path = os.path.dirname(rplib.__file__)
        # Generate report.
        rplib.rp_eval_generate_html_report(ws_scores, neg_ws_scores,
                                           args.out_folder, rplib_path,
                                           html_report_out=html_report_out,
                                           kmer2rank_dic=kmer2rank_dic,
                                           kmer2sc_dic=kmer2sc_dic,
                                           kmer2scstdev_dic=kmer2scstdev_dic,
                                           kmer2bestsc_dic=kmer2bestsc_dic,
                                           kmer2scrank_dic=kmer2scrank_dic,
                                           kmer2avgscrank_dic=kmer2avgscrank_dic,
                                           kmer2mm_dic=kmer2mm_dic,
                                           kmer_stdev_dic=kmer_stdev_dic,
                                           kmer2bestmm_dic=kmer2bestmm_dic,
                                           kmer2c_dic=kmer2c_dic,
                                           kmer_size=args.kmer_size,
                                           ch_info_dic=ch_info_dic,
                                           top_motif_file_dic=top_motif_file_dic,
                                           bottom_motif_file_dic=bottom_motif_file_dic,
                                           kmer_top_n=c_top_kmers,
                                           add_ws_scores=add_ws_scores,
                                           idx2id_dic=idx2id_dic,
                                           seqs_dic=seqs_dic,
                                           onlyseq=onlyseq,
                                           theme=args.theme,
                                           lookup_kmer=lookup_kmer,
                                           plots_subfolder=plots_subfolder)

    print("")
    print("EVALUATION OUTPUT FILES")
    print("=======================")
    for motif_out_file in top_motif_file_dic:
        print("Top motif plot:\n%s" %(motif_out_file))
    print("Top profile plots folder:\n%s" %(top_profiles_folder))
    if args.bottoms_up:
        for motif_out_file in bottom_motif_file_dic:
            print("Bottom motif plot:\n%s" %(motif_out_file))
            print("Bottom profile plots folder:\n%s" %(bottom_profiles_folder))
    print("Whole-site scores output table file:\n%s" %(rank_scores_out))
    if add_ws_scores:
        print("Two model whole-site scores output table file:\n%s" %(two_model_scores_out))
    if args.report:
        print("Output table file with k-mer statistics:\n%s" %(kmer_stats_out))
        if lookup_kmer:
                print("Output table file with lookup k-mer statistics:\n%s" %(lookup_kmer_stats_out))
        print("Model evaluation report .html:\n%s" %(html_report_out))
    if lookup_plot_file:
        print("Look-up k-mer %s plot:\n%s" %(lookup_kmer, lookup_plot_file))
    print("")


################################################################################

def main_predict(args):
    """
    Predict binding sites (whole site or binding profiles).

    """

    print("Running for you in PREDICT mode ... ")
    if args.mode == 1:
        print("Predicting whole sites ... ")
    if args.mode == 2:
        print("Predicting profiles and top-scoring sites ... ")

    # --in folder containing prediction data.
    assert os.path.isdir(args.in_folder), "--in folder does not exist"
    # --train-in folder containing trained model.
    assert os.path.isdir(args.in_train_folder), "--train-in folder does not exist"

    # Model and model parameter file.
    params_file = args.in_train_folder + "/final.params"
    model_file = args.in_train_folder + "/final.model"
    assert os.path.isfile(params_file), "missing model training parameter file %s" %(params_file)
    assert os.path.isfile(params_file), "missing model file %s" %(model_file)
    # Read in and check model parameters.
    params_dic = rplib.read_settings_into_dic(params_file)

    assert "rnn_type" in params_dic, "rnn_type info missing in model parameter file %s" %(params_file)
    assert "n_rnn_dim" in params_dic, "n_rnn_dim info missing in model parameter file %s" %(params_file)
    assert "n_rnn_layers" in params_dic, "n_rnn_layers info missing in model parameter file %s" %(params_file)
    assert "bidirect" in params_dic, "bidirect info missing in model parameter file %s" %(params_file)
    assert "add_fc_layer" in params_dic, "add_fc_layer info missing in model parameter file %s" %(params_file)
    assert "add_feat" in params_dic, "add_feat info missing in model parameter file %s" %(params_file)
    assert "embed" in params_dic, "embed info missing in model parameter file %s" %(params_file)
    assert "embed_k" in params_dic, "embed_k info missing in model parameter file %s" %(params_file)
    assert "embed_vocab_size" in params_dic, "embed_vocab_size info missing in model parameter file %s" %(params_file)
    assert "embed_dim" in params_dic, "embed_dim info missing in model parameter file %s" %(params_file)
    assert "n_class" in params_dic, "n_class info missing in model parameter file %s" %(params_file)
    assert "n_feat" in params_dic, "n_feat info missing in model parameter file %s" %(params_file)
    assert "batch_size" in params_dic, "batch_size info missing in model parameter file %s" %(params_file)
    assert "dropout_rate" in params_dic, "dropout_rate info missing in model parameter file %s" %(params_file)

    # Store model parameters in args.
    args.rnn_type = int(params_dic["rnn_type"])
    args.n_rnn_dim = int(params_dic["n_rnn_dim"])
    args.n_rnn_layers = int(params_dic["n_rnn_layers"])
    args.bidirect = False
    if params_dic["bidirect"] == "True":
        args.bidirect = True
    args.add_fc_layer = False
    if params_dic["add_fc_layer"] == "True":
        args.add_fc_layer = True
    args.add_feat = False
    if params_dic["add_feat"] == "True":
        args.add_feat = True
    args.embed = False
    if params_dic["embed"] == "True":
        args.embed = True
    args.embed_k = int(params_dic["embed_k"])
    args.embed_vocab_size = int(params_dic["embed_vocab_size"])
    args.embed_dim = int(params_dic["embed_dim"])
    args.n_class = int(params_dic["n_class"])
    args.n_feat = int(params_dic["n_feat"])
    args.batch_size = int(params_dic["batch_size"])
    args.dropout_rate = float(params_dic["dropout_rate"])

    # Get additional model training run parameters.
    rp_train_settings_file = args.in_train_folder + "/settings.rnaprot_train.out"
    assert os.path.exists(rp_train_settings_file), "rnaprot train settings file %s not found" %(rp_train_settings_file)
    train_settings_dic = rplib.read_settings_into_dic(rp_train_settings_file)
    assert "str_mode" in train_settings_dic, "dropout_rate info missing in train run parameter file %s" %(rp_train_settings_file)
    assert "only_seq" in train_settings_dic, "only_seq info missing in train run parameter file %s" %(rp_train_settings_file)
    args.only_seq = False
    if params_dic["only_seq"] == "True":
        args.only_seq = True
    onlyseq = args.only_seq
    # Checks.
    if onlseq:
        if args.embed:
            assert args.n_feat == 1, "onlyseq = True and embed = True, but # features != 1 (found %i)" %(args.n_feat)
        else:
            assert args.n_feat == 4, "onlyseq = True and embed = False, but # features != 4 (found %i)" %(args.n_feat)
    # Structure mode.
    args.str_mode = int(train_settings_dic["str_mode"])
    str_mode_check = [1, 2, 3, 4]
    assert args.str_mode in str_mode_check, "invalid str_mode setting found in train run parameter file %s" %(rp_train_settings_file)

    # Get model feature info dictionary.
    #import ast
    #feat_info_dic = ast.literal_eval(model_params_dic["model_params_dic"])
    # Get prediction set features.

    """
    Make sure --model-in and --in features.out are compatible.
    Features model was trained on need to be available / the same
    as in --in folder.

    """

    # Get model feature infos from --model-in.
    rp_train_features_file = args.in_train_folder + "/features.out"
    train_fid_dic = {}
    with open(rp_train_features_file) as f:
        for line in f:
            cols = line.strip().split("\t")
            cols_str = ",".join(cols)
            train_fid_dic[cols[0]] = cols_str
    f.closed
    assert train_fid_dic, "train_fid_dic empty"
    assert "fa" in train_fid_dic, "sequence feature \"fa\" missing in %s" %(rp_train_features_file)
    # Get feature infos from prediction dataset --in.
    rp_gp_features_file = args.in_folder + "/features.out"
    gp_fid_dic = {}
    with open(rp_gp_features_file) as f:
        for line in f:
            cols = line.strip().split("\t")
            cols_str = ",".join(cols)
            gp_fid_dic[cols[0]] = cols_str
    f.closed
    assert gp_fid_dic, "train_fid_dic empty"
    assert "fa" in gp_fid_dic, "sequence feature \"fa\" missing in %s" %(rp_gp_features_file)

    # Pairwise feature comparison (note that --gp can have more features, but should have the same!).
    for fid in train_fid_dic:
        assert fid in gp_fid_dic, "feature ID %s found in --train-in but missing in --in features.out file. --in and --model need to be compatible with each other regarding used features" %(fid)
        # Structure can differ between gp/gt and train features.out (depending on set --str-mode in train).
        if fid != "str":
            assert train_fid_dic[fid] == gp_fid_dic[fid], "feature ID %s annotation varies in --in and --model-in features.out (\"%s\" vs \"%s\"). --in and --model need to be compatible with each other regarding used features" %(fid, gp_fid_dic[fid], train_fid_dic[fid])

    # Generate results output folder.
    out_folder = args.out_folder
    if not os.path.exists(out_folder):
        os.makedirs(out_folder)
    model_folder = args.in_train_folder

    # Whole-site predictions output files.
    ws_scores_out = out_folder + "/" + "whole_site_scores.out"
    ws_scores_bed = out_folder + "/" + "whole_site_scores.bed"


    # hallo 123

    # Profile predictions output files.
    score_profiles_out = out_folder + "/" + "profiles.winext%i.out" %(args.win_ext)
    peak_regions_bed = out_folder + "/" + "profiles.winext%i.peak_regions.bed" %(args.win_ext)
    peak_regions_ref_bed = out_folder + "/" + "profiles.winext%i.peak_regions.ref.bed" %(args.win_ext)
    peak_pos_bed = out_folder + "/" + "profiles.winext%i.peak_positions.bed" %(args.win_ext)
    peak_pos_ref_bed = out_folder + "/" + "profiles.winext%i.peak_pos.ref.bed" %(args.win_ext)
    peak_pos_ext_bed = out_folder + "/" + "profiles.winext%i.peak_pos.ext.bed" %(args.win_ext)
    peak_pos_ext_ref_bed = out_folder + "/" + "profiles.winext%i.peak_pos.ext.ref.bed" %(args.win_ext)
    peak_pos_ext_merged_bed = out_folder + "/" + "profiles.winext%i.peak_pos.ext.merged.bed" %(args.win_ext)
    peak_pos_ext_merged_ref_bed = out_folder + "/" + "profiles.winext%i.peak_pos.ext.merged.ref.bed" %(args.win_ext)

    print("Model training input folder:       %s" %(model_folder))
    print("Prediction set input folder:       %s" %(args.in_folder))
    print("Prediction results output folder:  %s" %(out_folder))

    # Window length check.
    win_len = args.win_ext * 2 + 1

    # Read in region information.
    id2chr_dic = {}
    id2s_dic = {}
    id2e_dic = {}
    id2pol_dic = {}
    test_bed_in = args.in_folder + "/" + "test.bed"
    c_shorties = 0
    # only_seq : original input were sequences.
    only_seq = True
    if os.path.exists(test_bed_in):
        only_seq = False
        with open(test_bed_in) as f:
            for line in f:
                row = line.strip()
                cols = line.strip().split("\t")
                chr_id = cols[0]
                site_s = int(cols[1])
                site_e = int(cols[2])
                site_l = site_e - site_s
                if site_l < win_len:
                    c_shorties += 1
                site_id = cols[3]
                site_pol = cols[5]
                id2chr_dic[site_id] = chr_id
                id2s_dic[site_id] = site_s
                id2e_dic[site_id] = site_e
                id2pol_dic[site_id] = site_pol
        f.closed

    # Report sites < window length.
    if c_shorties:
        print("WARNING: %i --in sites < window length encountered" %(c_shorties))

    # Output mode settings.
    settings_file = out_folder + "/settings.rnaprot_predict.out"
    SETOUT = open(settings_file, "w")
    for arg in vars(args):
        SETOUT.write("%s\t%s\n" %(arg, str(getattr(args, arg))))
    SETOUT.close()

    # Check for CUDA support.
    if torch.cuda.is_available():
        print("CUDA: I'm available. Using GPU ... ")
    else:
        print("CUDA: I'm NOT available. Using CPU ... ")

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # Load data as graphs.
    print("Load prediction data ... ")
    seqs_dic, idx2id_dic, all_features = rplib.load_predict_data(args,
                                                        store_tensors=True)
    # Number of features (feature vector length).
    check_n_feat = len(all_features[0][0])
    assert check_n_feat, "invalid check_n_feat extracted from all_features (%s)" %(check_n_feat)
    assert args.n_feat == check_n_feat, "args.n_feat != check_n_feat (%i != %i)" %(args.n_feat, check_n_feat)
    print("# dataset features:  %i" %(args.n_feat))
    print("Dataset size:        %i" %(len(all_features)))


    """
    CASE 1: whole site predictions.

    - Run whole site predictions.
    - Output whole site scores table (+ reference bed).

    """

    # Whole site predictions.
    if args.mode == 1:

        # Load whole-site dataset.
        c_all_feat = len(all_features)
        all_labels = [1]*c_all_feat
        predict_dataset = RNNDataset(all_features, all_labels)
        predict_loader = DataLoader(dataset=predict_dataset, batch_size=args.batch_size, collate_fn=rnn_util.pad_collate, pin_memory=True)

        # Predict.
        print("Predict whole sites ... ")
        ws_scores = rnn_util.test_scores(predict_loader, model, device)

        # Checks.
        c_ws_scores = len(ws_scores)
        assert c_ws_scores == c_all_feat, "# whole-site scores != size features list (%i != %i)" %(c_ws_scores, c_all_feat)

        # Output whole site scores.
        WSOUT = open(ws_scores_out, "w")
        for i,sc in enumerate(ws_scores):
            seq_id = idx2id_dic[i]
            WSOUT.write("%s\t%f\n" %(seq_id, sc))
        WSOUT.close()
        # Output BED too with scores.
        if not only_seq:
            WSOUT = open(ws_scores_bed, "w")
            for i,sc in enumerate(ws_scores):
                seq_id = idx2id_dic[i]
                chr_id = id2chr_dic[seq_id]
                site_s = id2s_dic[seq_id]
                site_e = id2e_dic[seq_id]
                site_pol = id2pol_dic[seq_id]
                WSOUT.write("%s\t%i\t%i\t%s\t%f\t%s\n" %(chr_id, site_s, site_e, seq_id, sc, site_pol))
            WSOUT.close()

    """
    CASE 2: window profile + peak predictions.

    - Predict position-wise scoring profiles in sliding window fashion,
      with window size args.win_ext * 2 + 1 (truncated at borders)
    - Based on position-wise scores extract peak regions from profiles,
      defined as regions with scores >= args.sc_thr
    - Optionally control merging of nearby peak regions (and extended
      top position sites) by --max-merge-dist
    - If genomic or transcript regions given as input, map peak regions
      and top-scoring sites to reference (update coordinates).
Sliding window:
1+20
 1+1+20
  2+1+20

and make sliding window with mean/median length of positives, good idea!
This way, the positive whole-site scores can be better compared to the
sliding window scores.

Maybe one short sliding window too ? to detect local motifs ... ?

long_slide_extlr = 20
short_slide_extlr = 4
sal_win = 40
Or by default use the median / mean length of positive seqs


win_extlr = 20

sal_win = 40


Sliding saliency:
40
 40
  40
   40
Add scores to vector.
[0]
[0,0]
[0,0,0] ...

For both long_slide, short_slide, and sal_slide we get position-wise scores
bedgraph format ? to display in IGV would be goode


After this:
Tackle eval!

    """
    peaks_found = False
    wide_peaks_found = False

    # Profile and peak predictions.
    if args.mode == 2:

        print("CALEB: I have something for you ... ")

        # Create window graphs.
        all_win_feat = []
        seq_len_list = []
        c_all_seq_len = 0
        win_extlr = args.win_ext

        print("Create windows ... ")
        for feat_list in all_features:
            l_seq = len(feat_list)
            # l_seq == window graphs for sequence.
            seq_len_list.append(l_seq)
            c_all_seq_len += l_seq
            # For each position in feat_list.
            l_feat = len(feat_list)
            for wi in range(l_feat):
                reg_s = wi - win_extlr
                reg_e = wi + win_extlr + 1
                if reg_e > l_feat:
                    reg_e = l_feat
                if reg_s < 0:
                    reg_s = 0
                #all_win_feat.append(torch.tensor(feat_list[reg_s:reg_e], dtype=torch.float))
                all_win_feat.append(feat_list[reg_s:reg_e])

        # Checks.
        c_all_win_feat = len(all_win_feat)
        assert c_all_win_feat, "no windows extracted (size of window features list == 0)"
        assert c_all_seq_len == c_all_win_feat, "total sequence length != # of windows (%i != %i)" %(c_all_seq_len, c_all_win_feat)

        # Load dataset.
        all_win_feat_labels = [1]*c_all_win_feat
        predict_dataset = RNNDataset(all_win_feat, all_win_feat_labels)
        predict_loader = DataLoader(dataset=predict_dataset, batch_size=batch_size, collate_fn=rnn_util.my_collate, pin_memory=True)

        # Predict.
        print("Predict windows ... ")
        win_scores = rnn_util.get_scores(predict_loader, model, criterion)

        # Checks.
        c_win_scores = len(win_scores)
        assert c_win_scores == c_all_seq_len, "# window graph scores != total sequence length (%i != %i)" %(c_win_scores, c_all_seq_len)

        # Create list of profile scores.
        profile_scores_ll = []
        si = 0
        for l in seq_len_list:
            se = si + l
            profile_scores_ll.append(win_scores[si:se])
            si += l
        assert profile_scores_ll, "profile_scores_ll list empty"
        assert len(profile_scores_ll) == c_ws_scores, "length profile scores list != # whole-site scores"

        """
        Extract peak regions, and store peak regions and top scoring positions
        in various output files (if not only_seq also regions mapped to reference).

        """

        PROFOUT = open(score_profiles_out, "w")
        PEAKREGOUT = open(peak_regions_bed, "w")
        PEAKPOSOUT = open(peak_pos_bed, "w")
        PEAKPOSEXTOUT = open(peak_pos_ext_bed, "w")
        if not only_seq:
            PEAKREGREFOUT = open(peak_regions_ref_bed, "w")
            PEAKPOSREFOUT = open(peak_pos_ref_bed, "w")
            PEAKPOSEXTREFOUT = open(peak_pos_ext_ref_bed, "w")

        peaks_found = 0

        for idx in idx2id_dic:
            seq_id = idx2id_dic[idx]
            seq = seqs_dic[seq_id]
            seq_l = len(seq)
            scl_l = len(profile_scores_ll[idx])
            assert seq_l == scl_l, "differing lengths for sequence and profile scores list (seq_id: %s, %i != %i)" %(seq_id, seq_l, scl_l)

            if not only_seq:
                chr_id = id2chr_dic[seq_id]
                ref_s = id2s_dic[seq_id]
                ref_e = id2e_dic[seq_id]
                ref_pol = id2pol_dic[seq_id]

            seq_list = list(seq)

            PROFOUT.write(">%s\n" %(seq_id))
            for i,sc in enumerate(profile_scores_ll[idx]):
                nt = seq_list[i]
                pos = i + 1
                PROFOUT.write("%i\t%s\t%f\n" %(pos, nt, sc))

            # Get peak regions + positions.
            peak_list = rplib.list_extract_peaks(profile_scores_ll[idx],
                                                 max_merge_dist=args.max_merge_dist,
                                                 coords="bed",
                                                 sc_thr=args.sc_thr)

            # Peak count.
            peak_i = 0
            # Output peak regions.
            for peak_reg in peak_list:
                peak_s = peak_reg[0]
                peak_e = peak_reg[1]
                top_pos = peak_reg[2]
                top_sc = peak_reg[3]
                top_pos_s = top_pos - 1
                top_pos_ext_s = top_pos_s - args.win_ext
                top_pos_ext_e = top_pos + args.win_ext
                if top_pos_ext_s < 0:
                    top_pos_ext_s = 0
                if top_pos_ext_e > seq_l:
                    top_pos_ext_e = seq_l
                tp_ext_l = top_pos_ext_e - top_pos_ext_s
                peak_i += 1
                peaks_found += 1
                peak_id = "peak_%i" %(peak_i)
                PEAKREGOUT.write("%s\t%i\t%i\t%s,%i,%f\t0\t+\n" %(seq_id, peak_s, peak_e, peak_id, top_pos, top_sc))
                PEAKPOSOUT.write("%s\t%i\t%i\t%s,%i\t%f\t+\n" %(seq_id, top_pos_s, top_pos, peak_id, top_pos, top_sc))
                PEAKPOSEXTOUT.write("%s\t%i\t%i\t%s,%i\t%f\t+\n" %(seq_id, top_pos_ext_s, top_pos_ext_e, peak_id, top_pos, top_sc))
                # Reference coordinates.
                if not only_seq:
                    new_s, new_e = rplib.bed_convert_coords(peak_s, peak_e, ref_s, ref_e, ref_pol)
                    new_top_s, new_top_e = rplib.bed_convert_coords(top_pos-1, top_pos, ref_s, ref_e, ref_pol)
                    new_top_ext_s = new_top_s - args.win_ext
                    new_top_ext_e = new_top_e + args.win_ext
                    if new_top_ext_s < ref_s:
                        new_top_ext_s = ref_s
                    if new_top_ext_e > ref_e:
                        new_top_ext_e = ref_e
                    # Check.
                    tp_ext_ref_l = new_top_ext_e - new_top_ext_s
                    assert tp_ext_ref_l == tp_ext_l, "tp_ext_l != tp_ext_ref_l (%i != %i)" %(tp_ext_ref_l, tp_ext_l)
                    PEAKREGREFOUT.write("%s\t%i\t%i\t%s,%i,%f\t0\t%s\n" %(chr_id, new_s, new_e, peak_id, new_top_e, top_sc, ref_pol))
                    PEAKPOSREFOUT.write("%s\t%i\t%i\t%s,%i\t%f\t%s\n" %(chr_id, new_top_s, new_top_e, peak_id, new_top_e, top_sc, ref_pol))
                    PEAKPOSEXTREFOUT.write("%s\t%i\t%i\t%s,%i\t%f\t%s\n" %(chr_id, new_top_ext_s, new_top_ext_e, peak_id, new_top_e, top_sc, ref_pol))

            if not peak_i:
                print("WARNING: no profile peaks found for sequence %s ... " %(seq_id))

        PROFOUT.close()
        PEAKREGOUT.close()
        PEAKPOSOUT.close()
        PEAKPOSEXTOUT.close()
        if not only_seq:
            PEAKREGREFOUT.close()
            PEAKPOSREFOUT.close()
            PEAKPOSEXTREFOUT.close()

        if not peaks_found:
            print("WARNING: no profile peaks found for any input sequence!")
            print("         Try with lower --thr to obtain profile peaks for the given sequences")
            sys.exit()

        # Merge extended peaks.
        rplib.bed_sort_merge_output_top_entries(peak_pos_ext_bed, peak_pos_ext_merged_bed)
        if only_seq:
            if os.path.exists(peak_regions_ref_bed):
                os.remove(peak_regions_ref_bed)
            if os.path.exists(peak_pos_ref_bed):
                os.remove(peak_pos_ref_bed)
            if os.path.exists(peak_pos_ext_ref_bed):
                os.remove(peak_pos_ext_ref_bed)
        else:
            rplib.bed_sort_merge_output_top_entries(peak_pos_ext_ref_bed, peak_pos_ext_merged_ref_bed)


    """
    Output file summary.

    """

    if args.mode == 1:
        print("")
        print("WHOLE-SITE PREDICTION OUTPUT FILES")
        print("==================================")
        print("Whole site predictions output file:\n%s" %(ws_scores_out))
        if not only_seq:
            print("Whole site predictions BED file:\n%s" %(ws_scores_bed))
    if args.mode == 2:
        print("")
        print("PROFILE PREDICTION OUTPUT FILES")
        print("===============================")
        print("Profiles output file:\n%s" %(score_profiles_out))
        print("Peak regions (regions with scores above --thr):\n%s" %(peak_regions_bed))
        if not only_seq:
            print("Peak regions on reference:\n%s" %(peak_regions_ref_bed))
        print("Peak regions top scoring positions:\n%s" %(peak_pos_bed))
        if not only_seq:
            print("Peak regions top scoring positions on reference:\n%s" %(peak_pos_ref_bed))
        print("Peak regions extended top scoring positions:\n%s" %(peak_pos_ext_bed))
        if not only_seq:
            print("Peak regions extended top scoring positions on reference:\n%s" %(peak_pos_ext_ref_bed))
        print("Peak regions extended+merged top scoring positions:\n%s" %(peak_pos_ext_merged_bed))
        if not only_seq:
            print("Peak regions extended+merged top scoring positions on reference:\n%s" %(peak_pos_ext_merged_ref_bed))
    print("")


################################################################################

def main_gt(args):
    """
    Generate a training set.

    """

    print("Running for you in GT mode ... ")

    # Generate results output folder.
    if not os.path.exists(args.out_folder):
        os.makedirs(args.out_folder)

    """
    Output files.

    """
    # BED sites files.
    pos_bed_out = args.out_folder + "/" + "positives.bed"
    neg_bed_out = args.out_folder + "/" + "negatives.bed"
    # FASTA sequences of sites files.
    pos_fasta_out = args.out_folder + "/" + "positives.fa"
    neg_fasta_out = args.out_folder + "/" + "negatives.fa"
    # Conservation annotation files.
    pos_pc_con_out = args.out_folder + "/" + "positives.pc.con"
    neg_pc_con_out = args.out_folder + "/" + "negatives.pc.con"
    pos_pp_con_out = args.out_folder + "/" + "positives.pp.con"
    neg_pp_con_out = args.out_folder + "/" + "negatives.pp.con"
    # Transcript region annotation files.
    pos_tra_out = args.out_folder + "/" + "positives.tra"
    neg_tra_out = args.out_folder + "/" + "negatives.tra"
    # Repeat region annotation files.
    pos_rra_out = args.out_folder + "/" + "positives.rra"
    neg_rra_out = args.out_folder + "/" + "negatives.rra"
    # Exon-intron annotation files.
    pos_eia_out = args.out_folder + "/" + "positives.eia"
    neg_eia_out = args.out_folder + "/" + "negatives.eia"
    # Secondary structure annotation files.
    pos_str_out = args.out_folder + "/" + "positives.str"
    neg_str_out = args.out_folder + "/" + "negatives.str"
    # Mode settings file.
    settings_file = args.out_folder + "/" + "/settings.rnaprot_gt.out"
    # Chromosome or transcript lengths file.
    chr_lengths_file = args.out_folder + "/" + "reference_lengths.out"
    # Genes or transcript regions containing positives.
    regions_with_positives_bed = args.out_folder + "/" + "regions_with_positives.bed"
    # Feature table.
    feat_table_out = args.out_folder + "/" + "features.out"

    """
    Temporary output files.

    """
    # tmp files for processing --in sites.
    tmp1_bed = args.out_folder + "/" + "sites1.tmp.bed"
    tmp2_bed = args.out_folder + "/" + "sites2.tmp.bed"
    tmp3_bed = args.out_folder + "/" + "sites3.tmp.bed"
    # tmp files for processing --neg-in sites.
    neg_tmp1_bed = args.out_folder + "/" + "neg_sites1.tmp.bed"
    neg_tmp2_bed = args.out_folder + "/" + "neg_sites2.tmp.bed"
    neg_tmp3_bed = args.out_folder + "/" + "neg_sites3.tmp.bed"
    # Gene regions from --gtf.
    gene_regions_gtf_tmp_bed = args.out_folder + "/" + "gene_regions.gtf.tmp.bed"
    # Positives with context +col5 score = 0.
    positives_with_context_bed = args.out_folder + "/" + "positives_with_context.tmp.bed"
    # Negatives with context +col5 score = 0.
    negatives_with_context_bed = args.out_folder + "/" + "negatives_with_context.tmp.bed"
    # Regions for masking (no negatives from these regions).
    mask_negatives_bed = args.out_folder + "/" + "mask_negatives.tmp.bed"
    # Positive sites to shuffle / generate negatives from using bedtools shuffle.
    shuffle_in_bed = args.out_folder + "/" + "shuffle_in.tmp.bed"
    # Random negative sites returned from bedtools shuffle.
    random_negatives_bed = args.out_folder + "/" + "random_negatives.tmp.bed"
    # Random negative sites with unique IDs.
    random_negatives_unique_ids_bed = args.out_folder + "/" + "random_negatives_unique_ids.tmp.bed"
    # Extracted sequences for pos/neg sites.
    pos_tmp_fasta = args.out_folder + "/" + "positives.tmp.fa"
    neg_tmp_fasta = args.out_folder + "/" + "negatives.tmp.fa"
    # Most prominent transcripts BED regions.
    most_prominent_transcripts_bed = args.out_folder + "/" + "most_prominent_transcripts.tmp.bed"
    # Most prominent genes BED regions.
    most_prominent_genes_bed = args.out_folder + "/" + "most_prominent_genes.tmp.bed"
    # Transcript sites mapped to genome.
    pos_transcript_to_genome_bed = args.out_folder + "/" + "positives.transcript_to_genome.bed"
    neg_transcript_to_genome_bed = args.out_folder + "/" + "negatives.transcript_to_genome.bed"

    # Write generated feature infos to table.
    FEATOUT = open(feat_table_out, "w")

    """
    CASE 1: sequences provided as input (--in FASTA)
    ================================================

    --in FASTA and optionally --neg-fa are set.

    If --in FASTA is set, create negative sequences by shuffling
    Use --neg-fa FASTA to provide negative sequences (instead of
    shuffling positives to generate negatives)
    Supported additional features if --in FASTA:
    --str (secondary structure features)

    """

    # Check if --in file is FASTA file.
    if rplib.fasta_check_fasta_file(args.in_sites):
        # Read in sequences (this also checks for unique headers + converts to RNA).
        pos_seqs_dic = rplib.read_fasta_into_dic(args.in_sites)
        # Sequence character counts.
        seq_cc_dic = rplib.seqs_dic_count_chars(pos_seqs_dic)
        # Allowed sequence characters.
        allowed_nt_dic = {'A': 1, 'C': 1, 'G': 1, 'U': 1}
        for nt in seq_cc_dic:
            if nt not in allowed_nt_dic:
                assert False, "--in sequences contain \"%s\" character (allowed characters: A C G U)" %(nt)
        FEATOUT.write("fa\tC\tA,C,G,U\t-\n")

        # If negative sequences are given, no shuffling of positive sequences necessary to produce negatives.
        if args.in_neg_sites:
            assert os.path.exists(args.in_neg_sites), "--neg-in file \"%s\" not found" %(args.in_neg_sites)
            assert rplib.fasta_check_fasta_file(args.in_neg_sites), "--in FASTA requires --neg-in in same format (FASTA)"
            print("Generating training set from --in and --neg-in ... ")
            neg_seqs_dic = rplib.read_fasta_into_dic(args.in_neg_sites,
                                                     all_uc=True)
        else:
            print("Generating training set from --in (shuffling positives to get negatives) ... ")
            print("Set shuffling method: %i-nucleotide shuffling" %(args.shuffle_k))
            # Shuffle positives to generate negatives.
            neg_seqs_dic = rplib.ushuffle_sequences(pos_seqs_dic,
                                                    new_ids=True,
                                                    id_prefix="shuff_neg",
                                                    ushuffle_k=args.shuffle_k)
        # Output processed sequences.
        rplib.fasta_output_dic(pos_seqs_dic, pos_fasta_out, split=True)
        rplib.fasta_output_dic(neg_seqs_dic, neg_fasta_out, split=True)

        # Secondary structure stats dictionaries.
        pos_str_stats_dic = None
        neg_str_stats_dic = None

        # If structure features should be computed.
        if args.add_str:
            FEATOUT.write("str\tN\tE,H,I,M,S\tprob\n")
            # If report=True, collect statistics.
            if args.report:
                pos_str_stats_dic = {}
                neg_str_stats_dic = {}
            # Calculate structure features for positives.
            print("Get secondary structure features for positive set ... ")
            rplib.calc_str_elem_p(pos_fasta_out, pos_str_out,
                                  stats_dic=pos_str_stats_dic,
                                  plfold_u=args.plfold_u,
                                  plfold_l=args.plfold_l,
                                  plfold_w=args.plfold_w)
            # Calculate structure features for negatives.
            print("Get secondary structure features for negative set ... ")
            rplib.calc_str_elem_p(neg_fasta_out, neg_str_out,
                                  stats_dic=neg_str_stats_dic,
                                  plfold_u=args.plfold_u,
                                  plfold_l=args.plfold_l,
                                  plfold_w=args.plfold_w)

        # Sequence set stats.
        c_pos_out = len(pos_seqs_dic)
        c_neg_out = len(neg_seqs_dic)
        if args.report:
            # HTML report output file.
            html_report_out = args.out_folder + "/" + "report.rnaprot_gt.html"
            # Plots subfolder.
            plots_subfolder = "plots_rnaprot_gt"
            # Get library and logo path.
            rplib_path = os.path.dirname(rplib.__file__)
            # Generate report.
            rplib.rp_gt_generate_html_report(pos_seqs_dic, neg_seqs_dic, args.out_folder,
                                             "s", rplib_path,
                                             plots_subfolder=plots_subfolder,
                                             html_report_out=html_report_out,
                                             pos_str_stats_dic=pos_str_stats_dic,
                                             neg_str_stats_dic=neg_str_stats_dic,
                                             theme=args.theme,
                                             kmer_top=10,
                                             rna=True)

        # Mode settings output file.
        SETOUT = open(settings_file, "w")
        for arg in vars(args):
            SETOUT.write("%s\t%s\n" %(arg, str(getattr(args, arg))))
        SETOUT.close()

        print("# positive sequences output: %i" %(c_pos_out))
        print("# negative sequences output: %i" %(c_neg_out))
        print("")
        print("TRAINING SET OUTPUT FILES")
        print("=========================")
        print("Positives sequences .fa:\n%s" %(pos_fasta_out))
        print("Negatives sequences .fa:\n%s" %(neg_fasta_out))
        if args.add_str:
            print("Positives position-wise structural elements probabilities .str:\n%s" %(pos_str_out))
            print("Negatives position-wise structural elements probabilities .str:\n%s" %(neg_str_out))
        if args.report:
            print("Training set generation report .html:\n%s" %(html_report_out))
        print("")
        FEATOUT.close()
        sys.exit()

    """
    CASE 2: genomic or transcript sites given (--in BED)
    ====================================================

    This means:
        - Negatives have to be generated by random sampling,
          unless --neg-in BED is set (== no random negative generation).
        - Additional features can be added

    For genomic sites, the features are:
        - Position-wise exon-intron annotation (with options)
        - Position-wise transcript region annotation (with options)
        - Position-wise repeat region annotation
        - Position-wise conservation scores (phasCons and phyloP)
        - Structure features (base pairs and structural elements probabilities)

    For transcript sites, the features are:
        - Position-wise transcript region annotation (with options)
        - Position-wise repeat region annotation
        - Position-wise conservation scores (phasCons and phyloP)
        - Structure features (base pairs and structural elements probabilities)

    NOTE that exon-intron features make no sense for transcript sites, as each
    position would get the exon label (thus omitted for transcript sites)

    Negatives generation differs between genomic + transcript sites:
    For genomic sites, by default negative sites are selected randomly
    from gene regions containing positive sites.
    For transcript sites, by default negatives are selected randomly
    from transcripts containing positive sites.

    In case not enough random negatives can be extracted from the described
    regions, the search space is expanded:
    - For genomic sites, all gene regions with TSL1-5 transcripts will
      be extracted from the --gtf, and these regions will then be used
      for negatives extraction.
    - For transcript sites, all TSL1-5 transcripts (selecting the transcript
      with highest experimental evidence for each gene) will be used for
      negatives extraction.

    Regarding negative region lengths:
    Currently the region lengths correspond to the positive region lengths
    as bedtools shuffle keeps the lengths for each shuffled region.
    Future implementations could additionally include e.g.:
    The median positive length or expected positive length (after
    --seq-ext, if --mode 1)

    """
    # If --in are BED, require --gtf and --gen.
    if not args.in_gtf:
        print("ERROR: --gtf GTF file is required if --in receives BED file")
        sys.exit()
    if not args.in_2bit:
        print("ERROR: --gen .2bit file is required if --in receives BED file")
        sys.exit()
    # Some sanity checks regarding set values.
    if args.mode == 2:
        if (args.min_len-1) % 2: # if number even.
            assert False, "currently only uneven --min-len values supported"
        if args.min_len < 11 or args.min_len > 201:
            assert False, "use reasonable --min-len values (>= 11, <= 201)"
    min_len_ext = int( (args.min_len - 1) / 2)
    # Maximum site length after applying --seq-ext and --con-ext.
    max_site_length = 500
    if args.max_len > max_site_length:
        assert False, "use reasonable --max-len values for limiting input site lengths (<= %i)" %(max_site_length)
    # Expected site length after --seq-ext.
    exp_site_length = args.seq_ext * 2 + 1
    if exp_site_length > max_site_length:
        assert False, "use reasonable site extension values (--seq-ext) (<= --max-len)"
    if args.add_str:
        assert args.plfold_l < args.plfold_w, "--plfold-l must be smaller than --plfold-w"

    # Input checks.
    assert os.path.exists(args.in_sites), "--in BED file \"%s\" not found" %(args.in_sites)
    assert os.path.exists(args.in_gtf), "--gtf GTF file \"%s\" not found" %(args.in_gtf)
    assert os.path.exists(args.in_2bit), "--gen .2bit file \"%s\" not found" %(args.in_2bit)
    assert rplib.bed_check_six_col_format(args.in_sites), "--in BED file appears to be not in 6-column BED format"
    if args.mask_bed:
        assert os.path.exists(args.mask_bed), "--mask-bed BED file \"%s\" not found" %(args.mask_bed)
        assert rplib.bed_check_six_col_format(args.mask_bed), "--mask-bed BED file appears to be not in 6-column BED format"
    if args.pc_bw:
        assert os.path.exists(args.pc_bw), "--phastcons file \"%s\" not found" %(args.pc_bw)
    if args.pp_bw:
        assert os.path.exists(args.pp_bw), "--phylop file \"%s\" not found" %(args.pp_bw)
    if args.tr_reg_codon_annot:
        assert args.tr_reg_annot, "--tra-codons requires --tra to be effective"
    if args.tr_reg_border_annot:
        assert args.tr_reg_annot, "--tra-borders requires --tra to be effective"
    if args.intron_border_annot:
        assert args.exon_intron_annot, "--eia-ib requires --eia to be effective"
    if args.eia_all_ex:
        assert args.exon_intron_annot, "--eia-all-ex requires --eia to be effective"
    if args.exon_intron_n:
        assert args.exon_intron_annot, "--eia-n requires --eia to be effective"
    if args.eia_all_ex:
        assert not args.intron_border_annot, "--eia-all-ex incompatible with --eia-ib"
        assert not args.exon_intron_n, "--eia-all-ex incompatible with --eia-n"

    # Additional feature checks.
    if args.feat_in_1h and not args.feat_in:
        assert False, "--feat-in-1h requires --feat-in"
    if args.feat_in_norm and not args.feat_in:
        assert False, "--feat-in-norm requires --feat-in"
    if args.feat_in:
        assert rplib.bed_check_six_col_format(args.feat_in, nr_cols=4), "--feat-in table file: 4 columns (data ID, data type, BED path) expected"
        # Check feature ID lengths and BED file paths.
        max_feat_id_len = 20
        with open(args.feat_in) as f:
            for line in f:
                row = line.strip()
                cols = line.strip().split("\t")
                feat_id = cols[0]
                bed_file = cols[3]
                assert len(feat_id) <= max_feat_id_len, "in --feat-in: column 1 feature ID \"%s\" length > 20 encountered (use shorter ID length)" %(feat_id)
                assert os.path.exists(bed_file), "--feat-in column 4 BED file \"%s\" does not exist" %(bed_file)
        f.closed

    # If --in column 4 IDs should be kept, check their uniqueness.
    if args.keep_ids:
        assert rplib.bed_check_unique_ids(args.in_sites), "--in BED file \"%s\" column 4 IDs not unique. Disable --keep-ids or provide unique IDs" %(args.in_sites)
        if args.in_neg_sites:
            assert rplib.bed_check_unique_ids(args.in_neg_sites), "--neg-in BED file \"%s\" column 4 IDs not unique. Disable --keep-ids or provide unique IDs" %(args.in_neg_sites)
            assert rplib.bed_check_unique_ids_two_files(args.in_sites, args.in_neg_sites), "combined column 4 IDs of --neg-in BED and --in BED not unique. Disable --keep-ids or provide unique IDs"

    # Quick check what remains after applying score threshold.
    if args.sc_thr is not None:
        c_rem = rplib.bed_get_score_filtered_count(args.in_sites, args.sc_thr,
                                                   rev_filter=args.rev_filter)
        if not c_rem:
            print("ERROR: no remaining sites after filtering by set --thr score. Use lower score threshold or disable --thr")
            sys.exit()

    # Initial --neg-in checks.
    if args.in_neg_sites:
        assert os.path.exists(args.in_neg_sites), "--neg-in file \"%s\" not found" %(args.in_neg_sites)
        assert not rplib.fasta_check_fasta_file(args.in_neg_sites), "--neg-in seems to be in FASTA format, but BED format expected"
        assert rplib.bed_check_six_col_format(args.in_neg_sites), "--neg-in BED file appears to be not in 6-column BED format"

    # Mode settings output file.
    SETOUT = open(settings_file, "w")
    for arg in vars(args):
        SETOUT.write("%s\t%s\n" %(arg, str(getattr(args, arg))))
    SETOUT.close()

    # Get chromsome lengths from --gen.
    print("Get chromosome lengths from --gen ... ")
    chr_len_dic = rplib.get_chromosome_lengths_from_2bit(args.in_2bit, chr_lengths_file)
    gen_chr_len_dic = chr_len_dic

    # Get chromosome IDs.
    print("Get chromosome IDs from --in ... ")
    chr_ids_dic = rplib.bed_get_chromosome_ids(args.in_sites)

    # Check whether IDs are genomic or not.
    transcript_regions = False
    for chr_id in chr_ids_dic:
        if chr_id not in chr_len_dic:
            transcript_regions = True
            break
    # Demand pure transcript or genomic sites --in BED.
    if transcript_regions:
        for chr_id in chr_ids_dic:
            assert chr_id not in chr_len_dic, "chromosome and non-chromosome IDs encountered in --in BED column 1. --in column 1 must contain either chromsome or transcript IDs (conflicting IDs: \"%s\", \"%s\")" %(transcript_regions, chr_id)
        print("No chromosome IDs found in --in, interpret column 1 IDs as transcript IDs ... ")
    # Demand same thing for negative sites.
    if args.in_neg_sites:
        neg_chr_ids_dic = rplib.bed_get_chromosome_ids(args.in_neg_sites)
        for chr_id in neg_chr_ids_dic:
            if transcript_regions:
                assert chr_id not in chr_len_dic, "--in BED contains transcript sites, whereas --neg-in also contains chromosome IDs in BED column 1. Use either transcript or genomic sites for both --in and --neg-in"
            else:
                assert chr_id in chr_len_dic, "--in BED contains genomic sites, whereas --neg-in also contains non-chromosome IDs in BED column 1. Use either transcript or genomic sites for both --in and --neg-in"

    # If transcript_regions, there should be no genomic col1 IDs.
    tr_seqs_dic = {}
    if transcript_regions:
        # Check for set --exon-intron.
        if args.exon_intron_annot:
            print("ERROR: setting --eia is useless for transcript sites. Please disable or provide genomic input sites")
            sys.exit()
        if args.intron_border_annot:
            print("ERROR: setting --eia-ib is useless for transcript sites. Please disable or provide genomic input sites")
            sys.exit()
        if args.exon_intron_n:
            print("ERROR: setting --eia-ib is useless for transcript sites. Please disable or provide genomic input sites")
            sys.exit()
        # Read in transcript lengths from --gtf.
        print("Get transcript lengths from --gtf ... ")
        tr_len_dic = rplib.gtf_get_transcript_lengths(args.in_gtf)
        print("Verifying transcript IDs by checking their presence in --gtf ... ")
        for tr_id in chr_ids_dic:
            assert tr_id in tr_len_dic, "--in column 1 transcript ID \"%s\" not found in --gtf" %(tr_id)
        # Make a transcript regions .bed file.
        print("Output transcripts containing positive sites to BED ... ")
        rplib.bed_sequence_lengths_to_bed(tr_len_dic, regions_with_positives_bed,
                                           ids_dic=chr_ids_dic)
        if args.in_neg_sites:
            for tr_id in neg_chr_ids_dic:
                assert tr_id in tr_len_dic, "--neg-in column 1 transcript ID \"%s\" not found in --gtf" %(tr_id)
            chr_ids_dic.update(neg_chr_ids_dic)
        # Get spliced transcript sequences from gtf+2bit.
        print("Get transcript sequences from --gtf and --gen ... ")
        tr_seqs_dic = rplib.get_transcript_sequences_from_gtf(args.in_gtf, args.in_2bit,
                                                              lc_repeats=args.rep_reg_annot,
                                                              tr_ids_dic=chr_ids_dic)
        # Output transcript lengths file.
        print("Output transcript lengths file ... ")
        rplib.output_chromosome_lengths_file(tr_len_dic, chr_lengths_file)
        # Make a new chr_len_dic, only with transcript IDs from --in and their lengths.
        chr_len_dic = {}
        for tr_id in chr_ids_dic:
            tr_len = tr_len_dic[tr_id]
            chr_len_dic[tr_id] = tr_len
        # Update chromosome lengths dictionary.
        # chr_len_dic.update(tr_len_dic)
    else:
        # Genomic sites checks.
        if args.tr_reg_border_annot:
            print("ERROR: setting --tra-borders only supported for transcript sites. Please disable or provide transcript input sites.")
            sys.exit()

    """
    Process positives
    =================

    - Filter and extend positives
    - if --neg-in, filter and extend negatives as well
    - select best overlapping sites for --in, not for --neg-in
    - Disable overlap selection by --allow-overlaps

    """

    # Process --in positives.
    ps_count_dic = {}
    id2pl_dic = rplib.process_in_sites(args.in_sites, tmp1_bed, chr_len_dic, args,
                                    transcript_regions=transcript_regions,
                                    count_dic=ps_count_dic,
                                    id_prefix="CLIP")
    print("# positive sites found in --in:             %i" %(ps_count_dic['c_in']))
    if ps_count_dic['c_filt_max_len']:
        print("# positive sites filtered by --max-len:     %i" %(ps_count_dic['c_filt_max_len']))
    if ps_count_dic['c_filt_ref']:
        print("# positive sites removed (reference ID):    %i" %(ps_count_dic['c_filt_ref']))
    if ps_count_dic['c_filt_thr']:
        print("# positive sites removed (--thr):           %i" %(ps_count_dic['c_filt_thr']))
    assert ps_count_dic['c_out'], "no positive sites remaining after filtering and extension ... "
    print("# positive sites read in (post-filtering):  %i" %(ps_count_dic['c_out']))

    # If --neg-in negative sites given, process same as --in.
    if args.in_neg_sites:
        ns_count_dic = {}
        id2pl_dic = rplib.process_in_sites(args.in_neg_sites, neg_tmp1_bed, chr_len_dic, args,
                                        transcript_regions=transcript_regions,
                                        id2pl_dic=id2pl_dic,
                                        count_dic=ns_count_dic,
                                        id_prefix="neg")

        print("# negative sites found in --neg-in:         %i" %(ns_count_dic['c_in']))

        if ns_count_dic['c_filt_max_len']:
            print("# negative sites filtered by --max-len:     %i" %(ns_count_dic['c_filt_max_len']))
        if ns_count_dic['c_filt_ref']:
            print("# negative sites removed (reference ID):    %i" %(ns_count_dic['c_filt_ref']))
        if ns_count_dic['c_filt_thr']:
            print("# negative sites filtered by --thr:         %i" %(ns_count_dic['c_filt_thr']))
        assert ns_count_dic['c_out'], "no negative sites remaining after filtering and extension ... "
        print("# negative sites read in (post-filtering):  %i" %(ns_count_dic['c_out']))

    # Check whether scores work for best overlap sites selection.
    if not args.allow_overlaps:
        print("Select highest-scoring sites in case of overlaps ... ")
        # Check if scores are there.
        sc2c_dic = rplib.bed_get_score_to_count_dic(tmp1_bed)
        # Warning in case no col5 scores are present (e.g. each row with score 0).
        if len(sc2c_dic) == 1:
            print("WARNING: --in BED regions all contain the same column 5 score!")
            print("         To select best sites in case of overlaps, please add")
            print("            distinct BED column 5 scores to --in BED file.")
            print("                    Enabling --allow-overlaps ... ")
            args.allow_overlaps = True
    # If overlaps between sites allowed (should work with transcript + genomic sites).
    if args.allow_overlaps:
        rplib.make_file_copy(tmp1_bed, tmp2_bed)
    else:
        # Merge overlapping sites, selecting best scoring site for each overlap group.
        rplib.bed_sort_merge_output_top_entries(tmp1_bed, tmp2_bed,
                                                rev_filter=args.rev_filter)

    # For genomic sites, extract gene regions containing positives.
    if not transcript_regions:
        # Get gene regions from .gtf.
        print("Extract gene regions from --gtf ... ")
        rplib.gtf_extract_gene_bed(args.in_gtf, gene_regions_gtf_tmp_bed)
        # Get gene regions containing positives.
        params = "-u -s"
        print("Get gene regions containing positive sites ... ")
        rplib.intersect_bed_files(gene_regions_gtf_tmp_bed, args.in_sites, params, regions_with_positives_bed)
        # Filter positives by gene coverage.
        if args.no_gene_filter:
            rplib.make_file_copy(tmp2_bed, tmp3_bed)
        else:
            # Accept only --in sites that overlap with gene regions.
            print("Filter positive sites by gene overlap ... ")
            params = "-u -s"
            rplib.intersect_bed_files(tmp2_bed, gene_regions_gtf_tmp_bed, params, tmp3_bed)
            c_out = rplib.count_file_rows(tmp3_bed)
            assert c_out, "no remaining positive sites after filtering by gene overlap"
            print("# positive sites after filtering by gene coverage:  %i" %(c_out))
    else:
        rplib.make_file_copy(tmp2_bed, tmp3_bed)

    """
    Revise positives
    ================

    - Store rows with original scores in dictionary id2row_dic.
    - Make BED col5 scores = 0 for sequence extraction.
    - if --neg-in, revise --neg-in same way.

    """

    id2row_dic = rplib.revise_in_sites(tmp3_bed, positives_with_context_bed,
                                       chr_len_dic, id2pl_dic, args,
                                       transcript_regions=transcript_regions)
    assert id2row_dic, "no positive sites remaining after revision"
    if args.in_neg_sites:
        id2row_neg_dic = rplib.revise_in_sites(neg_tmp1_bed, negatives_with_context_bed,
                                               chr_len_dic, id2pl_dic, args,
                                               transcript_regions=transcript_regions)
        assert id2row_neg_dic, "no negative sites (--neg-in) remaining after revision"
        print("# selected positive sites:  %i" %(len(id2row_dic)))
        print("# selected negative sites:  %i" %(len(id2row_neg_dic)))
    else:
        print("# selected positive sites to generate negatives for:  %i" %(len(id2row_dic)))

    """
    Generate negatives
    ==================

    - Collect files with regions for masking and merge them.
    - Generate a file with sites to shuffle (use args.neg_factor
      times input sites) (shuffle_in_bed)
    - Use bedtools shuffle to generate random negatives in the
      regions of interest, without overlap with masked regions.
      In case # of random negatives != requested number, retry
      with all most prominent transcripts (transcript_regions=True),
      or all genes with TSL1 transcripts as search space
      (if --in sites are genomic).
    - Make random negative IDs unique.
    - Read in positive + negative region lengths.

    """
    # If requested number of negatives could not be extracted.
    round_two = False
    if not args.in_neg_sites:
        # Collect masking files and merge them to BED.
        print("Collect regions for masking (== no negatives from these regions) ... ")
        mask_files_list = []
        mask_files_list.append(args.in_sites)
        mask_files_list.append(positives_with_context_bed)
        if args.mask_bed:
            mask_files_list.append(args.mask_bed)
        rplib.merge_files(mask_files_list, mask_negatives_bed)
        # Generate shuffle file.
        shuffle_list = []
        # Generate random negatives, neg_factor times number of positives.
        for i in range(args.neg_factor):
            shuffle_list.append(positives_with_context_bed)
        rplib.merge_files(shuffle_list, shuffle_in_bed)

        # Generate negatives.
        print("Extract random negative regions ... ")
        check = rplib.bed_generate_random_negatives(shuffle_in_bed, chr_lengths_file, random_negatives_bed,
                                                    incl_bed=regions_with_positives_bed,
                                                    excl_bed=mask_negatives_bed)
        if not check:
            print("# random negatives extracted < # requested. Expand extraction space ... ")
            round_two = True
            if transcript_regions:
                # Get most prominent transcript regions.
                print("Get most prominent transcripts (TSL1-5) from --gtf ... ")
                mp_ids_dic = {}
                rplib.gtf_extract_most_prominent_transcripts(args.in_gtf, "dummy",
                                                             return_ids_dic=mp_ids_dic,
                                                             set_ids_dic=chr_ids_dic,
                                                             strict=True)
                assert mp_ids_dic, "no transcript IDs read into dictionary"
                # Output most prominent transcripts to BED.
                rplib.bed_sequence_lengths_to_bed(mp_ids_dic, most_prominent_transcripts_bed)
                # Rerun.
                print("Extract random negatives from most prominent transcripts ... ")
                check = rplib.bed_generate_random_negatives(shuffle_in_bed, chr_lengths_file, random_negatives_bed,
                                                    incl_bed=most_prominent_transcripts_bed,
                                                    excl_bed=mask_negatives_bed)
                # If still not enough, terminate.
                assert check, "still not enough random negatives after second round with most prominent transcripts (try with lower --neg-factor)"
                # Get list of transcript IDs with negative sites.
                neg_ids_dic = rplib.bed_get_chromosome_ids(random_negatives_bed)
                # Update transcript sequences dictionary.
                print("Get transcript sequences containing negative sites ... ")
                neg_tr_seqs_dic = rplib.get_transcript_sequences_from_gtf(args.in_gtf, args.in_2bit,
                                                                          lc_repeats=args.rep_reg_annot,
                                                                          tr_ids_dic=neg_ids_dic)
            else:
                # Get TSL gene regions.
                print("Get gene regions with TSL transcript support from --gtf ... ")
                rplib.gtf_extract_tsl_gene_bed(args.in_gtf, most_prominent_genes_bed,
                                               strict=True)
                # Rerun.
                print("Extract random negatives from gene regions with TSL transcript support ... ")
                check = rplib.bed_generate_random_negatives(shuffle_in_bed, chr_lengths_file, random_negatives_bed,
                                                    incl_bed=most_prominent_genes_bed,
                                                    excl_bed=mask_negatives_bed)
                # If still not enough, terminate.
                assert check, "still not enough random negatives after second round with most prominent genes (try with lower --neg-factor)"

        # Make unique IDs for extracted random negatives.
        new2oldid_dic = {}
        print("Make random negative IDs unique ... ")
        rplib.bed_process_bed_file(random_negatives_bed, random_negatives_unique_ids_bed,
                                new2oldid_dic=new2oldid_dic,
                                id_prefix="rand_neg",
                                generate_unique_ids=True)

        # Get negative .bed rows.
        id2row_neg_dic = rplib.bed_read_rows_into_dic(random_negatives_unique_ids_bed)

        # Check if negative regions were indeed selected based on incl_bed excl_bed.
        check_shuffle=True
        if check_shuffle:
            print("Checking bedtools shuffle output ... ")
            print("Look out for negative regions not fully overlapping with -incl or reference ...")
            # Get IDs for these regions.
            trouble_ids_dic = {}
            check_incl_bed = regions_with_positives_bed
            if round_two:
                if transcript_regions:
                    check_incl_bed = most_prominent_transcripts_bed
                else:
                    check_incl_bed = most_prominent_genes_bed
            warning = rplib.check_random_negatives(random_negatives_unique_ids_bed, check_incl_bed,
                                                   mask_negatives_bed, chr_lengths_file,
                                                   trouble_ids_dic=trouble_ids_dic,
                                                   report=False)
            if warning:
                print("WARNING: bedtools shuffle output contains such regions. Removing ...")
                assert trouble_ids_dic, "no IDs in trouble_ids_dic (i.e. negative regions) to remove"
                c_del = 0
                for neg_id in trouble_ids_dic:
                    del id2row_neg_dic[neg_id]
                    c_del += 1
                print("# regions removed due to bedtools shuffle behaving badly:  %i" %(c_del))
                print("CALEB: you behave now ... ")
                # Overwrite random_negatives_unique_ids_bed.
                rplib.bed_write_row_dic_into_file(id2row_neg_dic, random_negatives_unique_ids_bed)

    """
    Extract positive and negative sequences
    =======================================

    - For transcript sites, extract from sequence dictionary.
    - For genomic sites, extract from --gen in_2bit.
    - For genomic sites, delete sites with lengths != extracted sequence lengths
    - Remove N containing sequences / sites in both sets
    - Remove low complexity negatives (unless negatives given by --neg-in)

    """

    # Get region lengths.
    pos_bed_len_dic = rplib.bed_get_region_lengths(positives_with_context_bed)
    if not args.in_neg_sites:
        negatives_with_context_bed = random_negatives_unique_ids_bed
    neg_bed_len_dic = rplib.bed_get_region_lengths(negatives_with_context_bed)

    if transcript_regions:
        # Get sequences for pos/neg transcript sites.
        print("Extract sequences for transcript sites ... ")
        pos_context_seqs_dic = rplib.extract_transcript_sequences(id2row_dic, tr_seqs_dic)
        if round_two:
            neg_context_seqs_dic = rplib.extract_transcript_sequences(id2row_neg_dic, neg_tr_seqs_dic)
        else:
            neg_context_seqs_dic = rplib.extract_transcript_sequences(id2row_neg_dic, tr_seqs_dic)
    else:
        # Get sequences for pos/neg genomic sites.
        # Read in positive region lengths.
        print("Extract sequences for genomic sites ... ")
        # Extract positive sequences from .2bit.
        rplib.bed_extract_sequences_from_2bit(positives_with_context_bed, pos_tmp_fasta, args.in_2bit,
                                              lc_repeats=args.rep_reg_annot)
        # Extract negative sequences from .2bit.
        rplib.bed_extract_sequences_from_2bit(negatives_with_context_bed, neg_tmp_fasta, args.in_2bit,
                                              lc_repeats=args.rep_reg_annot)
        # Read in sequences.
        pos_context_seqs_dic = rplib.read_fasta_into_dic(pos_tmp_fasta,
                                                         skip_data_id="positive",
                                                         report=2)
        neg_context_seqs_dic = rplib.read_fasta_into_dic(neg_tmp_fasta,
                                                         skip_data_id="negative",
                                                         report=2)

    # Compare region lengths with sequence lengths, delete mismatches.
    print("Compare extracted sequence lengths to site lengths ... ")

    # IDs to delete lists.
    pos_del_list = []
    neg_del_list = []
    for neg_id in neg_context_seqs_dic:
        nseq_l = len(neg_context_seqs_dic[neg_id])
        nreg_l = neg_bed_len_dic[neg_id]
        if nseq_l != nreg_l:
            neg_del_list.append(neg_id)

    for pos_id in pos_context_seqs_dic:
        pseq_l = len(pos_context_seqs_dic[pos_id])
        preg_l = pos_bed_len_dic[pos_id]
        if pseq_l != preg_l:
            pos_del_list.append(pos_id)
    # Delete mismatches.
    for reg_id in pos_del_list:
        del pos_context_seqs_dic[reg_id]
    for reg_id in neg_del_list:
        del neg_context_seqs_dic[reg_id]

    c_pos_del = len(pos_del_list)
    c_neg_del = len(neg_del_list)
    c_pos_rem = len(pos_context_seqs_dic)
    c_neg_rem = len(neg_context_seqs_dic)

    assert neg_context_seqs_dic, "no negatives remaining after sequence vs region length comparison"
    assert pos_context_seqs_dic, "no positives remaining after sequence vs region length comparison"
    if args.in_neg_sites:
        if c_neg_rem < c_pos_rem:
            print("WARNING: less negatives than positives for dataset construction (#neg: %i, #pos: %i)" %(c_neg_rem, c_pos_rem))
    else:
        assert c_neg_rem > c_pos_rem, "not enough random negatives for dataset construction (#neg: %i, #pos: %i)" %(c_neg_rem, c_pos_rem)
    if c_pos_del:
        print("# positives removed due to region vs sequence length difference:  %i" %(c_pos_del))
    if c_neg_del:
        print("# negatives removed due to region vs sequence length difference:  %i" %(c_neg_del))

    # Remove N containing sequences.
    print("Remove N containing sequences ... ")
    pos_del_list = []
    neg_del_list = []
    for neg_id in neg_context_seqs_dic:
        if re.search("N", neg_context_seqs_dic[neg_id], re.I):
            neg_del_list.append(neg_id)
    for pos_id in pos_context_seqs_dic:
        if re.search("N", pos_context_seqs_dic[pos_id], re.I):
            pos_del_list.append(pos_id)
    # Delete mismatches.
    for reg_id in pos_del_list:
        del pos_context_seqs_dic[reg_id]
    for reg_id in neg_del_list:
        del neg_context_seqs_dic[reg_id]
    # Check / summarize.
    c_pos_del = len(pos_del_list)
    c_neg_del = len(neg_del_list)
    if c_pos_del:
        print("# N containing positives removed:  %i" %(c_pos_del))
    if c_neg_del:
        print("# N containing negatives removed:  %i" %(c_neg_del))
    assert neg_context_seqs_dic, "no negatives remaining after removing N containing ones"
    assert pos_context_seqs_dic, "no positives remaining after removing N containing ones"

    # Remove low complexity negatives.
    if args.in_neg_sites:
        print("Do not remove low complexity negative sequences if --neg-in ... ")
    else:
        neg_del_list = []
        print("Remove low complexity negative sequences (--neg-comp-thr %f) ... " %(args.neg_comp_thr))
        for neg_id in neg_context_seqs_dic:
            check_seq = neg_context_seqs_dic[neg_id].upper()
            # Get nt frequencies.
            count_dic = rplib.seq_count_nt_freqs(check_seq, rna=True)
            # Calculate sequence entropy.
            seq_entr = rplib.calc_seq_entropy(len(check_seq), count_dic)
            if seq_entr < args.neg_comp_thr:
                #print("Removing %s: %s (%f)" %(neg_id, check_seq, seq_entr))
                neg_del_list.append(neg_id)
        # Remove.
        for neg_id in neg_del_list:
            del neg_context_seqs_dic[neg_id]
        c_neg_del = len(neg_del_list)
        if c_neg_del:
            print("# low complexity negatives removed:  %i" %(c_neg_del))
        assert neg_context_seqs_dic, "no negatives remaining after removing low complexity ones"

        # Remaining negatives should still be > remaining positives.
        c_pos_rem = len(pos_context_seqs_dic)
        c_neg_rem = len(neg_context_seqs_dic)
        assert c_neg_rem > c_pos_rem, "not enough random negatives for dataset construction (#neg: %i, #pos: %i). Try with higher --neg-factor" %(c_neg_rem, c_pos_rem)

    """
    Select negatives
    ================

    - If --keep-add-neg is set, take all extracted negatives
    - If not, select same number as positives (random selection)

    """

    # Select same number of negative sites.
    if not args.keep_add_neg and not args.in_neg_sites:
        print("Randomly select subset of negatives (# == # positives) from extracted negatives ... ")
        # Randomly select negative IDs and store in list.
        rand_ids_list = rplib.random_order_dic_keys_into_list(neg_context_seqs_dic)
        # Number of positives = number of negatives.
        c_pos = len(pos_context_seqs_dic)
        # Go over remaining negative IDs (all - c_pos) and delete them.
        for neg_id in rand_ids_list[c_pos:]:
            del neg_context_seqs_dic[neg_id]

    """
    Create lowercase sequence context
    =================================

    - Make upstream + downstream sequence context lowercase.
      Upstream lowercase, uppercase, and downstream lowercase region lengths
      for each site ID are stored in id2pl_dic.
      For negatives, use new2oldid_dic for new random negative ID mapping
      to old associated positive ID, from which to get uppercase+lowercase
      lengths.
    - Output sequences + sites to finish FASTA+BED extraction part.

    """

    # Dictionaries for storing original lowercase+uppercase information (if --rra).
    pos_context_seqs_rep_lc_dic = {}
    neg_context_seqs_rep_lc_dic = {}
    # Store uppercase start and ends for sequence IDs.
    id2ucr_dic = {}
    if args.rep_reg_annot:
        # Store lowercase info in dics, then make sequences uppercase again.
        for pos_id in pos_context_seqs_dic:
            seq = pos_context_seqs_dic[pos_id]
            pos_context_seqs_rep_lc_dic[pos_id] = seq
            pos_context_seqs_dic[pos_id] = seq.upper()
            l_seq = len(seq)
        for neg_id in neg_context_seqs_dic:
            seq = neg_context_seqs_dic[neg_id]
            neg_context_seqs_rep_lc_dic[neg_id] = seq
            neg_context_seqs_dic[neg_id] = seq.upper()
            l_seq = len(seq)

    # Length statistics for final positives + negatives.
    final_pos_len_list = []
    final_neg_len_list = []
    final_pos_len_list = rplib.get_seq_len_list_from_dic(pos_context_seqs_dic)
    final_neg_len_list = rplib.get_seq_len_list_from_dic(neg_context_seqs_dic)
    c_final_pos = len(final_pos_len_list)
    c_final_neg = len(final_neg_len_list)

    # Output full-length (+ context) sequences.
    print("Output selected positives + negatives to BED and FASTA ... ")
    rplib.fasta_output_dic(pos_context_seqs_dic, pos_fasta_out, split=True)
    rplib.fasta_output_dic(neg_context_seqs_dic, neg_fasta_out, split=True)
    # Output BED regions.
    rplib.bed_write_row_dic_into_file(id2row_dic, pos_bed_out,
                                       id2out_dic=pos_context_seqs_dic)
    rplib.bed_write_row_dic_into_file(id2row_neg_dic, neg_bed_out,
                                       id2out_dic=neg_context_seqs_dic)
    # Delete negative entries not selected for final set.
    del_ids = []
    for neg_id in id2row_neg_dic:
        if neg_id not in neg_context_seqs_dic:
            del_ids.append(neg_id)
    for neg_id in del_ids:
        del id2row_neg_dic[neg_id]

    # Report numbers.
    neg_pos_ratio = c_final_neg / c_final_pos
    print("Obtained negatives to positives ratio:  %.3f" %(neg_pos_ratio))
    print("# positive training sites output:       %i" %(c_final_pos))
    print("# negative training sites output:       %i" %(c_final_neg))

    """
    Calculate additional features
    =============================

    Currently these are:
    structure (for sequence, genomic regions, and transcript regions)
    transcript region annotation (for genomic and transcript regions)
    exon-intron annotation (for genomic regions)
    conservation scores (for genomic and transcript regions)
    repeat region information (for genomic and transcript regions)

    1) structure (--str):
    To calculate probabilities of structural elements.
    .str: structural elements probabilities

    2) conservation scores (--phastcons, --phylop):
    Position-wise phyloP and / or phastCons scores to add for each
    site position.
    .pc.con : phastCons position-wise conservation scores file
    .pp.con : phyloP position-wise conservation scores file

    3) transcript region annotation (--tra, --tra-codons, --tra-borders)
    Add transcript region label annotations from --gtf to genomic or
    transcript sites.
    --tra: add 5'UTR (F), CDS (C), None (N), and 3'UTR (T) annotation
    to transcript or genomic sites.
    --tra-codons: add start (S) and stop (E) codon annotation to transcript
    or genomic sites.
    --tra-borders: add transcript start (A), transcript end (Z) nt, and
    exon border (B) annotation to transcript sites.

    4) exon-intron annotation (--eia, --eia-ib, --eia-n):
    For genomic sites only.
    Position-wise exon + intron annotation, for each genomic site position
    .eia : stores E or I labels for each site position
    If --eia-n, additional N label for regions not overlapping with
    transcripts (by default these are labelled as I)
    If --eia-ib, intron border labels (F: 5' intron start, T: 3' intron end)

    5) repeat region annotation (--rra)
    Add position-wise R (repeat) or N (no repeat) annotations to label
    repeat and non-repeat region sequences.
    Repeat region information is stored in --gen .2bit, with lower-case
    nucleotides belonging to repeat regions annotated by RepeatMasker and
    Tandem Repeats Finder (with period of 12 or less).

    """

    # Additional feature stats dictionaries.
    pos_str_stats_dic = None
    neg_str_stats_dic = None
    pos_phastcons_stats_dic = None
    neg_phastcons_stats_dic = None
    pos_phylop_stats_dic = None
    neg_phylop_stats_dic = None
    pos_eia_stats_dic = None
    neg_eia_stats_dic = None
    pos_tra_stats_dic = None
    neg_tra_stats_dic = None
    pos_rra_stats_dic = None
    neg_rra_stats_dic = None
    add_feat_dic_list = None
    if args.report and args.add_str:
        pos_str_stats_dic = {}
        neg_str_stats_dic = {}
    if args.report and args.pc_bw:
        pos_phastcons_stats_dic = {}
        neg_phastcons_stats_dic = {}
    if args.report and args.pp_bw:
        pos_phylop_stats_dic = {}
        neg_phylop_stats_dic = {}
    if args.report and args.exon_intron_annot:
        pos_eia_stats_dic = {}
        neg_eia_stats_dic = {}
    if args.report and args.tr_reg_annot:
        pos_tra_stats_dic = {}
        neg_tra_stats_dic = {}
    if args.report and args.rep_reg_annot:
        pos_rra_stats_dic = {}
        neg_rra_stats_dic = {}

    # Store feature infos.
    FEATOUT.write("fa\tC\tA,C,G,U\t-\n")
    if args.add_str:
        FEATOUT.write("str\tN\tE,H,I,M,S\tprob\n")
    if args.pc_bw:
        FEATOUT.write("pc.con\tN\tphastcons_score\tprob\n")
    if args.pp_bw:
        FEATOUT.write("pp.con\tN\tphylop_score\tminmax2\n")
    if args.tr_reg_annot:
        ll = ["F", "C", "N", "T"] # label list.
        if args.tr_reg_border_annot:
            ll.append("A")
            ll.append("Z")
            ll.append("B")
        if args.tr_reg_codon_annot:
            ll.append("S")
            ll.append("E")
        ll.sort()
        lls = ",".join(ll)
        FEATOUT.write("tra\tC\t%s\t-\n" %(lls))
    if args.exon_intron_annot:
        ll = ["E", "I"]
        if args.intron_border_annot:
            ll.append("T")
            ll.append("F")
        if args.exon_intron_n:
            ll.append("N")
        ll.sort()
        lls = ",".join(ll)
        FEATOUT.write("eia\tC\t%s\t-\n" %(lls))
    if args.rep_reg_annot:
        FEATOUT.write("rra\tC\tN,R\t-\n")

    """
    1) Secondary structure features.

    """
    # Secondary structure stats dictionaries.

    # HALLO123 ...

    if args.add_str:
        # Calculate structure features for positives.
        print("Get secondary structure information for positives ... ")
        rplib.calc_ext_str_features(id2row_dic, chr_len_dic,
                                    pos_str_out, args,
                                    check_seqs_dic=pos_context_seqs_dic,
                                    stats_dic=pos_str_stats_dic,
                                    tr_regions=transcript_regions,
                                    tr_seqs_dic=tr_seqs_dic)
        # Calculate structure features for negatives.
        if transcript_regions and round_two:
            tr_seqs_dic = neg_tr_seqs_dic
        print("Get secondary structure information for negatives ... ")
        rplib.calc_ext_str_features(id2row_neg_dic, chr_len_dic,
                                    neg_str_out, args,
                                    check_seqs_dic=neg_context_seqs_dic,
                                    stats_dic=neg_str_stats_dic,
                                    tr_regions=transcript_regions,
                                    tr_seqs_dic=tr_seqs_dic)

    """
    2) Conservation scores.

    """
    # More preprocessing if transcript_regions.
    if transcript_regions:
        if args.pc_bw or args.pp_bw or args.tr_reg_annot:
            print("Additional annotations for transcript sites require mapping to genome ... ")
            print("Mapping transcript sites to genome ... ")
            posid2hitc_dic = {}
            negid2hitc_dic = {}
            rplib.bed_convert_transcript_to_genomic_sites(pos_bed_out, args.in_gtf,
                                                          pos_transcript_to_genome_bed,
                                                          site2hitc_dic=posid2hitc_dic)
            rplib.bed_convert_transcript_to_genomic_sites(neg_bed_out, args.in_gtf,
                                                          neg_transcript_to_genome_bed,
                                                          site2hitc_dic=negid2hitc_dic)

    # If phastCons conservation scores (--pc-bw) are given.
    if args.pc_bw:
        if transcript_regions:
            print("Extracting phastCons conservation scores for transcript sites ... ")
            # For transcript sites.
            rplib.extract_conservation_scores(pos_transcript_to_genome_bed,
                                              pos_pc_con_out, args.pc_bw,
                                              stats_dic=pos_phastcons_stats_dic,
                                              merge_split_regions=True,
                                              report=False)
            rplib.extract_conservation_scores(neg_transcript_to_genome_bed,
                                              neg_pc_con_out, args.pc_bw,
                                              stats_dic=neg_phastcons_stats_dic,
                                              merge_split_regions=True,
                                              report=False)
        else:
            print("Extracting phastCons conservation scores for genomic sites ... ")
            # For genomic sites.
            rplib.extract_conservation_scores(pos_bed_out,
                                              pos_pc_con_out, args.pc_bw,
                                              stats_dic=pos_phastcons_stats_dic,
                                              merge_split_regions=False,
                                              report=False)
            rplib.extract_conservation_scores(neg_bed_out,
                                              neg_pc_con_out, args.pc_bw,
                                              stats_dic=neg_phastcons_stats_dic,
                                              merge_split_regions=False,
                                              report=False)

    # If phastCons conservation scores (--pp-bw) are given.
    if args.pp_bw:
        if transcript_regions:
            print("Extracting phyloP conservation scores for transcript sites ... ")
            # For transcript sites.
            rplib.extract_conservation_scores(pos_transcript_to_genome_bed,
                                              pos_pp_con_out, args.pp_bw,
                                              stats_dic=pos_phylop_stats_dic,
                                              merge_split_regions=True,
                                              report=False)
            rplib.extract_conservation_scores(neg_transcript_to_genome_bed,
                                              neg_pp_con_out, args.pp_bw,
                                              stats_dic=neg_phylop_stats_dic,
                                              merge_split_regions=True,
                                              report=False)
        else:
            print("Extracting phyloP conservation scores for genomic sites ... ")
            # For genomic sites.
            rplib.extract_conservation_scores(pos_bed_out,
                                              pos_pp_con_out, args.pp_bw,
                                              stats_dic=pos_phylop_stats_dic,
                                              merge_split_regions=False,
                                              report=False)
            rplib.extract_conservation_scores(neg_bed_out,
                                              neg_pp_con_out, args.pp_bw,
                                              stats_dic=neg_phylop_stats_dic,
                                              merge_split_regions=False,
                                              report=False)
        print("Normalizing phyloP scores ... ")
        rplib.phylop_norm_train_scores(pos_pp_con_out, neg_pp_con_out)

    """
    3) Transcript region annotations for transcript sites.

    """
    if args.tr_reg_annot and transcript_regions:
        print("Extracting transcript region annotations for transcript sites ... ")
        if args.tr_reg_codon_annot:
            print("Start + stop codon annotations enabled ... ")
        if args.tr_reg_border_annot:
            print("Transcript + exon border annotations enabled ... ")
        tr_ids_dic = chr_len_dic
        if round_two:
            tr_ids_dic = mp_ids_dic
        rplib.bed_get_transcript_annotations_from_gtf(tr_ids_dic,
                                            pos_transcript_to_genome_bed,
                                            args.in_gtf, pos_tra_out,
                                            stats_dic=pos_tra_stats_dic,
                                            codon_annot=args.tr_reg_codon_annot,
                                            border_annot=args.tr_reg_border_annot,
                                            merge_split_regions=True)
        rplib.bed_get_transcript_annotations_from_gtf(tr_ids_dic,
                                            neg_transcript_to_genome_bed,
                                            args.in_gtf, neg_tra_out,
                                            stats_dic=neg_tra_stats_dic,
                                            codon_annot=args.tr_reg_codon_annot,
                                            border_annot=args.tr_reg_border_annot,
                                            merge_split_regions=True)

    """
    4) Exon-intron + transcript region annotations for genomic sites.

    """
    if (args.exon_intron_annot or args.tr_reg_annot) and not transcript_regions:
        print("Get transcript annotations for genomic sites ... ")
        # Get transcript IDs to use for exon-intron or transcript region annotation.
        tr_ids_dic = {}
        # Extract tr_ids_dic.
        if args.tr_reg_annot or not args.eia_all_ex:
            if args.tr_list:
                print("Using transcript list from --tr-list to define transcript regions ... ")
                # Read in transcript IDs from --tr-list.
                tr_ids_dic = rplib.read_ids_into_dic(args.tr_list)
                # Read in all transcript IDs in --gtf.
                all_tr_ids_dic = rplib.gtf_get_transcript_ids(args.in_gtf)
                # Check if given --tr list IDs are in --gtf.
                del_tr_l = []
                for tr_id in tr_ids_dic:
                    if tr_id not in all_tr_ids_dic:
                        print("WARNING: transcript ID \"%s\" from --tr-list not found in --gtf. Ignoring transcript ... " %(tr_id))
                        del_tr_l.append(tr_id)
                for tr_id in del_tr_l:
                    del tr_ids_dic[tr_id]
                assert tr_ids_dic, "no transcript IDs from --tr-list remain after removing IDs not found in --gtf"
            else:
                # Create exon BED file from most prominent transcripts.
                strict=False
                if strict:
                    print("Get most prominent transcript for each gene from --gtf (TSL1-5 only)... ")
                else:
                    print("Get most prominent transcript for each gene from --gtf ... ")
                # Don't be strict (!= only TSL1-5 allowed).
                tr_ids_dic = rplib.gtf_extract_most_prominent_transcripts(args.in_gtf,
                                                "dummy",
                                                strict=strict,
                                                return_ids_dic=tr_ids_dic)

        # Exon-intron annotations for genomic sites.
        if args.exon_intron_annot:
            # All exons BED (--eia-all-ex case).
            all_exon_bed = False
            if args.eia_all_ex:
                random_id = uuid.uuid1()
                all_exon_bed = str(random_id) + ".tmp.bed"
                print("Extract all exon regions from --gtf (--eia-all-ex enabled) ... ")
                rplib.gtf_extract_unique_exon_bed(args.in_gtf, all_exon_bed,
                                                  use_ei_labels=True)

            print("Extract exon-intron annotations for genomic sites ... ")
            if not all_exon_bed:
                if args.tr_list:
                    print("Use exon regions of --tr-list transcripts from --gtf ... ")
                else:
                    print("Use exon regions of most prominent transcripts from --gtf ... ")
                if args.intron_border_annot:
                    print("Intron border annotation enabled ... ")
                if args.exon_intron_n:
                    print("Non-intron non-exon region annotation enabled ... ")
            # Positives.
            rplib.bed_get_exon_intron_annotations_from_gtf(tr_ids_dic, pos_bed_out,
                                        args.in_gtf, pos_eia_out,
                                        stats_dic=pos_eia_stats_dic,
                                        own_exon_bed=all_exon_bed,
                                        n_labels=args.exon_intron_n,
                                        intron_border_labels=args.intron_border_annot)
            # Negatives.
            rplib.bed_get_exon_intron_annotations_from_gtf(tr_ids_dic, neg_bed_out,
                                        args.in_gtf, neg_eia_out,
                                        stats_dic=neg_eia_stats_dic,
                                        own_exon_bed=all_exon_bed,
                                        n_labels=args.exon_intron_n,
                                        intron_border_labels=args.intron_border_annot)
            if all_exon_bed:
                if os.path.exists(all_exon_bed):
                    os.remove(all_exon_bed)

        # Transcript region annotations for genomic sites.
        if args.tr_reg_annot:
            print("Extracting transcript region annotations for genomic sites ... ")
            if args.tr_reg_codon_annot:
                print("Start + stop codon annotations enabled ... ")
            # Positives.
            rplib.bed_get_transcript_annotations_from_gtf(tr_ids_dic,
                                                pos_bed_out,
                                                args.in_gtf, pos_tra_out,
                                                stats_dic=pos_tra_stats_dic,
                                                codon_annot=args.tr_reg_codon_annot,
                                                border_annot=args.tr_reg_border_annot,
                                                merge_split_regions=False)
            # Negatives.
            rplib.bed_get_transcript_annotations_from_gtf(tr_ids_dic,
                                                neg_bed_out,
                                                args.in_gtf, neg_tra_out,
                                                stats_dic=neg_tra_stats_dic,
                                                codon_annot=args.tr_reg_codon_annot,
                                                border_annot=args.tr_reg_border_annot,
                                                merge_split_regions=False)

    """
    5) Repeat region annotations for genomic and transcript sites.

    """
    if args.rep_reg_annot:
        print("Extracting repeat region annotations for positives ... ")
        rplib.fasta_get_repeat_region_annotations(pos_context_seqs_rep_lc_dic, pos_rra_out,
                                                  stats_dic=pos_rra_stats_dic)
        print("Extracting repeat region annotations for negatives ... ")
        rplib.fasta_get_repeat_region_annotations(neg_context_seqs_rep_lc_dic, neg_rra_out,
                                                  stats_dic=neg_rra_stats_dic)


    """
    6) Additional BED features.

    Feature ID (column 1):
    Up to 20 characters, ID for features, used as file ending
    Feature type (column 2):
    C (categorical), N (numerical)
    Feature format (column 3):
        0 (treat feature as categorical == no normalization, do not change values)
        1 (col5 value == score), meaning apply min-max normalization
        2 (col5 value == p-value)
    Feature BED file path (column 4):
    Path to BED file to annotate feature regions.

    """
    feat_id_list = []

    if args.feat_in:

        print("Extracting additional BED features from --feat-in ... ")

        # Store dataset infos.
        fids_dic = {}
        fid2type_dic = {} # C,N
        fid2format_dic = {} # 0,1,2
        fid2path_dic = {}
        # Read in infos.
        with open(args.feat_in) as f:
            for line in f:
                row = line.strip()
                cols = line.strip().split("\t")
                feat_id = cols[0]
                feat_type = cols[1]
                feat_format = int(cols[2])
                bed_path = cols[3]
                # Convert to valid file ending.
                new_feat_id = rplib.get_valid_file_ending(feat_id)
                if feat_id != new_feat_id:
                    print("Converted feature ID \"%s\" to \"%s\" to have valid file ending" %(feat_id, new_feat_id))
                assert new_feat_id not in fids_dic, "non-unique feature ID \"%s\" in --feat-in table file" %(new_feat_id)
                fids_dic[new_feat_id] = 1
                fid2type_dic[new_feat_id] = feat_type
                fid2format_dic[new_feat_id] = feat_format
                fid2path_dic[new_feat_id] = bed_path
        f.closed
        assert fids_dic, "no features read in from --feat-in table"

        if args.report:
            add_feat_dic_list = []

        for feat_id in fids_dic:
            bed_path = fid2path_dic[feat_id]
            feat_type = fid2type_dic[feat_id]
            feat_format = fid2format_dic[feat_id]
            p_values = False
            if feat_format == 2:
                p_values = True
            # If all BED features should be treated as one-hot.
            if args.feat_in_1h:
                feat_type = "C"
            # Output feature info.
            if feat_type == "C":
                FEATOUT.write("%s\tC\t0,1\tone_hot\n" %(feat_id))
            else:
                if feat_format:
                    if p_values:
                        FEATOUT.write("%s\tN\tbed_score\tprob\n" %(feat_id))
                    else:
                        FEATOUT.write("%s\tN\tbed_score\tminmax_norm\n" %(feat_id))
                else:
                    FEATOUT.write("%s\tN\tbed_score\t-\n" %(feat_id))

            # Stats dictionaries.
            pos_stats_dic = None
            neg_stats_dic = None
            if args.report:
                pos_stats_dic = {}
                neg_stats_dic = {}

            """
            Check if --feat-in regions are purely genomic or transcript,
            and whether mapping to transcriptome is necessary.
            """

            bed_ids_dic = rplib.bed_get_chromosome_ids(bed_path)
            if transcript_regions:
                print("Check whether \"%s\" feature BED contains exclusively genomic or transcript regions ..." %(feat_id))
                # Check if IDs are transcript IDs.
                need_to_map = False
                if not rplib.check_dic1_keys_in_dic2(bed_ids_dic, tr_len_dic):
                    print("Regions not exclusively transcript in \"%s\" feature BED, check for genomic regions ... " %(feat_id))
                    # Check if IDs are genomic IDs.
                    if rplib.check_dic1_keys_in_dic2(bed_ids_dic, gen_chr_len_dic):
                        need_to_map = True
                    else:
                        assert False, "Regions not exclusively genomic in in \"%s\" feature BED. Supply either solely transcript or solely genomic regions (no mixed regions allowed so far)" %(feat_id)
                if need_to_map:
                    # Temp mapping folder.
                    map_out_folder = args.out_folder + "/" + "tmp_map_out"
                    if os.path.exists(map_out_folder):
                        shutil.rmtree(map_out_folder)
                    os.makedirs(map_out_folder)

                    print("\"%s\" feature BED contains genomic regions ... " %(feat_id))
                    #print("Get all transript IDs from --gtf ...")
                    #all_tr_ids_dic =  dsglib.gtf_get_transcript_ids(args.in_gtf)
                    print("Map \"%s\" feature BED regions to transcriptome, using all --gtf transcripts ... " %(feat_id))

                    # Map feature BED genomic regions to transcriptome.
                    rplib.convert_genome_positions_to_transcriptome(bed_path,
                                                                    map_out_folder,
                                                                    args.in_gtf,
                                                                    tr_len_dic)

                    all_comp_transcript_hits_bed = map_out_folder + "/" + "transcript_hits_complete.bed"
                    all_incomp_transcript_hits_bed = map_out_folder + "/" + "transcript_hits_incomplete.bed"
                    assert os.path.exists(all_comp_transcript_hits_bed), "Mapping file %s not found" %(all_comp_transcript_hits_bed)
                    assert os.path.exists(all_incomp_transcript_hits_bed), "Mapping file %s not found" %(all_incomp_transcript_hits_bed)
                    merge_list = []
                    merge_list.append(all_comp_transcript_hits_bed)
                    merge_list.append(all_incomp_transcript_hits_bed)
                    rplib.merge_files(merge_list, bed_path)

            else:
                # Check if IDs are genomic IDs.
                assert rplib.check_dic1_keys_in_dic2(bed_ids_dic, chr_len_dic), "Regions not exclusively genomic in in \"%s\" feature BED. Please provide genomic regions matching --gtf and --gen files" %(feat_id)


            print("Annotating \"%s\" feature ... " %(feat_id))

            # Training set.
            pos_feat_out = args.out_folder + "/" + "positives." + feat_id
            rplib.bed_get_feature_annotations(pos_bed_out, bed_path, pos_feat_out,
                                              feat_type=feat_type,
                                              stats_dic=pos_stats_dic,
                                              disable_pol=False)
            # Add additional stats to pos_stats_dic.
            if args.report:
                pos_stats_dic["set"] = "positives"
                pos_stats_dic["feat_id"] = feat_id
                add_feat_dic_list.append(pos_stats_dic)
            neg_feat_out = args.out_folder + "/" + "negatives." + feat_id
            rplib.bed_get_feature_annotations(neg_bed_out, bed_path, neg_feat_out,
                                              feat_type=feat_type,
                                              stats_dic=neg_stats_dic,
                                              disable_pol=False)
            # Add additional stats to neg_stats_dic.
            if args.report:
                neg_stats_dic["set"] = "negatives"
                neg_stats_dic["feat_id"] = feat_id
                add_feat_dic_list.append(neg_stats_dic)
            # Normalize feature values.
            if args.feat_in_norm and feat_format and feat_type == "N":
                print("Min-max normalizing \"%s\" feature values ... " %(feat_id))
                rplib.feat_min_max_norm_train_scores(pos_feat_out, neg_feat_out,
                                                     p_values=p_values,
                                                     int_whole_nr=True)
            feat_id_list.append(feat_id)

    FEATOUT.close()

    # Delete feature BED transcript mapping folder if present.
    map_out_folder = args.out_folder + "/" + "tmp_map_out"
    if os.path.exists(map_out_folder):
        shutil.rmtree(map_out_folder)

    """
    Generate HTML report including various training dataset statistics.

    """
    dataset_type =  "g"
    if transcript_regions:
        dataset_type =  "t"

    if args.report:
        # Gene biotype statistics for target genes.
        gbt_stats = True
        # All gene biotype counts.
        all_gbtc_dic = {}
        # Target gene biotype counts.
        target_gbtc_dic = {}
        # Target region to hit count (transcript or gene ID -> hit count).
        t2hc_dic = {}
        # Target region to info.
        t2i_dic = {}
        if gbt_stats:
            # Calculate additional stats for report.
            print("Extract additional statistics for report ... ")
            # Get count stats for gene biotypes.
            region_ids_dic = rplib.bed_get_region_ids(regions_with_positives_bed)
            # Get gene biotype counts for target genes.
            if transcript_regions:
                target_gbtc_dic = rplib.gtf_get_gene_biotypes_from_transcript_ids(
                                            region_ids_dic, args.in_gtf,
                                            all_gbtc_dic=all_gbtc_dic)

                # Get transcript ID to hit count dictionary.
                t2hc_dic = rplib.bed_get_chromosome_ids(pos_bed_out)
                # Get transcript infos.
                t2i_dic = rplib.gtf_get_transcript_infos(region_ids_dic, args.in_gtf)
            else:
                target_gbtc_dic = rplib.gtf_get_gene_biotypes(
                                            region_ids_dic, args.in_gtf,
                                            all_gbtc_dic=all_gbtc_dic)
                # Get overlapping site counts for each gene.
                t2hc_dic = rplib.bed_intersect_count_region_overlaps(gene_regions_gtf_tmp_bed, pos_bed_out)
                # Get gene infos.
                t2i_dic = rplib.gtf_get_gene_infos(region_ids_dic, args.in_gtf)

        # HTML report output file.
        html_report_out = args.out_folder + "/" + "report.rnaprot_gt.html"
        # Plots subfolder.
        plots_subfolder = "plots_rnaprot_gt"
        # Get library and logo path.
        rplib_path = os.path.dirname(rplib.__file__)
        # Generate report.
        rplib.rp_gt_generate_html_report(pos_context_seqs_dic, neg_context_seqs_dic,
                                         args.out_folder, dataset_type, rplib_path,
                                         html_report_out=html_report_out,
                                         plots_subfolder=plots_subfolder,
                                         pos_str_stats_dic=pos_str_stats_dic,
                                         neg_str_stats_dic=neg_str_stats_dic,
                                         pos_phastcons_stats_dic=pos_phastcons_stats_dic,
                                         neg_phastcons_stats_dic=neg_phastcons_stats_dic,
                                         pos_phylop_stats_dic=pos_phylop_stats_dic,
                                         neg_phylop_stats_dic=neg_phylop_stats_dic,
                                         pos_eia_stats_dic=pos_eia_stats_dic,
                                         neg_eia_stats_dic=neg_eia_stats_dic,
                                         pos_tra_stats_dic=pos_tra_stats_dic,
                                         neg_tra_stats_dic=neg_tra_stats_dic,
                                         pos_rra_stats_dic=pos_rra_stats_dic,
                                         neg_rra_stats_dic=neg_rra_stats_dic,
                                         add_feat_dic_list=add_feat_dic_list,
                                         target_gbtc_dic=target_gbtc_dic,
                                         all_gbtc_dic=all_gbtc_dic,
                                         t2hc_dic=t2hc_dic,
                                         t2i_dic=t2i_dic,
                                         kmer_top=10,
                                         target_top=20,
                                         theme=args.theme,
                                         rna=True)

    """
    Take out the trash.

    """
    clean_up = True
    if clean_up:
        print("Removing temporary files ... ")
        # Remove tmp files.
        if os.path.exists(tmp1_bed):
            os.remove(tmp1_bed)
        if os.path.exists(tmp2_bed):
            os.remove(tmp2_bed)
        if os.path.exists(tmp3_bed):
            os.remove(tmp3_bed)
        if os.path.exists(neg_tmp1_bed):
            os.remove(neg_tmp1_bed)
        if os.path.exists(neg_tmp2_bed):
            os.remove(neg_tmp2_bed)
        if os.path.exists(neg_tmp3_bed):
            os.remove(neg_tmp3_bed)
        if os.path.exists(gene_regions_gtf_tmp_bed):
            os.remove(gene_regions_gtf_tmp_bed)
        if os.path.exists(positives_with_context_bed):
            os.remove(positives_with_context_bed)
        if os.path.exists(negatives_with_context_bed):
            os.remove(negatives_with_context_bed)
        if os.path.exists(mask_negatives_bed):
            os.remove(mask_negatives_bed)
        if os.path.exists(shuffle_in_bed):
            os.remove(shuffle_in_bed)
        if os.path.exists(random_negatives_bed):
            os.remove(random_negatives_bed)
        if os.path.exists(random_negatives_unique_ids_bed):
            os.remove(random_negatives_unique_ids_bed)
        if os.path.exists(pos_tmp_fasta):
            os.remove(pos_tmp_fasta)
        if os.path.exists(neg_tmp_fasta):
            os.remove(neg_tmp_fasta)
        if os.path.exists(most_prominent_transcripts_bed):
            os.remove(most_prominent_transcripts_bed)
        if os.path.exists(most_prominent_genes_bed):
            os.remove(most_prominent_genes_bed)
        if os.path.exists(pos_transcript_to_genome_bed):
            os.remove(pos_transcript_to_genome_bed)
        if os.path.exists(neg_transcript_to_genome_bed):
            os.remove(neg_transcript_to_genome_bed)

    """
    Output file summary.

    """
    print("")
    print("TRAINING SET OUTPUT FILES")
    print("=========================")
    print("Positives sequences .fa:\n%s" %(pos_fasta_out))
    print("Negatives sequences .fa:\n%s" %(neg_fasta_out))
    print("Positives sites .bed:\n%s" %(pos_bed_out))
    print("Negatives sites .bed:\n%s" %(neg_bed_out))
    if args.add_str:
        print("Positives structural elements probabilities .str:\n%s" %(pos_str_out))
        print("Negatives structural elements probabilities .str:\n%s" %(neg_str_out))
    if args.pc_bw:
        print("Positives phastCons conservation scores .pc.con:\n%s" %(pos_pc_con_out))
        print("Negatives phastCons conservation scores .pc.con:\n%s" %(neg_pc_con_out))
    if args.pp_bw:
        print("Positives phyloP conservation scores .pp.con:\n%s" %(pos_pp_con_out))
        print("Negatives phyloP conservation scores .pp.con:\n%s" %(neg_pp_con_out))
    if args.exon_intron_annot:
        print("Positives exon-intron region annotations .eia:\n%s" %(pos_eia_out))
        print("Negatives exon-intron region annotations .eia:\n%s" %(neg_eia_out))
    if args.tr_reg_annot:
        print("Positives transcript region annotations .tra:\n%s" %(pos_tra_out))
        print("Negatives transcript region annotations .tra:\n%s" %(neg_tra_out))
    if args.rep_reg_annot:
        print("Positives repeat region annotations .rra:\n%s" %(pos_rra_out))
        print("Negatives repeat region annotations .rra:\n%s" %(neg_rra_out))
    if feat_id_list:
        for add_fid in feat_id_list:
            pos_feat_out = args.out_folder + "/" + "positives." + add_fid
            neg_feat_out = args.out_folder + "/" + "negatives." + add_fid
            print("Positives --feat-in annotations .%s:\n%s" %(add_fid, pos_feat_out))
            print("Positives --feat-in annotations .%s:\n%s" %(add_fid, neg_feat_out))
    if args.report:
        print("Training set generation report .html:\n%s" %(html_report_out))
    print("")


################################################################################

def main_gp(args):
    """
    Generate a prediction (or test) set.

    """

    print("Running for you in GP mode ... ")

    # Generate results output folder.
    if not os.path.exists(args.out_folder):
        os.makedirs(args.out_folder)

    """
    Output files.

    """
    test_bed_out = args.out_folder + "/" + "test.bed"
    test_fasta_out = args.out_folder + "/" + "test.fa"
    # Additional feature files.
    test_pc_con_out = args.out_folder + "/" + "test.pc.con"
    test_pp_con_out = args.out_folder + "/" + "test.pp.con"
    test_tra_out = args.out_folder + "/" + "test.tra"
    test_rra_out = args.out_folder + "/" + "test.rra"
    test_eia_out = args.out_folder + "/" + "test.eia"
    test_str_out = args.out_folder + "/" + "test.str"
    # Mode settings file.
    settings_file = args.out_folder + "/" + "/settings.rnaprot_gp.out"
    # Chromosome or transcript lengths file.
    chr_lengths_file = args.out_folder + "/" + "reference_lengths.out"
    # Genes or transcript regions containing --in sites.
    regions_with_sites_bed = args.out_folder + "/" + "regions_with_sites.bed"
    # Feature table.
    feat_table_out = args.out_folder + "/" + "features.out"
    # Transcript sites mapped to genome.
    test_transcript_to_genome_bed = args.out_folder + "/" + "test_transcript_to_genome.bed"

    # Checks.
    assert os.path.exists(args.in_train_folder), "--train in folder not found"
    check_model_file = args.in_train_folder + "/final.model"
    check_params_file = args.in_train_folder + "/final.params"
    assert os.path.exists(check_model_file), "--train-in contains no .model file (final.model missing). Please train model without --gen-cv option"
    assert os.path.exists(check_params_file), "--train-in contains no .params file (final.params missing). Please train model without --gen-cv option"

    # Get infos from rnaprot train settings (partly ported over from gt).
    rp_train_settings_file = args.in_train_folder + "/settings.rnaprot_train.out"
    assert os.path.exists(rp_train_settings_file), "rnaprot train settings file %s not found" %(rp_train_settings_file)
    train_settings_dic = rplib.read_settings_into_dic(rp_train_settings_file)

    # Get used model features.
    rp_train_features_file = args.in_train_folder + "/features.out"
    train_fid_dic = rplib.read_settings_into_dic(rp_train_features_file,
                                                 val_col2=True)
    assert "fa" in train_fid_dic, "sequence feature \"fa\" missing in %s" %(rp_train_features_file)

    # Additional feature file.
    feat_in_file = False
    if args.feat_in:
        assert os.path.exists(args.feat_in), "provided --feat-in file not found"
        if train_settings_dic["feat_in"] != "None":
            if os.path.exists(train_settings_dic["feat_in"]):
                print("WARNING: previous --feat-in file path found in --train-in, but will be overwritten by set --feat-in")
        feat_in_file = args.feat_in
    else:
        if train_settings_dic["feat_in"] != "None":
            if os.path.exists(train_settings_dic["feat_in"]):
                print("Using --feat-in file specified in --train-in ... ")
                feat_in_file = train_settings_dic["feat_in"]
    # Get additional feature IDs.
    add_fid_dic = {}
    args.feat_in_norm = False
    args.feat_in_1h = False
    if feat_in_file:
        with open(feat_in_file) as f:
            for line in f:
                row = line.strip()
                cols = line.strip().split("\t")
                feat_id = cols[0]
                add_fid_dic[feat_id] = 1
                assert feat_id in train_fid_dic, "--feat-in \"%s\" ID not a feature of --train-in model" %(feat_id)
        f.closed
        if train_settings_dic["feat_in_norm"] == "True":
            args.feat_in_norm = True
        if train_settings_dic["feat_in_1h"] == "True":
            args.feat_in_1h = True
    assert add_fid_dic, "no additional features read in from specified --feat-in %s" %(feat_in_file)

    # Region features.
    args.exon_intron_annot = False
    args.intron_border_annot = False
    args.exon_intron_n = False
    args.tr_reg_annot = False
    args.tr_reg_codon_annot = False
    args.tr_reg_border_annot = False
    args.rep_reg_annot = False
    # Structure features.
    args.plfold_u = None
    args.plfold_l = None
    args.plfold_w = None
    args.add_str = False

    in_file_fasta = False
    if rplib.fasta_check_fasta_file(args.in_sites):
        in_file_fasta = True

    onlyseq = False
    if len(train_fid_dic) == 1:
        onlyseq = True
    if not in_file_fasta:
        # Check for GTF and genome file.
        if "in_gtf" in train_settings_dic and not args.in_gtf:
            assert os.path.exists(train_settings_dic["in_gtf"]), "please provide a GTF file via --gtf (ideally the same that was used to generate training data for training model from --train-in)"
            args.in_gtf = train_settings_dic["in_gtf"]
            print("Using --gtf file specified in --train-in ... ")
        if "in_2bit" in train_settings_dic and not args.in_2bit:
            assert os.path.exists(train_settings_dic["in_2bit"]), "please provide a genomic 2.bit file via --gen (ideally the same that was used to generate training data for training model from --train-in)"
            args.in_2bit = train_settings_dic["in_2bit"]
            print("Using --gen file specified in --train-in ... ")

    """
    fa	C	A,C,G,U	-
    bpp.str	N	bp_prob	prob
    elem_p.str	N	p_e,p_h,p_i,p_m,p_s	prob
    pc.con	N	phastcons_score	prob
    pp.con	N	phylop_score	minmax2
    tra	C	C,F,N,T	-
    eia	C	E,I	-
    rra	C	N,R	-
    """

    # Check features used to train model.
    add_feat_found = False
    std_feat_found = False
    for fid in train_fid_dic:
        if fid == "fa":
            continue
        if fid == "pc.con":
            std_feat_found = True
            if not args.pc_bw:
                assert os.path.exists(train_settings_dic["pc_bw"]), "please provide a --phastcons file (since model was trained with phastCons conservation feature)"
                args.pc_bw = train_settings_dic["pc_bw"]
            else:
                assert os.path.exists(args.pc_bw), "provided --phastcons file %s not found" %(args.pc_bw)
        elif fid == "pp.con":
            std_feat_found = True
            if not args.pp_bw:
                assert os.path.exists(train_settings_dic["pp_bw"]), "please provide a --phylop file (since model was trained with phyloP conservation feature)"
                args.pp_bw = train_settings_dic["pp_bw"]
            else:
                assert os.path.exists(args.pp_bw), "provided --phylop file %s not found" %(args.pp_bw)
        elif fid == "eia":
            std_feat_found = True
            if train_settings_dic["tr_list"] != "None":
                if os.path.exists(train_settings_dic["tr_list"]) and args.tr_list:
                    print("WARNING: previous --tr-list file path found in --train-in, but will be overwritten by set --tr-list")
                if os.path.exists(train_settings_dic["tr_list"]) and not args.tr_list:
                    print("Using --tr-list file specified in --train-in ... ")
                    args.tr_list = train_settings_dic["tr_list"]
                if not os.path.exists(train_settings_dic["tr_list"]) and not args.tr_list:
                    print("WARNING: --train-in model was trained using specified transcript information via a --tr-list file but no file is provided")
                    args.tr_list = False
            args.exon_intron_annot = True
            if train_settings_dic["intron_border_annot"]  == "True":
                args.intron_border_annot = True
            if train_settings_dic["exon_intron_n"]  == "True":
                args.exon_intron_n = True
            if train_settings_dic["eia_all_ex"]  == "True":
                if not args.eia_all_ex:
                    print("WARNING: --train-in model was trained with --eia-all-ex which is currently disabled!")
                    print("         Please set --eia-all-ex if desired for exon-intron annotation ... ")
            if args.eia_all_ex:
                assert not args.intron_border_annot, "--eia-all-ex incompatible with --eia-ib setting from --train-in model. Please unset --eia-all-ex or train model without --eia-ib or use --eia-all-ex setting in rnaprot gt"
                assert not args.exon_intron_n, "--eia-all-ex incompatible with --eia-n setting from --train-in model. Please unset --eia-all-ex or train model without --eia-n or use --eia-all-ex setting in rnaprot gt"

        elif fid == "tra":
            std_feat_found = True
            if train_settings_dic["tr_list"] != "None":
                if os.path.exists(train_settings_dic["tr_list"]) and args.tr_list:
                    print("WARNING: previous --tr-list path found in --train-in, but will be overwritten by set --tr-list")
                if os.path.exists(train_settings_dic["tr_list"]) and not args.tr_list:
                    print("Using --tr-list file specified in --train-in ... ")
                    args.tr_list = train_settings_dic["tr_list"]
                if not os.path.exists(train_settings_dic["tr_list"]) and not args.tr_list:
                    print("WARNING: --train-in model was trained using specified transcript information via a --tr-list file but no file is provided")
                    args.tr_list = False
            args.tr_reg_annot = True
            if train_settings_dic["tr_reg_codon_annot"]  == "True":
                args.tr_reg_codon_annot = True
            if train_settings_dic["tr_reg_border_annot"]  == "True":
                args.tr_reg_border_annot = True
        elif fid == "rra":
            std_feat_found = True
            args.rep_reg_annot = True
        elif fid == "str":
            args.add_str = True
            args.plfold_u = int(train_settings_dic["plfold_u"])
            args.plfold_l = int(train_settings_dic["plfold_l"])
            args.plfold_w = int(train_settings_dic["plfold_w"])
        else:
            add_feat_found = True
            assert feat_in_file, "--train-in model was trained with additional features, but no additional feature file set (please provide same --feat-in file)"
            assert fid in add_fid_dic, "feature ID %s not found in additional feature file %s. Please provide correct --feat-in file" %(fid, feat_in_file)


    """
    Temporary output files.

    """
    # tmp files for processing --in sites.
    tmp_bed = args.out_folder + "/" + "test.tmp.bed"
    # Gene regions from --gtf.
    gene_regions_gtf_tmp_bed = args.out_folder + "/" + "gene_regions.gtf.tmp.bed"
    # Processed --in sites +col5 score = 0.
    processed_in_sites_bed = args.out_folder + "/" + "processed_in_sites_sc0.bed"
    # Extracted site sequences (twoBitToFa output).
    tmp_fasta = args.out_folder + "/" + "test.tmp.fa"

    # Write generated feature infos to table.
    FEATOUT = open(feat_table_out, "w")

    """
    CASE 1: sequences provided as input (--in FASTA)
    ================================================

    --in FASTA given.

    If --in FASTA is set, possible features beside sequence:
    --str and optionally --bp-in (custom base pairs)

    """

    # Check if --in file is FASTA file.
    if in_file_fasta:
        if std_feat_found or add_feat_found:
            assert False, "FASTA sequences supplied via --in, but --train-in model was trained on additional features. Please provide BED regions ... "
        # Delete old BED files.
        if os.path.exists(test_bed_out):
            os.remove(test_bed_out)
        # Read in sequences (this also checks for unique headers + converts to RNA).
        print("Processing --in FASTA sequences ... ")
        print("Read in --in FASTA sequences and convert to RNA ... ")
        test_seqs_dic = rplib.read_fasta_into_dic(args.in_sites)
        # Output sequences.
        rplib.fasta_output_dic(test_seqs_dic, test_fasta_out, split=True)

        # Sequence character counts.
        seq_cc_dic = rplib.seqs_dic_count_chars(test_seqs_dic)
        allowed_nt_dic = {'A': 1, 'C': 1, 'G': 1, 'U': 1}
        for nt in seq_cc_dic:
            if nt not in allowed_nt_dic:
                assert False, "--in sequences contain \"%s\" character (allowed characters: A C G U)" %(nt)
        FEATOUT.write("fa\tC\tA,C,G,U\t-\n")

        # If structure features should be computed.
        if args.add_str:
            if args.report:
                test_str_stats_dic = {}
            FEATOUT.write("str\tN\tE,H,I,M,S\tprob\n")
            # Calculate structure features for positives.
            print("Get secondary structure features ... ")
            rplib.calc_str_elem_p(test_fasta_out, test_str_out,
                                  stats_dic=test_str_stats_dic,
                                  plfold_u=args.plfold_u,
                                  plfold_l=args.plfold_l,
                                  plfold_w=args.plfold_w)

        # Sequence set stats.
        c_test_out = len(test_seqs_dic)
        if args.report:
            # HTML report output file.
            html_report_out = args.out_folder + "/" + "report.rnaprot_gp.html"
            # Plots subfolder.
            plots_subfolder = "plots_rnaprot_gp"
            # Get library and logo path.
            rplib_path = os.path.dirname(rplib.__file__)
            # Generate report.
            rplib.rp_gp_generate_html_report(test_seqs_dic, args.out_folder,
                                             "s", rplib_path,
                                             plots_subfolder=plots_subfolder,
                                             html_report_out=html_report_out,
                                             test_str_stats_dic=test_str_stats_dic,
                                             theme=args.theme,
                                             kmer_top=10,
                                             rna=True)

        # Mode settings output file.
        SETOUT = open(settings_file, "w")
        for arg in vars(args):
            SETOUT.write("%s\t%s\n" %(arg, str(getattr(args, arg))))
        SETOUT.close()

        print("# prediction set sequences output: %i" %(c_test_out))
        print("")
        print("PREDICTION SET OUTPUT FILES")
        print("===========================")
        print("Sequences .fa:\n%s" %(test_fasta_out))
        if args.add_str:
            print("Structural elements probabilities .elem_p.str:\n%s" %(test_str_out))
        if args.report:
            print("Prediction set generation report .html:\n%s" %(html_report_out))
        print("")
        FEATOUT.close()
        sys.exit()

    """
    CASE 2: genomic or transcript sites given (--in BED)
    ====================================================

    This means:
        - Additional features can be added

    For genomic sites, the features are:
        - Position-wise exon-intron annotation (with options)
        - Position-wise transcript region annotation (with options)
        - Position-wise repeat region annotation
        - Position-wise conservation scores (phasCons and phyloP)
        - Structure features (base pairs and structural elements probabilities)

    For transcript sites, the features are:
        - Position-wise transcript region annotation (with options)
        - Position-wise repeat region annotation
        - Position-wise conservation scores (phasCons and phyloP)
        - Structure features (base pairs and structural elements probabilities)

    NOTE that exon-intron features make no sense for transcript sites, as each
    position would get the exon label (thus omitted for transcript sites)

    """
    # If --in are BED, require --gtf and --gen.
    if not args.in_gtf:
        print("ERROR: --gtf GTF file is required if --in receives BED file")
        sys.exit()
    if not args.in_2bit:
        print("ERROR: --gen .2bit file is required if --in receives BED file")
        sys.exit()
    # Input checks.
    assert os.path.exists(args.in_sites), "--in BED file \"%s\" not found" %(args.in_sites)
    assert os.path.exists(args.in_gtf), "--gtf GTF file \"%s\" not found" %(args.in_gtf)
    assert os.path.exists(args.in_2bit), "--gen .2bit file \"%s\" not found" %(args.in_2bit)
    assert rplib.bed_check_six_col_format(args.in_sites), "--in BED file appears to be not in 6-column BED format"
    if args.pc_bw:
        assert os.path.exists(args.pc_bw), "--phastcons file \"%s\" not found" %(args.pc_bw)
    if args.pp_bw:
        assert os.path.exists(args.pp_bw), "--phylop file \"%s\" not found" %(args.pp_bw)
    if args.tr_reg_codon_annot:
        assert args.tr_reg_annot, "--tra-codons requires --tra to be effective"
    if args.tr_reg_border_annot:
        assert args.tr_reg_annot, "--tra-borders requires --tra to be effective"
    if args.intron_border_annot:
        assert args.exon_intron_annot, "--eia-ib requires --eia to be effective"
    if args.exon_intron_n:
        assert args.exon_intron_annot, "--eia-n requires --eia to be effective"
    # Check uniqueness of IDs.
    assert rplib.bed_check_unique_ids(args.in_sites), "--in BED file \"%s\" column 4 IDs not unique. Please provide BED entries with unique column 4 IDs" %(args.in_sites)

    # Mode settings output file.
    SETOUT = open(settings_file, "w")
    for arg in vars(args):
        SETOUT.write("%s\t%s\n" %(arg, str(getattr(args, arg))))
    SETOUT.close()

    # Get chromsome lengths from --gen.
    print("Get chromosome lengths from --gen ... ")
    chr_len_dic = rplib.get_chromosome_lengths_from_2bit(args.in_2bit, chr_lengths_file)
    gen_chr_len_dic = chr_len_dic
    # Get chromosome IDs.
    print("Get chromosome IDs from --in ... ")
    chr_ids_dic = rplib.bed_get_chromosome_ids(args.in_sites)

    # Check whether IDs are genomic or not.
    transcript_regions = False
    for chr_id in chr_ids_dic:
        if chr_id not in chr_len_dic:
            transcript_regions = True
            break
    # Demand pure transcript or genomic sites --in BED.
    if transcript_regions:
        for chr_id in chr_ids_dic:
            assert chr_id not in chr_len_dic, "chromosome and non-chromosome IDs encountered in --in BED column 1. --in column 1 must contain either chromsome or transcript IDs (conflicting IDs: \"%s\", \"%s\")" %(transcript_regions, chr_id)
        print("No chromosome IDs found in --in, interpret column 1 IDs as transcript IDs ... ")

    # If transcript_regions, there should be no genomic col1 IDs.
    tr_seqs_dic = {}
    core_ids_label_dic = {}
    if transcript_regions:
        # Check for set --exon-intron.
        if args.exon_intron_annot:
            print("ERROR: setting --eia is useless for transcript sites. Please use --train-in model not trained with --eia or provide genomic input sites")
            sys.exit()
        if args.intron_border_annot:
            print("ERROR: setting --eia-ib is useless for transcript sites. Please use --train-in model not trained with --eia or provide genomic input sites")
            sys.exit()
        if args.exon_intron_n:
            print("ERROR: setting --eia-ib is useless for transcript sites. Please use --train-in model not trained with --eia or provide genomic input sites")
            sys.exit()
        # Read in transcript lengths from --gtf.
        print("Get transcript lengths from --gtf ... ")
        tr_len_dic = rplib.gtf_get_transcript_lengths(args.in_gtf)
        print("Verifying transcript IDs by checking their presence in --gtf ... ")
        for tr_id in chr_ids_dic:
            assert tr_id in tr_len_dic, "--in column 1 transcript ID \"%s\" not found in --gtf" %(tr_id)
        # Make a transcript regions .bed file.
        print("Output transcripts containing test sites to BED ... ")
        rplib.bed_sequence_lengths_to_bed(tr_len_dic, regions_with_sites_bed,
                                           ids_dic=chr_ids_dic)

        # Get spliced transcript sequences from gtf+2bit.
        print("Get transcript sequences from --gtf and --gen ... ")
        tr_seqs_dic = rplib.get_transcript_sequences_from_gtf(args.in_gtf, args.in_2bit,
                                                              lc_repeats=args.rep_reg_annot,
                                                              tr_ids_dic=chr_ids_dic)
        # Output transcript lengths file.
        print("Output transcript lengths file ... ")
        rplib.output_chromosome_lengths_file(tr_len_dic, chr_lengths_file)
        # Make a new chr_len_dic, only with transcript IDs from --in and their lengths.
        chr_len_dic = {}
        for tr_id in chr_ids_dic:
            tr_len = tr_len_dic[tr_id]
            chr_len_dic[tr_id] = tr_len
    else:
        # Genomic sites checks.
        if args.tr_reg_border_annot:
            print("ERROR: setting --tra-borders only supported for transcript sites. Please disable or provide transcript input sites.")
            sys.exit()
        # Check for genomic region _e or _p (split) IDs (exon regions).
        in_reg_ids_dic = rplib.bed_get_region_ids(args.in_sites)
        core_ids_dic = rplib.get_core_id_to_part_counts_dic(in_reg_ids_dic,
                                                            label_dic=core_ids_label_dic)
        if args.exon_intron_annot:
            print("Genomic regions and --eia enabled: check for split IDs ... ")
            for core_id in core_ids_dic:
                assert core_ids_dic[core_id] == 1, "--eia enabled but genomic split IDs (== exon regions) found. Disable --eia or use transcript regions"
    """
    Process --in sites
    ==================

    Filter --in sites based on:
    Reference ID (only standard chromosome or transcript IDs allowed)
    Sites with gene overlap (--gene-filter)

    """

    # Process --in positives.
    count_stats_dic = {}
    # Lowercase uppercase lowercase part lengths.
    id2pl_dic = {}

    id2row_dic = rplib.process_test_sites(args.in_sites, tmp_bed, chr_len_dic,
                                          id2pl_dic, args,
                                          check_ids=True,
                                          transcript_regions=transcript_regions,
                                          count_dic=count_stats_dic)
    print("# sites found in --in:               %i" %(count_stats_dic['c_in']))
    if count_stats_dic['c_filt_ref']:
        print("# sites removed (reference ID):      %i" %(count_stats_dic['c_filt_ref']))

    assert count_stats_dic['c_out'], "no sites remaining after filtering ... "
    print("# sites remaining (post-filtering):  %i" %(count_stats_dic['c_out']))

    # For genomic sites, extract gene regions containing positives.
    if not transcript_regions:
        # Get gene regions from .gtf.
        print("Extract gene regions from --gtf ... ")
        rplib.gtf_extract_gene_bed(args.in_gtf, gene_regions_gtf_tmp_bed)
        # Get gene regions containing positives.
        params = "-u -s"
        print("Get gene regions containing --in sites ... ")
        rplib.intersect_bed_files(gene_regions_gtf_tmp_bed, args.in_sites, params, regions_with_sites_bed)
        # Filter positives by gene coverage.
        if not args.gene_filter:
            rplib.make_file_copy(tmp_bed, processed_in_sites_bed)
        else:
            # Accept only --in sites that overlap with gene regions.
            print("Filter --in sites by gene overlap ... ")
            params = "-u -s"
            rplib.intersect_bed_files(tmp_bed, gene_regions_gtf_tmp_bed, params, processed_in_sites_bed)
            c_out = rplib.count_file_rows(processed_in_sites_bed)
            assert c_out, "no remaining --in sites after filtering by gene overlap"
            print("# --in sites after filtering by gene coverage:  %i" %(c_out))
    else:
        rplib.make_file_copy(tmp_bed, processed_in_sites_bed)

    """
    Extract site sequences + filter
    ===============================

    - For transcript sites, extract from sequence dictionary.
    - For genomic sites, extract from --gen in_2bit.
    - For genomic sites, delete sites with lengths != extracted sequence lengths.
    - Remove N containing sequences / sites.

    """

    # Get region lengths.
    test_bed_len_dic = rplib.bed_get_region_lengths(processed_in_sites_bed)

    if transcript_regions:
        # Get sequences for transcript sites.
        print("Extract sequences for transcript sites ... ")
        test_seqs_dic = rplib.extract_transcript_sequences(id2row_dic, tr_seqs_dic)
    else:
        # Get sequences for genomic sites.
        print("Extract sequences for genomic sites ... ")
        # Extract positive sequences from .2bit.
        rplib.bed_extract_sequences_from_2bit(processed_in_sites_bed, tmp_fasta, args.in_2bit,
                                               lc_repeats=args.rep_reg_annot)
        # Read in sequences.
        test_seqs_dic = rplib.read_fasta_into_dic(tmp_fasta)

    # Compare region lengths with sequence lengths, delete mismatches.
    print("Compare extracted sequence lengths to site lengths ... ")

    # IDs to delete lists.
    test_del_list = []
    for seq_id in test_seqs_dic:
        seq_l = len(test_seqs_dic[seq_id])
        reg_l = test_bed_len_dic[seq_id]
        if seq_l != reg_l:
            test_del_list.append(seq_id)

    # Delete mismatches.
    for seq_id in test_del_list:
        del test_seqs_dic[seq_id]
    # Deleted and remaining counts.
    c_test_del = len(test_del_list)
    c_test_rem = len(test_seqs_dic)

    assert test_seqs_dic, "no sites remaining after sequence vs region length comparison"
    if c_test_del:
        print("# sites removed due to region vs sequence length difference:  %i" %(c_test_del))

    # Remove N containing sequences.
    print("Remove N containing sequences ... ")
    test_del_list = []
    neg_del_list = []
    for seq_id in test_seqs_dic:
        if re.search("N", test_seqs_dic[seq_id], re.I):
            test_del_list.append(seq_id)

    # Delete mismatches.
    for seq_id in test_del_list:
        del test_seqs_dic[seq_id]

    # Check / summarize.
    c_test_del = len(test_del_list)
    if c_test_del:
        print("# N containing sites removed:  %i" %(c_test_del))
    assert test_seqs_dic, "no sites remaining after removing N containing ones"

    """
    Create lowercase sequence context
    =================================

    - Make upstream + downstream sequence context lowercase.
      Upstream lowercase, uppercase, and downstream lowercase region lengths
      for each site ID are stored in id2pl_dic.
      For negatives, use new2oldid_dic for new random negative ID mapping
      to old associated positive ID, from which to get uppercase+lowercase
      lengths.
    - Output sequences + sites to finish FASTA+BED extraction part.

    """

    # Dictionaries for storing original lowercase+uppercase information (if --rra).
    test_seqs_rep_lc_dic = {}
    if args.rep_reg_annot:
        # Store lowercase info in dics, then make sequences uppercase again.
        for seq_id in test_seqs_dic:
            seq = test_seqs_dic[seq_id]
            test_seqs_rep_lc_dic[seq_id] = seq
            test_seqs_dic[seq_id] = seq.upper()

    # Length statistics for final positives + negatives.
    final_len_list = []
    final_len_list = rplib.get_seq_len_list_from_dic(test_seqs_dic)
    c_final_test = len(final_len_list)

    test_ids_output_dic = {}
    for test_id in test_seqs_dic:
        test_ids_output_dic[test_id] = 1

    # Merge split _e or _p sites into one sequence.
    if core_ids_label_dic:
        print("Split BED regions encountered. Merging split region sequences ... ")
        c_del = 0
        c_new = 0
        c_before = len(test_seqs_dic)
        for core_id in core_ids_label_dic:
            new_id = core_id
            new_seq = ""
            part_label = core_ids_label_dic[core_id]
            c_parts = core_ids_dic[core_id]
            assert c_parts > 1, "c_parts == 1 but core_ids_label_dic should only contain entries > 2 (core_id: %s)" %(core_id)
            part_i = 0
            for i in range(c_parts):
                part_i = i + 1
                part_id = core_id + "_" + part_label + str(part_i)
                assert part_id in test_seqs_dic, "part ID %s not in test_seqs_dic" %(part_id)
                new_seq += test_seqs_dic[part_id]
                del test_seqs_dic[part_id]
                c_del += 1
            test_seqs_dic[new_id] = new_seq
            c_new += 1
        c_after = len(test_seqs_dic)
        print("# of regions before merging:  %i" %(c_before))
        print("# of regions after merging:   %i (-%i +%i)" %(c_after, c_del, c_new))
        c_final_test = c_after

    # Output full-length (+ context) sequences.
    print("Output selected sites to BED and FASTA ... ")
    rplib.fasta_output_dic(test_seqs_dic, test_fasta_out, split=True)

    # Output BED regions.
    rplib.bed_write_row_dic_into_file(id2row_dic, test_bed_out,
                                       id2out_dic=test_ids_output_dic)
    print("# prediction sites output:  %i" %(c_final_test))

    """
    Calculate additional features
    =============================

    Currently these are:
    structure (for sequence, genomic regions, and transcript regions)
    transcript region annotation (for genomic and transcript regions)
    exon-intron annotation (for genomic regions)
    conservation scores (for genomic and transcript regions)
    repeat region information (for genomic and transcript regions)

    1) structure (--str):
    To calculate probabilities of structural elements.
    .str: unpaired + structural elements probabilities

    2) conservation scores (--phastcons, --phylop):
    Position-wise phyloP and / or phastCons scores to add for each
    site position.
    .pc.con : phastCons position-wise conservation scores file
    .pp.con : phyloP position-wise conservation scores file

    3) transcript region annotation (--tra, --tra-codons, --tra-borders)
    Add transcript region label annotations from --gtf to genomic or
    transcript sites.
    --tra: add 5'UTR (F), CDS (C), None (N), and 3'UTR (T) annotation
    to transcript or genomic sites.
    --tra-codons: add start (S) and stop (E) codon annotation to transcript
    or genomic sites.
    --tra-borders: add transcript start (A), transcript end (Z) nt, and
    exon border (B) annotation to transcript sites.

    4) exon-intron annotation (--eia, --eia-ib, --eia-n):
    For genomic sites only.
    Position-wise exon + intron annotation, for each genomic site position
    .eia : stores E or I labels for each site position
    If --eia-n, additional N label for regions not overlapping with
    transcripts (by default these are labelled as I).
    If --eia-ib, intron border labels (F: 5' intron start, T: 3' intron end)

    5) repeat region annotation (--rra)
    Add position-wise R (repeat) or N (no repeat) annotations to label
    repeat and non-repeat region sequences.
    Repeat region information is stored in --gen .2bit, with lower-case
    nucleotides belonging to repeat regions annotated by RepeatMasker and
    Tandem Repeats Finder (with period of 12 or less).

    """

    # Additional feature stats dictionaries.
    test_str_stats_dic = None
    test_phastcons_stats_dic = None
    test_phylop_stats_dic = None
    test_eia_stats_dic = None
    test_tra_stats_dic = None
    test_rra_stats_dic = None
    add_feat_dic_list = None
    if args.report and args.add_str:
        test_str_stats_dic = {}
    if args.report and args.pc_bw:
        test_phastcons_stats_dic = {}
    if args.report and args.pp_bw:
        test_phylop_stats_dic = {}
    if args.report and args.exon_intron_annot:
        test_eia_stats_dic = {}
    if args.report and args.tr_reg_annot:
        test_tra_stats_dic = {}
    if args.report and args.rep_reg_annot:
        test_rra_stats_dic = {}

    # Store feature infos.
    FEATOUT.write("fa\tC\tA,C,G,U\t-\n")
    if args.add_str:
        FEATOUT.write("str\tN\tE,H,I,M,S\tprob\n")
    if args.pc_bw:
        FEATOUT.write("pc.con\tN\tphastcons_score\tprob\n")
    if args.pp_bw:
        FEATOUT.write("pp.con\tN\tphylop_score\tminmax2\n")
    if args.tr_reg_annot:
        ll = ["F", "C", "N", "T"] # label list.
        if args.tr_reg_border_annot:
            ll.append("A")
            ll.append("Z")
            ll.append("B")
        if args.tr_reg_codon_annot:
            ll.append("S")
            ll.append("E")
        ll.sort()
        lls = ",".join(ll)
        FEATOUT.write("tra\tC\t%s\t-\n" %(lls))
    if args.exon_intron_annot:
        ll = ["E", "I"]
        if args.intron_border_annot:
            ll.append("T")
            ll.append("F")
        if args.exon_intron_n:
            ll.append("N")
        ll.sort()
        lls = ",".join(ll)
        FEATOUT.write("eia\tC\t%s\t-\n" %(lls))
    if args.rep_reg_annot:
        FEATOUT.write("rra\tC\tN,R\t-\n")

    """
    1) Secondary structure features.

    """
    # Secondary structure stats dictionaries.

    if args.add_str:

        # Calculate structure features for test set.
        print("Get secondary structure information ... ")
        rplib.calc_ext_str_features(id2row_dic, chr_len_dic,
                                    test_str_out, args,
                                    stats_dic=test_str_stats_dic,
                                    check_seqs_dic=test_seqs_dic,
                                    tr_regions=transcript_regions,
                                    tr_seqs_dic=tr_seqs_dic)

    """
    2) Conservation scores.

    """
    # More preprocessing if transcript_regions.
    if transcript_regions:
        if args.pc_bw or args.pp_bw or args.tr_reg_annot:
            print("Additional annotations for transcript sites require mapping to genome ... ")
            print("Mapping transcript sites to genome ... ")
            id2hitc_dic = {}
            rplib.bed_convert_transcript_to_genomic_sites(test_bed_out, args.in_gtf,
                                                          test_transcript_to_genome_bed,
                                                          site2hitc_dic=id2hitc_dic)

    # If phastCons conservation scores (--pc-bw) are given.
    if args.pc_bw:
        if transcript_regions:
            print("Extracting phastCons conservation scores for transcript sites ... ")
            # For transcript sites.
            rplib.extract_conservation_scores(test_transcript_to_genome_bed,
                                              test_pc_con_out, args.pc_bw,
                                              stats_dic=test_phastcons_stats_dic,
                                              merge_split_regions=True,
                                              report=False)
        else:
            print("Extracting phastCons conservation scores for genomic sites ... ")
            # For genomic sites.
            rplib.extract_conservation_scores(test_bed_out,
                                              test_pc_con_out, args.pc_bw,
                                              stats_dic=test_phastcons_stats_dic,
                                              merge_split_regions=True,
                                              report=False)

    # If phastCons conservation scores (--pp-bw) are given.
    if args.pp_bw:
        if transcript_regions:
            print("Extracting phyloP conservation scores for transcript sites ... ")
            # For transcript sites.
            rplib.extract_conservation_scores(test_transcript_to_genome_bed,
                                              test_pp_con_out, args.pp_bw,
                                              stats_dic=test_phylop_stats_dic,
                                              merge_split_regions=True,
                                              report=False)
        else:
            print("Extracting phyloP conservation scores for genomic sites ... ")
            # For genomic sites.
            rplib.extract_conservation_scores(test_bed_out,
                                              test_pp_con_out, args.pp_bw,
                                              stats_dic=test_phylop_stats_dic,
                                              merge_split_regions=True,
                                              report=False)
        print("Normalizing phyloP scores ... ")
        rplib.phylop_norm_test_scores(test_pp_con_out)

    """
    3) Transcript region annotations for transcript sites.

    """
    if args.tr_reg_annot and transcript_regions:
        print("Extracting transcript region annotations for transcript sites ... ")
        if args.tr_reg_codon_annot:
            print("Start + stop codon annotations enabled ... ")
        if args.tr_reg_border_annot:
            print("Transcript + exon border annotations enabled ... ")
        tr_ids_dic = chr_len_dic
        rplib.bed_get_transcript_annotations_from_gtf(tr_ids_dic,
                                            test_transcript_to_genome_bed,
                                            args.in_gtf, test_tra_out,
                                            stats_dic=test_tra_stats_dic,
                                            codon_annot=args.tr_reg_codon_annot,
                                            border_annot=args.tr_reg_border_annot,
                                            merge_split_regions=True)

    """
    4) Exon-intron + transcript region annotations for genomic sites.

    """
    if (args.exon_intron_annot or args.tr_reg_annot) and not transcript_regions:
        print("Get transcript annotations for genomic sites ... ")
        # Get transcript IDs to use for exon-intron or transcript region annotation.
        tr_ids_dic = {}
        # Extract tr_ids_dic.
        if args.tr_reg_annot or not args.eia_all_ex:
            if args.tr_list:
                print("Using transcript list from --tr-list to define transcript regions ... ")
                # Read in transcript IDs from --tr-list.
                tr_ids_dic = rplib.read_ids_into_dic(args.tr_list)
                # Read in all transcript IDs in --gtf.
                all_tr_ids_dic = rplib.gtf_get_transcript_ids(args.in_gtf)
                # Check if given --tr list IDs are in --gtf.
                del_tr_l = []
                for tr_id in tr_ids_dic:
                    if tr_id not in all_tr_ids_dic:
                        print("WARNING: transcript ID \"%s\" from --tr-list not found in --gtf. Ignoring transcript ... " %(tr_id))
                        del_tr_l.append(tr_id)
                for tr_id in del_tr_l:
                    del tr_ids_dic[tr_id]
                assert tr_ids_dic, "no transcript IDs from --tr-list remain after removing IDs not found in --gtf"
            else:
                # Create exon BED file from most prominent transcripts.
                strict = False
                if strict:
                    print("Get most prominent transcript for each gene from --gtf (TSL1-5 only)... ")
                else:
                    print("Get most prominent transcript for each gene from --gtf ... ")
                # Don't be strict (!= only TSL1-5 allowed).
                tr_ids_dic = rplib.gtf_extract_most_prominent_transcripts(args.in_gtf,
                                                "dummy",
                                                strict=strict,
                                                return_ids_dic=tr_ids_dic)

        # Exon-intron annotations for genomic sites.
        if args.exon_intron_annot:

            # All exons BED (--eia-all-ex case).
            all_exon_bed = False
            if args.eia_all_ex:
                random_id = uuid.uuid1()
                all_exon_bed = str(random_id) + ".tmp.bed"
                print("Extract all exon regions from --gtf (--eia-all-ex enabled) ... ")
                rplib.gtf_extract_unique_exon_bed(args.in_gtf, all_exon_bed,
                                                  use_ei_labels=True)

            print("Extract exon-intron annotations for genomic sites ... ")
            if not all_exon_bed:
                if args.tr_list:
                    print("Use exon regions of --tr-list transcripts from --gtf ... ")
                else:
                    print("Use exon regions of most prominent transcripts from --gtf ... ")
                if args.intron_border_annot:
                    print("Intron border annotation enabled ... ")
                if args.exon_intron_n:
                    print("Non-intron non-exon region annotation enabled ... ")

            rplib.bed_get_exon_intron_annotations_from_gtf(tr_ids_dic, test_bed_out,
                                        args.in_gtf, test_eia_out,
                                        stats_dic=test_eia_stats_dic,
                                        own_exon_bed=all_exon_bed,
                                        n_labels=args.exon_intron_n,
                                        intron_border_labels=args.intron_border_annot)

            if all_exon_bed:
                if os.path.exists(all_exon_bed):
                    os.remove(all_exon_bed)

        # Transcript region annotations for genomic sites.
        if args.tr_reg_annot:
            print("Extracting transcript region annotations for genomic sites ... ")
            if args.tr_reg_codon_annot:
                print("Start + stop codon annotations enabled ... ")
            rplib.bed_get_transcript_annotations_from_gtf(tr_ids_dic,
                                                test_bed_out,
                                                args.in_gtf, test_tra_out,
                                                stats_dic=test_tra_stats_dic,
                                                codon_annot=args.tr_reg_codon_annot,
                                                border_annot=args.tr_reg_border_annot,
                                                merge_split_regions=True)

    """
    5) Repeat region annotations for genomic and transcript sites.

    """
    if args.rep_reg_annot:
        print("Extracting repeat region annotations ... ")
        rplib.fasta_get_repeat_region_annotations(test_seqs_rep_lc_dic, test_rra_out,
                                                  stats_dic=test_rra_stats_dic)

    """
    6) Additional BED features.

    Feature ID (column 1):
    Up to 20 characters, ID for features, used as file ending
    Feature type (column 2):
    C (categorical), N (numerical)
    Feature format (column 3):
        0 (treat feature as categorical == no normalization, do not change values)
        1 (col5 value == score), meaning apply min-max normalization
        2 (col5 value == p-value)
    Feature BED file path (column 4):
    Path to BED file to annotate feature regions.

    """
    feat_id_list = []

    if feat_in_file:

        print("Extracting additional BED features from --feat-in ... ")

        # Store dataset infos.
        fids_dic = {}
        fid2type_dic = {} # C,N
        fid2format_dic = {} # 0,1,2
        fid2path_dic = {}
        # Read in infos.
        with open(feat_in_file) as f:
            for line in f:
                row = line.strip()
                cols = line.strip().split("\t")
                feat_id = cols[0]
                feat_type = cols[1]
                feat_format = int(cols[2])
                bed_path = cols[3]
                # Convert to valid file ending.
                new_feat_id = rplib.get_valid_file_ending(feat_id)
                if feat_id != new_feat_id:
                    print("Converted feature ID \"%s\" to \"%s\" to have valid file ending" %(feat_id, new_feat_id))
                assert new_feat_id not in fids_dic, "non-unique feature ID \"%s\" in --feat-in table file" %(new_feat_id)
                fids_dic[new_feat_id] = 1
                fid2type_dic[new_feat_id] = feat_type
                fid2format_dic[new_feat_id] = feat_format
                fid2path_dic[new_feat_id] = bed_path
        f.closed
        assert fids_dic, "no features read in from --feat-in table"

        if args.report:
            add_feat_dic_list = []

        for feat_id in fids_dic:
            bed_path = fid2path_dic[feat_id]
            feat_type = fid2type_dic[feat_id]
            feat_format = fid2format_dic[feat_id]
            p_values = False
            if feat_format == 2:
                p_values = True
            # If all BED features should be treated as one-hot.
            if args.feat_in_1h:
                feat_type = "C"
            # Output feature info.
            if feat_type == "C":
                FEATOUT.write("%s\tC\t0,1\tone_hot\n" %(feat_id))
            else:
                if feat_format:
                    if p_values:
                        FEATOUT.write("%s\tN\tbed_score\tprob\n" %(feat_id))
                    else:
                        FEATOUT.write("%s\tN\tbed_score\tminmax_norm\n" %(feat_id))
                else:
                    FEATOUT.write("%s\tN\tbed_score\t-\n" %(feat_id))

            # Stats dictionaries.
            test_stats_dic = None
            if args.report:
                test_stats_dic = {}

            """
            Check if --feat-in regions are purely genomic or transcript,
            and whether mapping to transcriptome is necessary.
            """

            bed_ids_dic = rplib.bed_get_chromosome_ids(bed_path)
            if transcript_regions:
                print("Check whether \"%s\" feature BED contains exclusively genomic or transcript regions ..." %(feat_id))
                # Check if IDs are transcript IDs.
                need_to_map = False
                if not rplib.check_dic1_keys_in_dic2(bed_ids_dic, tr_len_dic):
                    print("Regions not exclusively transcript in \"%s\" feature BED, check for genomic regions ... " %(feat_id))
                    # Check if IDs are genomic IDs.
                    if rplib.check_dic1_keys_in_dic2(bed_ids_dic, gen_chr_len_dic):
                        need_to_map = True
                    else:
                        assert False, "Regions not exclusively genomic in in \"%s\" feature BED. Supply either solely transcript or solely genomic regions (no mixed regions allowed so far)" %(feat_id)
                if need_to_map:
                    # Temp mapping folder.
                    map_out_folder = args.out_folder + "/" + "tmp_map_out"
                    if os.path.exists(map_out_folder):
                        shutil.rmtree(map_out_folder)
                    os.makedirs(map_out_folder)

                    print("\"%s\" feature BED contains genomic regions ... " %(feat_id))
                    #print("Get all transript IDs from --gtf ...")
                    #all_tr_ids_dic =  dsglib.gtf_get_transcript_ids(args.in_gtf)
                    print("Map \"%s\" feature BED regions to transcriptome, using all --gtf transcripts ... " %(feat_id))

                    # Map feature BED genomic regions to transcriptome.
                    rplib.convert_genome_positions_to_transcriptome(bed_path,
                                                                    map_out_folder,
                                                                    args.in_gtf,
                                                                    tr_len_dic)

                    all_comp_transcript_hits_bed = map_out_folder + "/" + "transcript_hits_complete.bed"
                    all_incomp_transcript_hits_bed = map_out_folder + "/" + "transcript_hits_incomplete.bed"
                    assert os.path.exists(all_comp_transcript_hits_bed), "Mapping file %s not found" %(all_comp_transcript_hits_bed)
                    assert os.path.exists(all_incomp_transcript_hits_bed), "Mapping file %s not found" %(all_incomp_transcript_hits_bed)
                    merge_list = []
                    merge_list.append(all_comp_transcript_hits_bed)
                    merge_list.append(all_incomp_transcript_hits_bed)
                    rplib.merge_files(merge_list, bed_path)

            else:
                # Check if IDs are genomic IDs.
                assert rplib.check_dic1_keys_in_dic2(bed_ids_dic, chr_len_dic), "Regions not exclusively genomic in in \"%s\" feature BED. Please provide genomic regions matching --gtf and --gen files" %(feat_id)


            print("Annotating \"%s\" feature ... " %(feat_id))

            # Training set.
            test_feat_out = args.out_folder + "/" + "test." + feat_id
            rplib.bed_get_feature_annotations(test_bed_out, bed_path, test_feat_out,
                                              feat_type=feat_type,
                                              stats_dic=test_stats_dic,
                                              disable_pol=False)
            # Add additional stats to pos_stats_dic.
            if args.report:
                test_stats_dic["set"] = "test"
                test_stats_dic["feat_id"] = feat_id
                add_feat_dic_list.append(test_stats_dic)
            neg_feat_out = args.out_folder + "/" + "negatives." + feat_id
            # Normalize feature values.
            if args.feat_in_norm and feat_format and feat_type == "N":
                print("Min-max normalizing \"%s\" feature values ... " %(feat_id))
                rplib.feat_min_max_norm_test_scores(test_feat_out,
                                                    p_values=p_values,
                                                    int_whole_nr=True)
            feat_id_list.append(feat_id)

    FEATOUT.close()

    # Delete feature BED transcript mapping folder if present.
    map_out_folder = args.out_folder + "/" + "tmp_map_out"
    if os.path.exists(map_out_folder):
        shutil.rmtree(map_out_folder)

    """
    Generate HTML report including various training dataset statistics.

    """
    dataset_type =  "g"
    if transcript_regions:
        dataset_type =  "t"

    if args.report:
        # Gene biotype statistics for target genes.
        gbt_stats = True
        # All gene biotype counts.
        all_gbtc_dic = {}
        # Target gene biotype counts.
        target_gbtc_dic = {}
        # Target region to hit count (transcript or gene ID -> hit count).
        t2hc_dic = {}
        # Target region to info.
        t2i_dic = {}
        if gbt_stats:
            # Calculate additional stats for report.
            print("Extract additional statistics for report ... ")
            # Get count stats for gene biotypes.
            region_ids_dic = rplib.bed_get_region_ids(regions_with_sites_bed)
            # Get gene biotype counts for target genes.
            if transcript_regions:
                target_gbtc_dic = rplib.gtf_get_gene_biotypes_from_transcript_ids(
                                            region_ids_dic, args.in_gtf,
                                            all_gbtc_dic=all_gbtc_dic)

                # Get transcript ID to hit count dictionary.
                t2hc_dic = rplib.bed_get_chromosome_ids(test_bed_out)
                # Get transcript infos.
                t2i_dic = rplib.gtf_get_transcript_infos(region_ids_dic, args.in_gtf)
            else:
                target_gbtc_dic = rplib.gtf_get_gene_biotypes(
                                            region_ids_dic, args.in_gtf,
                                            all_gbtc_dic=all_gbtc_dic)
                # Get overlapping site counts for each gene.
                t2hc_dic = rplib.bed_intersect_count_region_overlaps(gene_regions_gtf_tmp_bed, test_bed_out)
                # Get gene infos.
                t2i_dic = rplib.gtf_get_gene_infos(region_ids_dic, args.in_gtf)

        # HTML report output file.
        html_report_out = args.out_folder + "/" + "report.rnaprot_gp.html"
        # Plots subfolder.
        plots_subfolder = "plots_rnaprot_gp"
        # Get library and logo path.
        rplib_path = os.path.dirname(rplib.__file__)
        # Generate report.
        rplib.rp_gp_generate_html_report(test_seqs_dic,
                                         args.out_folder, dataset_type, rplib_path,
                                         html_report_out=html_report_out,
                                         plots_subfolder=plots_subfolder,
                                         test_str_stats_dic=test_str_stats_dic,
                                         test_phastcons_stats_dic=test_phastcons_stats_dic,
                                         test_phylop_stats_dic=test_phylop_stats_dic,
                                         test_eia_stats_dic=test_eia_stats_dic,
                                         test_tra_stats_dic=test_tra_stats_dic,
                                         test_rra_stats_dic=test_rra_stats_dic,
                                         add_feat_dic_list=add_feat_dic_list,
                                         target_gbtc_dic=target_gbtc_dic,
                                         all_gbtc_dic=all_gbtc_dic,
                                         t2hc_dic=t2hc_dic,
                                         t2i_dic=t2i_dic,
                                         kmer_top=10,
                                         target_top=20,
                                         theme=args.theme,
                                         rna=True)

    """
    Take out the trash.

    """
    clean_up = True
    if clean_up:
        print("Removing temporary files ... ")
        # Remove tmp files.
        if os.path.exists(tmp_bed):
            os.remove(tmp_bed)
        if os.path.exists(gene_regions_gtf_tmp_bed):
            os.remove(gene_regions_gtf_tmp_bed)
        if os.path.exists(processed_in_sites_bed):
            os.remove(processed_in_sites_bed)
        if os.path.exists(tmp_fasta):
            os.remove(tmp_fasta)
        if os.path.exists(test_transcript_to_genome_bed):
            os.remove(test_transcript_to_genome_bed)

    """
    Output file summary.

    """
    print("")
    print("PREDICTION SET OUTPUT FILES")
    print("===========================")
    print("Sequences .fa:\n%s" %(test_fasta_out))
    print("Sites .bed:\n%s" %(test_bed_out))
    if args.add_str:
        print("Structural elements probabilities .str:\n%s" %(test_str_out))
    if args.pc_bw:
        print("phastCons conservation scores .pc.con:\n%s" %(test_pc_con_out))
    if args.pp_bw:
        print("phyloP conservation scores .pp.con:\n%s" %(test_pp_con_out))
    if args.exon_intron_annot:
        print("Exon-intron region annotations .eia:\n%s" %(test_eia_out))
    if args.tr_reg_annot:
        print("Transcript region annotations .tra:\n%s" %(test_tra_out))
    if args.rep_reg_annot:
        print("Repeat region annotations .rra:\n%s" %(test_rra_out))
    if feat_id_list:
        for add_fid in feat_id_list:
            test_feat_out = args.out_folder + "/" + "test." + add_fid
            print("Additional --feat-in annotations .%s:\n%s" %(add_fid, test_feat_out))
    if args.report:
        print("Prediction set generation report .html:\n%s" %(html_report_out))
    print("")


################################################################################

if __name__ == '__main__':
    # Setup argparse.
    parser = setup_argument_parser()
    # Print help if no parameter is set.
    if len(sys.argv) < 2:
        parser.print_help()
        sys.exit()
    # Read in command line arguments.
    args = parser.parse_args()

    # Show some banner.
    banner = """
      ____  _   _____    ____  ____  ____  ______
     / __ \/ | / /   |  / __ \/ __ \/ __ \/_  __/
    / /_/ /  |/ / /| | / /_/ / /_/ / / / / / /
   / _, _/ /|  / ___ |/ ____/ _, _/ /_/ / / /
  /_/ |_/_/ |_/_/  |_/_/   /_/ |_|\____/ /_/                                                     
"""
    print(banner)

    # Drop a line.
    print(rplib.drop_a_line())

    # Are my tools ready?
    assert rplib.is_tool("bedtools"), "bedtools not in PATH!"
    assert rplib.is_tool("twoBitToFa"), "twoBitToFa not in PATH!"
    assert rplib.is_tool("twoBitInfo"), "twoBitInfo not in PATH!"
    assert rplib.is_tool("bigWigAverageOverBed"), "bigWigAverageOverBed not in PATH!"

    # Run selected mode.
    if args.which == 'train':
        main_train(args)
    elif args.which == 'eval':
        main_eval(args)
    elif args.which == 'predict':
        main_predict(args)
    elif args.which == 'gt':
        main_gt(args)
    elif args.which == 'gp':
        main_gp(args)

################################################################################
